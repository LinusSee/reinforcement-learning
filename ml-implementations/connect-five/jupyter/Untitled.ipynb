{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\").unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(40, interpolation=Image.CUBIC),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return (int(env.state[0] * scale + screen_width / 2.0))\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger such as 800x1200x3.\n",
    "    # Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode=\"rgb_array\").transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height * 0.4) : int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > ( screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2)\n",
    "        \n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation=\"none\")\n",
    "plt.title(\"Example extracted screen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape returned from AI gym.\n",
    "# Typical dimensions at this point are close to 3x40x90 which is the result of a clamped and\n",
    "# downscaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10_000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title(\"Training...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeroes(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "           \n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
    "    # This converts batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which the simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([ s for s in batch.next_state if s is not None ])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
