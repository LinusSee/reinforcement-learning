{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original article\n",
    "https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/dungeon-simulator/dungeon_simulator.py\n",
    "import random\n",
    "\n",
    "\n",
    "class DungeonSimulator:\n",
    "\tdef __init__(self, length=5, slip=0.1, small=2, large=10):\n",
    "\t\tself.length = length\n",
    "\t\tself.slip = slip # Probability of flipping the action\n",
    "\t\tself.small = small\n",
    "\t\tself.large = large\n",
    "\t\tself.state = 0\n",
    "\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t''' Executes the action and returns the next state and the received reward.'''\n",
    "\t\tif random.random() < self.slip:\n",
    "\t\t\taction = not action\n",
    "\t\treward = 0\n",
    "\t\tif action == BACKWARD:\n",
    "\t\t\treward = self.small\n",
    "\t\t\tself.state = 0\n",
    "\t\telif action == FORWARD:\n",
    "\t\t\tif self.state < self.length - 1:\n",
    "\t\t\t\tself.state += 1\n",
    "\t\t\t\treward = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\treward = self.large\n",
    "\n",
    "\t\treturn self.state, reward\n",
    "\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.state = 0\n",
    "\t\treturn self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Drunkard:\n",
    "    def __init__(self):\n",
    "        self.q_table = None\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        # Random walk\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        pass # I don't care! I'm drunk!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Accountant:\n",
    "    def __init__(self):\n",
    "        # Spreadsheet (Q-table) for rewards accounting\n",
    "        self.q_table = [\n",
    "            [ 0, 0, 0, 0, 0 ], # FORWARD states\n",
    "            [ 0, 0, 0, 0, 0 ] # BACKWARD states\n",
    "        ]\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        # Is FORWARD reward bigger?\n",
    "        if self.q_table[FORWARD][state] > self.q_table[BACKWARD][state]:\n",
    "            return FORWARD\n",
    "        elif self.q_table[BACKWARD][state] > self.q_table[FORWARD][state]:\n",
    "            return BACKWARD\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.q_table[action][old_state] += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Gambler:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10000):\n",
    "        self.q_table = [\n",
    "            [ 0, 0, 0, 0, 0 ], # FORWARD states\n",
    "            [ 0, 0, 0, 0, 0 ] # BACKWARD states\n",
    "        ]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations # Shift from exploration to exploitation\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate:\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action()\n",
    "        \n",
    "    def greedy_action(self, state):\n",
    "        if self.q_table[FORWARD][state] > self.q_table[BACKWARD][state]:\n",
    "            return FORWARD\n",
    "        elif self.q_table[BACKWARD][state] > self.q_table[FORWARD][state]:\n",
    "            return BACKWARD\n",
    "        \n",
    "        return self.random_action()\n",
    "    \n",
    "    def random_action(self):\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Old Q-table value\n",
    "        old_value = self.q_table[action][old_state]\n",
    "        future_action = self.greedy_action(new_state)\n",
    "        future_reward = self.q_table[future_action][new_state]\n",
    "        \n",
    "        # Main Q-table updating algorithm\n",
    "        new_value = old_value + self.learning_rate * (reward + self.discount * future_reward - old_value)\n",
    "        self.q_table[action][old_state] = new_value\n",
    "        \n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepGambler:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10_000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / 10_000\n",
    "        \n",
    "        # Input has five neurons, each represents a single game state\n",
    "        self.input_count = 5\n",
    "        # Output is two neurons, each represents a Q-value for each action\n",
    "        self.output_count = 2\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "        \n",
    "    def define_model(self):\n",
    "        '''Define tensorflow model graph.'''\n",
    "        # Input is an array of 5 items (states one-hot encoded)\n",
    "        # Input is 2-dimensional due to possibility of batched training data (why does this change the input?)\n",
    "        # NOTE: In this example we assume no batching\n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[ None, self.input_count ])\n",
    "        \n",
    "        # Two hidden layers of 16 neurons with sigmoid activation initialized to zero for stability\n",
    "        fc1 = tf.layers.dense(self.model_input, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((self.input_count, 16))))\n",
    "        fc2 = tf.layers.dense(fc1, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((16, self.output_count))))\n",
    "        \n",
    "        # Output is two values, Q for both possible actions (FORWARD and BACKWARD)\n",
    "        # Output is 2-dimensional, due to possibility of batched training data (again, why??)\n",
    "        self.model_output = tf.layers.dense(fc2, self.output_count)\n",
    "        \n",
    "        # This is for feeding training output (a.k.a ideal target values)\n",
    "        self.target_output = tf.placeholder(shape=[ None, self.output_count ], dtype=tf.float32)\n",
    "        # Loss is mean squared difference between current output and ideal target values\n",
    "        loss = tf.losses.mean_squared_error(self.target_output, self.model_output)\n",
    "        # Optimizer adjusts weights to minimize loss, with the speed of the learning rate\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        # Initializer sets weights to initial values\n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "        \n",
    "    def get_Q(self, state):\n",
    "        '''Ask model to estimate Q value for specific state (via inference).'''\n",
    "        # Model input: Single state represented by array of 5 items (state one-hot encoded)\n",
    "        # Model output: Array of Q values for single state\n",
    "        return self.session.run(self.model_output, feed_dict={ self.model_input: self.to_one_hot(state) })[0]\n",
    "    \n",
    "    def to_one_hot(self, state):\n",
    "        '''Turn state into 2d one_hot tensor (e.g. 3 -> [[ 0, 0, 0, 1, 0]]).'''\n",
    "        one_hot = np.zeros((1, 5))\n",
    "        one_hot[0, [ state ]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate: # Exploit\n",
    "            return self.greedy_action(state)\n",
    "        else: # Explore\n",
    "            return self.random_action()\n",
    "        \n",
    "    def greedy_action(self, state):\n",
    "        '''Returns the action with the bigger Q-value, as estimated by our model (via inference)'''\n",
    "        return np.argmax(self.get_Q(state))\n",
    "    \n",
    "    def random_action(self):\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def train(self, old_state, action, reward, new_state):\n",
    "        # Ask the model for the Q values of the old state\n",
    "        old_state_Q_values = self.get_Q(old_state)\n",
    "        # Ask the model for the Q values of the new state\n",
    "        new_state_Q_values = self.get_Q(new_state)\n",
    "        # Change the Q value of the action we took to what we expect (so we can train towards our expected Q values)\n",
    "        old_state_Q_values[action] = reward + self.discount * np.amax(new_state_Q_values)\n",
    "        \n",
    "        # Set up training data\n",
    "        training_input = self.to_one_hot(old_state)\n",
    "        target_output = [ old_state_Q_values ]\n",
    "        training_data = { self.model_input: training_input, self.target_output: target_output }\n",
    "        \n",
    "        # Train\n",
    "        self.session.run(self.optimizer, feed_dict=training_data)\n",
    "        \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Train our model with new data\n",
    "        self.train(old_state, action, reward, new_state)\n",
    "            \n",
    "        # Shift our exploration_rate toward zero\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 16)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepPytorchGambler:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 5\n",
    "        self.output_count = 2\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        #self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = \"cpu\"\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state):\n",
    "        x = torch.tensor(self.to_one_hot(state)).float().to(self.device)\n",
    "        return self.model(x)[0]\n",
    "        \n",
    "    def to_one_hot(self, state):\n",
    "        '''Turn state into 2d one_hot tensor (e.g. 3 -> [[ 0, 0, 0, 1, 0]]).'''\n",
    "        one_hot = np.zeros((1, 5))\n",
    "        one_hot[0, [ state ]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.randrange(0, 2) # Maybe change the probability distribution?\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        #print(\"Greedy1:\", torch.max(self.get_Q(state), 0)[0])\n",
    "        #print(\"Greedy2:\", torch.max(self.get_Q(state), 0)[1])\n",
    "        return torch.max(self.get_Q(state), 0)[1]\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        old_state_values = self.get_Q(old_state)\n",
    "        new_state_values = self.get_Q(new_state)\n",
    "        \n",
    "        new_reward = reward + self.discount * torch.max(new_state_values)\n",
    "        updated_state_values = torch.tensor(old_state_values)\n",
    "        #print(old_state_values)\n",
    "        #print(new_state_values)\n",
    "        #print(updated_state_values)\n",
    "        updated_state_values[action] = new_reward\n",
    "        \n",
    "        \n",
    "        old_state_values = torch.tensor(old_state_values, device=self.device).float()\n",
    "        updated_state_values = torch.tensor(updated_state_values, device=self.device).float()\n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = torch.autograd.Variable(F.smooth_l1_loss(old_state_values, updated_state_values), requires_grad=True)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD = 0\n",
    "BACKWARD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--agent\", type=str, default=\"GAMBLER\", help=\"Which agent to use\")\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=0.1, help=\"How quickly the algorithm tries to learn\")\n",
    "parser.add_argument(\"--discount\", type=float, default=0.95, help=\"Discount for estimated future action\") # Reward?\n",
    "parser.add_argument(\"--iterations\", type=int, default=2000, help=\"Iteration count\")\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "discount = 0.95\n",
    "iterations = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a091fa90e091>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "agent = Drunkard() # 12754\n",
    "agent = Accountant() # 17548\n",
    "agent = Gambler() # 25890\n",
    "\n",
    "# 21186 with learning rate 0.01 (not as good, because a deep neural net is overkill for this simple Q-table)\n",
    "# Seems to vary quite a bit\n",
    "agent = DeepGambler(learning_rate=learning_rate) \n",
    "agent = DeepPytorchGambler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dungeon = DungeonSimulator()\n",
    "dungeon.reset()\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"step\": 0, \"total_reward\": 0}\n",
      "{\"step\": 250, \"total_reward\": 318}\n",
      "{\"step\": 500, \"total_reward\": 666}\n",
      "{\"step\": 750, \"total_reward\": 946}\n",
      "{\"step\": 1000, \"total_reward\": 1320}\n",
      "{\"step\": 1250, \"total_reward\": 1712}\n",
      "{\"step\": 1500, \"total_reward\": 2010}\n",
      "{\"step\": 1750, \"total_reward\": 2368}\n",
      "{\"step\": 2000, \"total_reward\": 2662}\n",
      "{\"step\": 2250, \"total_reward\": 3014}\n",
      "{\"step\": 2500, \"total_reward\": 3400}\n",
      "{\"step\": 2750, \"total_reward\": 3870}\n",
      "{\"step\": 3000, \"total_reward\": 4250}\n",
      "{\"step\": 3250, \"total_reward\": 4634}\n",
      "{\"step\": 3500, \"total_reward\": 5064}\n",
      "{\"step\": 3750, \"total_reward\": 5382}\n",
      "{\"step\": 4000, \"total_reward\": 5790}\n",
      "{\"step\": 4250, \"total_reward\": 6278}\n",
      "{\"step\": 4500, \"total_reward\": 6666}\n",
      "{\"step\": 4750, \"total_reward\": 7174}\n",
      "{\"step\": 5000, \"total_reward\": 7664}\n",
      "{\"step\": 5250, \"total_reward\": 8280}\n",
      "{\"step\": 5500, \"total_reward\": 8810}\n",
      "{\"step\": 5750, \"total_reward\": 9342}\n",
      "{\"step\": 6000, \"total_reward\": 9976}\n",
      "{\"step\": 6250, \"total_reward\": 10752}\n",
      "{\"step\": 6500, \"total_reward\": 11402}\n",
      "{\"step\": 6750, \"total_reward\": 12190}\n",
      "{\"step\": 7000, \"total_reward\": 12828}\n",
      "{\"step\": 7250, \"total_reward\": 13698}\n",
      "{\"step\": 7500, \"total_reward\": 14702}\n",
      "{\"step\": 7750, \"total_reward\": 15764}\n",
      "{\"step\": 8000, \"total_reward\": 16752}\n",
      "{\"step\": 8250, \"total_reward\": 17724}\n",
      "{\"step\": 8500, \"total_reward\": 18522}\n",
      "{\"step\": 8750, \"total_reward\": 19650}\n",
      "{\"step\": 9000, \"total_reward\": 20634}\n",
      "{\"step\": 9250, \"total_reward\": 21558}\n",
      "{\"step\": 9500, \"total_reward\": 22678}\n",
      "{\"step\": 9750, \"total_reward\": 23638}\n"
     ]
    }
   ],
   "source": [
    "for step in range(iterations):\n",
    "    old_state = dungeon.state\n",
    "    action = agent.get_next_action(old_state)\n",
    "    new_state, reward = dungeon.take_action(action)\n",
    "    agent.update(old_state, new_state, action, reward)\n",
    "    \n",
    "    total_reward += reward\n",
    "    if step % 250 == 0:\n",
    "        print(json.dumps({\"step\": step, \"total_reward\": total_reward}))\n",
    "        \n",
    "    time.sleep(0.00001) # Avoid spamming stdout too fast (why?)\n",
    "    \n",
    "#print(\"Final Q-table:\", agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
