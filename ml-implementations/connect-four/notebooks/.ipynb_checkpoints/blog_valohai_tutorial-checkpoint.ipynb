{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original article\n",
    "https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/dungeon-simulator/dungeon_simulator.py\n",
    "import random\n",
    "\n",
    "\n",
    "class DungeonSimulator:\n",
    "\tdef __init__(self, length=5, slip=0.1, small=2, large=10):\n",
    "\t\tself.length = length\n",
    "\t\tself.slip = slip # Probability of flipping the action\n",
    "\t\tself.small = small\n",
    "\t\tself.large = large\n",
    "\t\tself.state = 0\n",
    "\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t''' Executes the action and returns the next state and the received reward.'''\n",
    "\t\tif random.random() < self.slip:\n",
    "\t\t\taction = not action\n",
    "\t\treward = 0\n",
    "\t\tif action == BACKWARD:\n",
    "\t\t\treward = self.small\n",
    "\t\t\tself.state = 0\n",
    "\t\telif action == FORWARD:\n",
    "\t\t\tif self.state < self.length - 1:\n",
    "\t\t\t\tself.state += 1\n",
    "\t\t\t\treward = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\treward = self.large\n",
    "\n",
    "\t\treturn self.state, reward\n",
    "\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.state = 0\n",
    "\t\treturn self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Drunkard:\n",
    "    def __init__(self):\n",
    "        self.q_table = None\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        # Random walk\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        pass # I don't care! I'm drunk!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Accountant:\n",
    "    def __init__(self):\n",
    "        # Spreadsheet (Q-table) for rewards accounting\n",
    "        self.q_table = [\n",
    "            [ 0, 0, 0, 0, 0 ], # FORWARD states\n",
    "            [ 0, 0, 0, 0, 0 ] # BACKWARD states\n",
    "        ]\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        # Is FORWARD reward bigger?\n",
    "        if self.q_table[FORWARD][state] > self.q_table[BACKWARD][state]:\n",
    "            return FORWARD\n",
    "        elif self.q_table[BACKWARD][state] > self.q_table[FORWARD][state]:\n",
    "            return BACKWARD\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.q_table[action][old_state] += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Gambler:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10000):\n",
    "        self.q_table = [\n",
    "            [ 0, 0, 0, 0, 0 ], # FORWARD states\n",
    "            [ 0, 0, 0, 0, 0 ] # BACKWARD states\n",
    "        ]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations # Shift from exploration to exploitation\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate:\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action()\n",
    "        \n",
    "    def greedy_action(self, state):\n",
    "        if self.q_table[FORWARD][state] > self.q_table[BACKWARD][state]:\n",
    "            return FORWARD\n",
    "        elif self.q_table[BACKWARD][state] > self.q_table[FORWARD][state]:\n",
    "            return BACKWARD\n",
    "        \n",
    "        return self.random_action()\n",
    "    \n",
    "    def random_action(self):\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Old Q-table value\n",
    "        old_value = self.q_table[action][old_state]\n",
    "        future_action = self.greedy_action(new_state)\n",
    "        future_reward = self.q_table[future_action][new_state]\n",
    "        \n",
    "        # Main Q-table updating algorithm\n",
    "        new_value = old_value + self.learning_rate * (reward + self.discount * future_reward - old_value)\n",
    "        self.q_table[action][old_state] = new_value\n",
    "        \n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepGambler:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10_000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / 10_000\n",
    "        \n",
    "        # Input has five neurons, each represents a single game state\n",
    "        self.input_count = 5\n",
    "        # Output is two neurons, each represents a Q-value for each action\n",
    "        self.output_count = 2\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "        \n",
    "    def define_model(self):\n",
    "        '''Define tensorflow model graph.'''\n",
    "        # Input is an array of 5 items (states one-hot encoded)\n",
    "        # Input is 2-dimensional due to possibility of batched training data (why does this change the input?)\n",
    "        # NOTE: In this example we assume no batching\n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[ None, self.input_count ])\n",
    "        \n",
    "        # Two hidden layers of 16 neurons with sigmoid activation initialized to zero for stability\n",
    "        fc1 = tf.layers.dense(self.model_input, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((self.input_count, 16))))\n",
    "        fc2 = tf.layers.dense(fc1, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((16, self.output_count))))\n",
    "        \n",
    "        # Output is two values, Q for both possible actions (FORWARD and BACKWARD)\n",
    "        # Output is 2-dimensional, due to possibility of batched training data (again, why??)\n",
    "        self.model_output = tf.layers.dense(fc2, self.output_count)\n",
    "        \n",
    "        # This is for feeding training output (a.k.a ideal target values)\n",
    "        self.target_output = tf.placeholder(shape=[ None, self.output_count ], dtype=tf.float32)\n",
    "        # Loss is mean squared difference between current output and ideal target values\n",
    "        loss = tf.losses.mean_squared_error(self.target_output, self.model_output)\n",
    "        # Optimizer adjusts weights to minimize loss, with the speed of the learning rate\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        # Initializer sets weights to initial values\n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "        \n",
    "    def get_Q(self, state):\n",
    "        '''Ask model to estimate Q value for specific state (via inference).'''\n",
    "        # Model input: Single state represented by array of 5 items (state one-hot encoded)\n",
    "        # Model output: Array of Q values for single state\n",
    "        return self.session.run(self.model_output, feed_dict={ self.model_input: self.to_one_hot(state) })[0]\n",
    "    \n",
    "    def to_one_hot(self, state):\n",
    "        '''Turn state into 2d one_hot tensor (e.g. 3 -> [[ 0, 0, 0, 1, 0]]).'''\n",
    "        one_hot = np.zeros((1, 5))\n",
    "        one_hot[0, [ state ]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate: # Exploit\n",
    "            return self.greedy_action(state)\n",
    "        else: # Explore\n",
    "            return self.random_action()\n",
    "        \n",
    "    def greedy_action(self, state):\n",
    "        '''Returns the action with the bigger Q-value, as estimated by our model (via inference)'''\n",
    "        return np.argmax(self.get_Q(state))\n",
    "    \n",
    "    def random_action(self):\n",
    "        return FORWARD if random.random() < 0.5 else BACKWARD\n",
    "    \n",
    "    def train(self, old_state, action, reward, new_state):\n",
    "        # Ask the model for the Q values of the old state\n",
    "        old_state_Q_values = self.get_Q(old_state)\n",
    "        # Ask the model for the Q values of the new state\n",
    "        new_state_Q_values = self.get_Q(new_state)\n",
    "        # Change the Q value of the action we took to what we expect (so we can train towards our expected Q values)\n",
    "        old_state_Q_values[action] = reward + self.discount * np.amax(new_state_Q_values)\n",
    "        \n",
    "        # Set up training data\n",
    "        training_input = self.to_one_hot(old_state)\n",
    "        target_output = [ old_state_Q_values ]\n",
    "        training_data = { self.model_input: training_input, self.target_output: target_output }\n",
    "        \n",
    "        # Train\n",
    "        self.session.run(self.optimizer, feed_dict=training_data)\n",
    "        \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Train our model with new data\n",
    "        self.train(old_state, action, reward, new_state)\n",
    "            \n",
    "        # Shift our exploration_rate toward zero\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 16)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepPytorchGambler:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 5\n",
    "        self.output_count = 2\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state):\n",
    "        x = torch.tensor(self.to_one_hot(state)).to(self.device)\n",
    "        return self.model(x)\n",
    "        \n",
    "    def to_one_hot(self, state):\n",
    "        '''Turn state into 2d one_hot tensor (e.g. 3 -> [[ 0, 0, 0, 1, 0]]).'''\n",
    "        one_hot = np.zeros((1, 5))\n",
    "        one_hot[0, [ state ]] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.randrange(0, 5) # Maybe change the probability distribution?\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        #print(\"Greedy1:\", torch.max(self.get_Q(state), 0)[0])\n",
    "        #print(\"Greedy2:\", torch.max(self.get_Q(state), 0)[1])\n",
    "        return torch.max(self.get_Q(state), 0)[1]\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        old_state_values = self.get_Q(old_state)\n",
    "        new_state_values = self.get_Q(new_state)\n",
    "        \n",
    "        new_reward = reward + self.discount * torch.max(new_state_values)\n",
    "        updated_state_values = torch.tensor(old_state_values)\n",
    "        updated_state_values[action] = new_reward\n",
    "        \n",
    "        \n",
    "        old_state_values = torch.tensor(old_state_values, device=self.device).float()\n",
    "        updated_state_values = torch.tensor(updated_state_values, device=self.device).float()\n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = torch.autograd.Variable(F.smooth_l1_loss(old_state_values, updated_state_values), requires_grad=True)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD = 0\n",
    "BACKWARD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--agent\", type=str, default=\"GAMBLER\", help=\"Which agent to use\")\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=0.1, help=\"How quickly the algorithm tries to learn\")\n",
    "parser.add_argument(\"--discount\", type=float, default=0.95, help=\"Discount for estimated future action\") # Reward?\n",
    "parser.add_argument(\"--iterations\", type=int, default=2000, help=\"Iteration count\")\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "discount = 0.95\n",
    "iterations = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a091fa90e091>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c017bbe78b81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Seems to vary quite a bit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepGambler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepPytorchGambler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-f442336dfaf1>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, learning_rate, discount, exploration_rate, iterations, trained_model)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f442336dfaf1>\u001b[0m in \u001b[0;36mdefine_model\u001b[1;34m(self, trained_model)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "agent = Drunkard() # 12754\n",
    "agent = Accountant() # 17548\n",
    "agent = Gambler() # 25890\n",
    "\n",
    "# 21186 with learning rate 0.01 (not as good, because a deep neural net is overkill for this simple Q-table)\n",
    "# Seems to vary quite a bit\n",
    "agent = DeepGambler(learning_rate=learning_rate) \n",
    "agent = DeepPytorchGambler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dungeon = DungeonSimulator()\n",
    "dungeon.reset()\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(iterations):\n",
    "    old_state = torch.tensor(dungeon.state)\n",
    "    action = agent.get_next_action(old_state)\n",
    "    new_state, reward = dungeon.take_action(action)\n",
    "    agent.update(old_state, new_state, action, reward)\n",
    "    \n",
    "    total_reward += reward\n",
    "    if step % 250 == 0:\n",
    "        print(json.dumps({\"step\": step, \"total_reward\": total_reward}))\n",
    "        \n",
    "    time.sleep(0.00001) # Avoid spamming stdout too fast (why?)\n",
    "    \n",
    "#print(\"Final Q-table:\", agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
