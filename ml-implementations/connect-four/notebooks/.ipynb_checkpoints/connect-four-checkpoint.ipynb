{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tactive_player = self.current_player\n",
    "\t\tinactive_player = self.__negated_player(active_player)\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, np.copy(self.board), active_player, -2, inactive_player, 0\n",
    "\n",
    "\t\tself.__play_move(action)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(action)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(action)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, np.copy(self.board), active_player, 0, inactive_player, 0\n",
    "\t\t\telif winner == self.PLAYER1:\n",
    "\t\t\t\treturn self.__game_over, np.copy(self.board), active_player, 1000, inactive_player, -100\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn self.__game_over, np.copy(self.board), active_player, -100, inactive_player, 1000\n",
    "\n",
    "\t\treturn self.__game_over, np.copy(self.board), active_player, 0, inactive_player, 0\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\t#print(self.board)\n",
    "\t\tboard = self.board\n",
    "\t\tboard = np.where(board == 1, \"X\", board)\n",
    "\t\tboard = np.where(board == \"-1.0\", \"O\", board)\n",
    "\t\tprint(np.where(board == \"0.0\", \"-\", board))\n",
    "\n",
    "\tdef __play_move(self, action):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\tif 41 > action < 0:\n",
    "\t\t\treturn False\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tif x >= self.width or y >= self.height:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\theight_x = self.__column_height(x)\n",
    "\n",
    "\t\tif y != height_x:\n",
    "\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, last_action):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) >= 42:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, last_action):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(last_action)\n",
    "\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, last_action):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\tx = action % self.width\n",
    "\t\ty = action // self.width\n",
    "\t\treturn x, y\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 64)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(64, 42)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepQPytorchAgent:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state):\n",
    "        x = torch.tensor(state.flatten(), device=self.device).float()\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_Q_batch(self, states):\n",
    "        #x = torch.tensor(state.flatten(), device=self.device).float()\n",
    "        return self.model(states)\n",
    "        \n",
    "    def next_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.randrange(0, 42) # Maybe change the probability distribution?\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        #print(\"Greedy1:\", torch.max(self.get_Q(state), 0)[0])\n",
    "        #print(\"Greedy2:\", torch.max(self.get_Q(state), 0)[1])\n",
    "        return torch.max(self.get_Q(state), 0)[1]\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def update_batch(self, old_states, new_states, actions, rewards):\n",
    "        self.train_batch(old_states, new_states, actions, rewards)\n",
    "        \n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.05, self.exploration_rate - self.exploration_delta)\n",
    "\n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        #print(\"OldState:\", old_state)\n",
    "        #print(\"NewState:\", new_state)\n",
    "        #print(\"Reward:\", reward)\n",
    "        old_state_values = self.get_Q(old_state)\n",
    "        new_state_values = self.get_Q(new_state).detach()\n",
    "        #print(\"OldQ:\", old_state_values)\n",
    "        #print(\"NewQ:\", new_state_values)\n",
    "        \n",
    "        new_reward = reward + self.discount * torch.max(new_state_values)\n",
    "        #print(\"NewReward:\", new_reward)\n",
    "        updated_state_values = old_state_values.clone().detach() # Check if detach could cause problems\n",
    "        #print(\"BeforeUpdate:\", updated_state_values)\n",
    "        #print(\"Updating action:\", action)\n",
    "        updated_state_values[action] = new_reward\n",
    "        #print(\"OldAfterUpdate:\", old_state_values)\n",
    "        #print(\"UpdAfterUpdate:\", updated_state_values)\n",
    "        \n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        #loss = torch.autograd.Variable(F.smooth_l1_loss(old_state_values, updated_state_values), requires_grad=True)\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update\n",
    "        \n",
    "    def train_batch(self, old_states, new_states, actions, rewards):\n",
    "        old_state_values = self.get_Q_batch(old_states).gather(1, actions)\n",
    "        new_state_values = torch.zeros(len(new_states), device=self.device)\n",
    "        new_state_values = self.get_Q_batch(new_states).max(1)[0].detach()\n",
    "        \n",
    "        # Expected values\n",
    "        updated_state_values = rewards + (self.discount * next_state_values)\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for param in policy_net.parameters():\n",
    "        #    param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class DeepQTensorflowAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10_000):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "    \n",
    "    def define_model(self):\n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[ None, self.input_count ])\n",
    "        \n",
    "        fc1 = tf.layers.dense(self.model_input, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((self.input_count, 5))))\n",
    "        fc2 = tf.layers.dense(fc1, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((6, self.output_count))))\n",
    "        \n",
    "        self.model_output = tf.layers.dense(fc2, self.output_count)\n",
    "        \n",
    "        self.target_output = tf.placeholder(shape=[ None, self.output_count ], dtype=tf.float32)\n",
    "        loss = tf.losses.mean_squared_error(self.target_output, self.model_output)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "    \n",
    "    def get_Q(self, state):\n",
    "        # Batching!! Dimensions!\n",
    "        return self.session.run(self.model_output, feed_dict={ self.model_input: [state.flatten()] })[0]\n",
    "        \n",
    "    def next_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.randrange(0, 42) # Maybe change the probability distribution?\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        return np.argmax(self.get_Q(state))\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.05, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        old_state_values = self.get_Q(old_state)\n",
    "        new_state_values = self.get_Q(new_state)\n",
    "        \n",
    "        new_reward = reward + self.discount * np.amax(new_state_values)\n",
    "        old_state_values[action] = new_reward\n",
    "        \n",
    "        training_input = [old_state.flatten()]\n",
    "        target_output = [ old_state_values ]\n",
    "        training_data = { self.model_input: training_input, self.target_output: target_output }\n",
    "        \n",
    "        self.session.run(self.optimizer, feed_dict=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('old_state', 'next_state', 'action', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active, passive):\n",
    "    old_state = np.copy(game.board)\n",
    "    action = active.next_action(old_state)\n",
    "    \n",
    "    game_over, new_state, _, reward, _, _ = game.take_action(action)\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, new_state, action, reward\n",
    "            \n",
    "    # if the move was invalid, add data and repeat\n",
    "    if reward < 0:\n",
    "        return False, old_state, new_state, action, reward\n",
    "        \n",
    "    # Play another move until the move is a right one and add the data to the memory\n",
    "    passive_reward = -1\n",
    "    counting_stars = 0\n",
    "    while passive_reward < 0:\n",
    "        passive_action = passive.next_action(new_state)\n",
    "        game_over, _, _, passive_reward, _, cur_reward = game.take_action(passive_action)\n",
    "        \n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "                print(\"Counting:\", counting_stars)\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, new_state, action, cur_reward\n",
    "    return False, old_state, new_state, action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, passive, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return active, passive\n",
    "    \n",
    "    '''batch = memory.sample(batch_size)\n",
    "    # https://stackoverflow.com/a/19343/3343043 --> Batch of transition to transition of batches\n",
    "    batch = Transition(*zip(*batch))\n",
    "    \n",
    "    old_state_batch = torch.cat(batch.old_state)\n",
    "    #print(old_state_batch)\n",
    "    #print(batch.next_state)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    #print(next_state_batch)\n",
    "    #action_batch = torch.cat(batch.action)\n",
    "    #reward_batch = torch.cat(batch.reward)\n",
    "    action_batch = torch.stack(batch.action, dim=0)\n",
    "    reward_batch = torch.stack(batch.reward, dim=0)\n",
    "    \n",
    "    #for transition in batch:\n",
    "    #    active.update(*transition)\n",
    "    active.update_batch(old_state_batch, next_state_batch, action_batch, reward_batch)\n",
    "        \n",
    "    model = active.model\n",
    "    active = DeepQPytorchAgent(iterations=iterations, exploration_rate=active.exploration_rate, trained_model=model)\n",
    "    passive = DeepQPytorchAgent(iterations=iterations, exploration_rate=active.exploration_rate, trained_model=model)'''\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    old_state_batch = torch.cat(batch.old_state)\n",
    "    action_batch = torch.stack(batch.action, dim=0)\n",
    "    \n",
    "    \n",
    "    # Expected values\n",
    "    #updated_state_values = rewards + (self.discount * next_state_values)\n",
    "    loss = F.smooth_l1_loss(old_state_batch, action_batch.unsqueeze(1))\n",
    "        \n",
    "    active.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # for param in policy_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    active.optimizer.step()\n",
    "        \n",
    "    \n",
    "    return active, passive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, passive, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    batch = memory.sample(batch_size)\n",
    "    \n",
    "    for transition in batch:\n",
    "        active.update(*transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    game.print_board()\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Input:\", input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 20_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#deep_Q_learning = DeepQPytorchAgent(iterations=iterations)\n",
    "#deep_Q_dummy = DeepQPytorchAgent(iterations=iterations)\n",
    "\n",
    "deep_Q_learning = DeepQPytorchAgent(iterations=iterations)\n",
    "deep_Q_dummy = DeepQTensorflowAgent(iterations=iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "val = torch.tensor(0).to(device)\n",
    "print(val)\n",
    "print(game.board)\n",
    "game.take_action(val)\n",
    "print(game.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3 # Number of games to play\n",
    "batch_size = 64\n",
    "memory = ReplayMemory(10000)\n",
    "active = DeepQPytorchAgent(iterations=epochs*batch_size*20)\n",
    "passive = DeepQPytorchAgent(iterations=epochs*batch_size*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0], [0], [0], [0], [0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [[0]]*6\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].append(2)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor([ 0.1056, -0.3761,  0.1901, -0.1401,  0.2217, -0.2116,  0.2512, -0.0119,\n",
      "        -0.0270, -0.2430, -0.2196, -0.1786, -0.7201,  0.0084, -0.0861,  0.1246,\n",
      "        -0.0944, -0.2649, -0.4163,  0.0134,  0.3990, -0.3819,  0.0832,  0.2741,\n",
      "        -0.7078, -0.2675, -0.0077, -0.4094,  0.3806,  0.2224,  0.1599,  0.3160,\n",
      "        -0.1265, -0.3721,  0.2548,  0.2171, -0.0918, -0.0919,  0.2539, -0.3113,\n",
      "         0.3727, -0.1222], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(20, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.get_Q(example_board), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [0]]\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Time taken: 187490\n",
      "Time taken in sec: 187.49\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using memory replay\n",
    "total_rewards = [ [0] for epoch in range(epochs) ]\n",
    "total = [0]\n",
    "print(total_rewards)\n",
    "start = int(round(time.time() * 1000))\n",
    "for epoch in range(epochs):\n",
    "    #invalids = []\n",
    "    #invalid = 0\n",
    "    #for iteration in range(1, iterations + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    game_over = False\n",
    "    game = ConnectFourSimulator()\n",
    "    while not game_over:\n",
    "        optimize_model(active, passive, memory, batch_size)\n",
    "        passive.model.load_state_dict(active.model.state_dict())\n",
    "            \n",
    "\n",
    "        #if iteration % 100 == 0:\n",
    "        #    invalids.append(invalid)\n",
    "        #    invalid = 0\n",
    "        #if iteration % 250 == 0:\n",
    "        #    print(\"Iteration:\", iteration)\n",
    "                      \n",
    "        game_over, old_state, next_state, action, reward = transition(game, active, passive)\n",
    "        \n",
    "        #old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "        #next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "        #action = torch.tensor(action, device=active.device).float()\n",
    "        #reward = torch.tensor(reward, device=active.device).float()\n",
    "        #if reward != -2:\n",
    "        memory.push(old_state, next_state, action, reward)\n",
    "        total_rewards[epoch].append(total_rewards[epoch][-1] + reward)\n",
    "        total.append(total[-1] + reward)\n",
    "end = int(round(time.time() * 1000))\n",
    "print(\"Time taken:\", (end - start))\n",
    "print(\"Time taken in sec:\", (end - start) / 1000)\n",
    "# Time without batching: 657sec (10), 95 (5), 399 (5)\n",
    "# Time with batching: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "40\n",
      "[0, -2, -2, -4, -6, -8, -10, -12, -14, -16, -16, -18, -18, -18, -20, -22, -24, -26, -28, -30, -32, -34, -36, -38, -40, -42, -44, -44, -46, -48, -50, -52, -54, -56, -58, -60, -62, -62, -64, 36]\n"
     ]
    }
   ],
   "source": [
    "print(len(total_rewards))\n",
    "print(len(total_rewards[0]))\n",
    "print(total_rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "[0, -2, -2, -4, -6, -8, -10, -12, -14, -16, -16, -18, -18, -18, -20, -22, -24, -26, -28, -30, -32, -34, -36, -38, -40, -42, -44, -44, -46, -48, -50, -52, -54, -56, -58, -60, -62, -62, -64, 36, 36, 34, 32, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8, 8, 6, 4, 2, 0, -2, -2, -4, -4, -6, -6, -8, -10, -12, -14, -16, -18, 82, 82, 80, 78, 76, 74, 74, 72, 70, 70, 68, 66, 64, 62, 60, 58, 58, 56, 54, 52, 50, 50, 48, 46, 44, 44, 42, 40, 38, 36, 34, 32, 30, 28, 28, 26, 24, 22, 20, 20, 18, 16, 14, 14, 12, 10, 10, 8, 6, 4, 2, 0, -2, -4, -6, -8, -10, -12, -14, -16, -18, -20, -22, -24, -26, -28, -30, -32, -34, -36, -38, -40, -42, -44, -46, -48, -50, -52, -54, -56, -58, -60, -62, -64, -66, -68, -70, -72, -74, -76, -78, -80, -80, -82, -84, -86, -86, -88, -90, -92, -94, -96, -98, -100, -102, -104, -106, -108, -110, -112, -112, -114, -116, -118, -120, -122, -124, -126, -128, -130, -132, -134, -136, -138, -140, -142, -144, -146, -148, -150, -152, -154, -156, -158, -160, -162, -164, -166, -168, -170, -172, -174, -176, -178, -180, -182, -184, -186, -188, -190, -192, -194, -196, -198, -200, -200, -202, -204, -206, -208, -210, -212, -214, -216, -218, -220, -222, -224, -226, -228, -230, -232, -234, -236, -238, -240, -242, -244, -246, -248, -250, -252, -254, -256, -258, -260, -262, -264, -266, -268, -270, -272, -274, -274, -276, -278, -280, -282, -284, -286, -288, -290, -292, -294, -296, -298, -300, -302, -304, -306, -308, -310, -312, -314, -314, -316, -318, -320, -322, -324, -326, -328, -330, -332, -334, -336, -338, -340, -342, -344, -346, -348, -350, -352, -352, -354, -356, -358, -360, -362, -364, -366, -368, -370, -372, -374, -376, -378, -380, -382, -384, -386, -388, -390, -392, -394, -396, -398, -400, -300]\n"
     ]
    }
   ],
   "source": [
    "print(len(total))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20aa8908>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot(total_rewards[0])\n",
    "plt.plot(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(invalids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invalids = []\n",
    "invalid = 0\n",
    "for step in range(10000):\n",
    "    if step % 250 == 0:\n",
    "        print(\"Still running at iteration\", step)\n",
    "        \n",
    "    \n",
    "    old_state = game.board\n",
    "    action = deep_Q_learning.next_action(old_state)\n",
    "    game_over, new_state, cur_player, cur_reward, _, _ = game.take_action(action)\n",
    "    \n",
    "    if (step + 1) % 100 == 0:\n",
    "            invalids.append(invalid)\n",
    "            invalid = 0\n",
    "    if cur_reward < 0: # Invalid action\n",
    "        deep_Q_learning.update(old_state, new_state, action, cur_reward)\n",
    "        invalid += 1\n",
    "        continue\n",
    "        \n",
    "    #print(game_over)\n",
    "    #print(new_state)\n",
    "    #print(cur_player)\n",
    "    #print(cur_reward)\n",
    "    \n",
    "    if game_over:\n",
    "        deep_Q_learning.update(old_state, new_state, action, cur_reward)\n",
    "        game = ConnectFourSimulator()\n",
    "        continue\n",
    "    \n",
    "    next_action = deep_Q_dummy.next_action(new_state)\n",
    "    game_over, next_state, _, active_reward, passive_player, passive_reward = game.take_action(action)\n",
    "    \n",
    "    counting_stars = 0\n",
    "    while active_reward < 0: # Invalid move (infinite loop possible?)\n",
    "        next_action = deep_Q_dummy.next_action(new_state)\n",
    "        game_over, next_state, _, active_reward, passive_player, passive_reward = game.take_action(next_action)\n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "            print(\"Counting:\", counting_stars)\n",
    "            print(\"Using action:\", action)\n",
    "    \n",
    "    #print(game_over)\n",
    "    #print(next_state)\n",
    "    #print(passive_player)\n",
    "    #print(passive_reward)\n",
    "    \n",
    "    if game_over:\n",
    "        deep_Q_learning.update(old_state, new_state, action, passive_reward)\n",
    "        game = ConnectFourSimulator()\n",
    "    else:\n",
    "        deep_Q_learning.update(old_state, new_state, action, cur_reward)\n",
    "    # Missing: check for invalid move\n",
    "    # Maybe add passive mode to game (for the 2nd player)\n",
    "    #print(\"----------------------------\")\n",
    "        \n",
    "print(\"Invalids:\", invalids)\n",
    "print(\"Total:\", sum(invalids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error in the end comes from the network predicting a result, which is wrong and since exploration is way down it almost\n",
    "# always predicts the same action which is always wrong. Should somehow learn though (maybe replay necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Use memory replay --> DONE\n",
    "- Maybe higher rewards needed for backpropagation of Q values?\n",
    "- View reward function by playing vs the network\n",
    "- View network output for certain states\n",
    "<br>\n",
    "<br>\n",
    "- Do I even backpropagate the reward to other states than the winning one in any way?\n",
    "- Maybe the problem are few games (not enough possibilities learned) -> More iterations like 10_000 games instead of iterations\n",
    "- Learning rate?\n",
    "<br>\n",
    "<br>\n",
    "- Rework memory replay batch size and epochs analog to pytorch tutorial\n",
    "- Plot metrics (e.g. total reward every iteration)\n",
    "- Rework code --> Readability and reusability\n",
    "- Maybe rework greedy policy\n",
    "- Test the pytorch agent on the dungeon example --> DONE: Works\n",
    "- Try increasing the performance (For running in the cloud) -> Use timer\n",
    "- Maybe no punishment for invalid moves?\n",
    "- Pass possible moves to network?\n",
    "- Only give out copies of the state... --> FIXED (This literally ruined every single state in the memory...)\n",
    "- Copy pytorch tensors via .copy().detach() (maybe more effectively possible as well?)\n",
    "<br>\n",
    "<br>\n",
    "- How to choose rewards and how does the agent learn the rules (punishment for invalid moves?)\n",
    "- How much training is needed for a game?\n",
    "- Evaluation tactics: Total reward\n",
    "- Model too big?\n",
    "- Don't copy model to update agent? --> Constantly creating optimizer and agent again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_board = np.zeros(shape=(6, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board2 = np.array([\n",
    "    [0,0,1,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "tensor([-13.0249, -12.8161, -12.8495, -12.8286, -12.7423, -12.7911, -12.3781,\n",
      "        -12.6096, -12.8508, -12.5369, -12.5844, -12.3680, -12.7838, -13.2813,\n",
      "        -12.4085, -12.9579, -13.5792, -12.6219, -12.9588, -12.5184, -12.3944,\n",
      "        -13.2534, -12.9007, -13.0470, -12.8450, -12.6577, -12.7453, -12.3042,\n",
      "        -12.5059, -12.7056, -13.1666, -13.2435, -12.9010, -13.4268, -12.7798,\n",
      "        -13.5422, -12.2493, -13.4611, -13.2215, -12.3743, -13.1867, -13.0753],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.get_Q(example_board), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36, device='cuda:0')\n",
      "tensor([-13.0265, -12.8177, -12.8511, -12.8302, -12.7438, -12.7926, -12.3796,\n",
      "        -12.6112, -12.8525, -12.5384, -12.5860, -12.3695, -12.7853, -13.2829,\n",
      "        -12.4100, -12.9595, -13.5807, -12.6235, -12.9604, -12.5199, -12.3959,\n",
      "        -13.2550, -12.9023, -13.0485, -12.8466, -12.6592, -12.7468, -12.3057,\n",
      "        -12.5074, -12.7070, -13.1681, -13.2452, -12.9026, -13.4283, -12.7813,\n",
      "        -13.5438, -12.2508, -13.4627, -13.2231, -12.3758, -13.1882, -13.0770],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board2))\n",
    "print(active.get_Q(example_board2))\n",
    "print(torch.max(active.get_Q(example_board2), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36, device='cuda:0')\n",
      "tensor([-13.0244, -12.8156, -12.8489, -12.8281, -12.7417, -12.7906, -12.3776,\n",
      "        -12.6091, -12.8503, -12.5364, -12.5839, -12.3675, -12.7833, -13.2807,\n",
      "        -12.4079, -12.9574, -13.5787, -12.6214, -12.9583, -12.5179, -12.3939,\n",
      "        -13.2529, -12.9002, -13.0464, -12.8445, -12.6572, -12.7448, -12.3037,\n",
      "        -12.5054, -12.7051, -13.1660, -13.2430, -12.9005, -13.4262, -12.7793,\n",
      "        -13.5416, -12.2488, -13.4605, -13.2210, -12.3737, -13.1862, -13.0748],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(empty_board))\n",
    "print(active.get_Q(empty_board))\n",
    "print(torch.max(active.get_Q(empty_board), 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game = ConnectFourSimulator()\n",
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    if not game_over:\n",
    "        confirmation = \"r\"\n",
    "        while confirmation == \"r\":\n",
    "            pc_action = active.next_action(board)\n",
    "            print(pc_action)\n",
    "            confirmation = input()\n",
    "            if confirmation == \"c\":\n",
    "                game_over, board, _, _, _, _ = game.take_action(pc_action)\n",
    "                print(game_over)\n",
    "                print(board)\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = memory.sample(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = [ game for game in games if game[3] == 100 or game[3] == -100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " [[ 0. -1. -1.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1. -1.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  1.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 11\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 1. -1. -1.  1.  1.  1.  1.]\n",
      " [-1.  1.  1. -1. -1.  1. -1.]\n",
      " [-1. -1. -1.  1.  1.  1.  1.]\n",
      " [-1. -1.  1.  0. -1. -1.  1.]\n",
      " [-1.  1. -1.  0.  1.  0. -1.]\n",
      " [ 0.  0.  0.  0. -1.  0.  1.]]\n",
      "After:\n",
      " [[ 1. -1. -1.  1.  1.  1.  1.]\n",
      " [-1.  1.  1. -1. -1.  1. -1.]\n",
      " [-1. -1. -1.  1.  1.  1.  1.]\n",
      " [-1. -1.  1.  1. -1. -1.  1.]\n",
      " [-1.  1. -1.  0.  1.  0. -1.]\n",
      " [ 0.  0.  0.  0. -1.  0.  1.]]\n",
      "With action: 24\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0.  1.  1.  0.  1. -1. -1.]\n",
      " [ 0.  0.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0. -1.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0. -1.  0.  0.  1.  1.]\n",
      " [ 0.  0. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 3\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for state in terminal_states:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "After:\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "With action: 39\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "After:\n",
      " [[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "With action: 3\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 3\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 13\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 26\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 36\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 33\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 35\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 1\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 8\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 34\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 4\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 10\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 8\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 16\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 28\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 39\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 33\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 38\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 24\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 33\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 16\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 30\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 30\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 25\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 37\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 29\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 29\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 1\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 15\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 40\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 28\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 35\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 41\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 10\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 14\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1.  0.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 12\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1. -1.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1. -1.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 7\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0. -1. -1.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0. -1. -1.  1.  1. -1.  0.]\n",
      " [ 0.  1.  0.  1.  1.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]]\n",
      "With action: 11\n",
      "Reward: 100\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "After:\n",
      " [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "With action: 2\n",
      "Reward: 0\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 10\n",
      "Reward: -2\n",
      "---------------------------------------\n",
      "Before:\n",
      " [[ 0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "After:\n",
      " [[ 0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "With action: 29\n",
      "Reward: -2\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for state in memory.memory[:42]:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"Reward:\", state[3])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
