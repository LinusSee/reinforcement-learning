{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.valid_actions = list(range(self.width))\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tassert not self.__game_over, \"Game is already finished.\"\n",
    "\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, self.board, (-2, 0)\n",
    "\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.__play_move(x, y)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(x, y)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(x, y)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, self.board, (0, 0)\n",
    "\t\t\treturn self.__game_over, self.board, (1, -1)\n",
    "\n",
    "\t\treturn self.__game_over, self.board, (0, 0)\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\tboard = self.board\n",
    "\t\tboard = np.where(board == 1, \"X\", board)\n",
    "\t\tboard = np.where(board == \"-1.0\", \"O\", board)\n",
    "\t\tprint(np.where(board == \"0.0\", \"-\", board))\n",
    "\n",
    "\tdef __play_move(self, x, y):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\tis_valid = action in self.valid_actions\n",
    "\n",
    "\t\tif is_valid:\n",
    "\t\t\tif self.__column_height(action) >= (self.height - 1):\n",
    "\t\t\t\tself.valid_actions.remove(action)\n",
    "\t\treturn is_valid\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, x, y):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) >= 42:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(x, y)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, x, y):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, x, y):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(x, y)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\treturn action, self.__column_height(action)\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        self.fc3 = nn.Linear(120, 120)\n",
    "        self.fc4 = nn.Linear(120, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepQPytorchAgent:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 7\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state_batch):\n",
    "        return self.model(state_batch)\n",
    "        \n",
    "    def next_action(self, state, valid_actions):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action(valid_actions)\n",
    "        else:\n",
    "            return self.greedy_action(state, valid_actions)\n",
    "        \n",
    "    def random_action(self, valid_actions):\n",
    "        action = random.randrange(0, len(valid_actions))\n",
    "        return valid_actions[action]\n",
    "    \n",
    "    def greedy_action(self, state_batch, valid_actions):\n",
    "        Q_values = self.get_Q(state_batch)[0]\n",
    "        Q_values = self.normalized_Q(Q_values, valid_actions, -1.0)\n",
    "        action = torch.max(Q_values, 0)[1]\n",
    "        assert action in valid_actions, \"Only valid actions may be selected, action {} with state\\n{}\\n and Q values\\n{}\\n and valid actions\\n{}\".format(action, state_batch.view((6, 7)), Q_values.view((6, 7)), valid_actions)\n",
    "        return action\n",
    "    \n",
    "    def normalized_Q(self, Q_values, valid_actions, normalize_value=-1.0):\n",
    "        '''Takes a single Q value array and sets invalid actions to the given normalize_value.'''\n",
    "        for x in range(0, self.output_count):\n",
    "            if not x in valid_actions:\n",
    "                Q_values[x] = normalize_value\n",
    "        return Q_values\n",
    "        \n",
    "    def update(self, old_states, new_states, actions, rewards, valid_actions_batch):\n",
    "        self.train(old_states, new_states, actions, rewards, valid_actions_batch)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_states, next_states, actions, rewards, valid_actions_batch):\n",
    "        old_state_values = self.get_Q(old_states)\n",
    "        next_state_values = self.get_Q(next_states).detach()\n",
    "        \n",
    "        for x in range(len(next_state_values)):\n",
    "            valid_actions = valid_actions_batch[x]\n",
    "            next_state_values[x] = self.normalized_Q(next_state_values[x], valid_actions)\n",
    "            \n",
    "        new_rewards = rewards + self.discount * torch.max(next_state_values, dim=1)[0]\n",
    "        updated_state_values = old_state_values.clone().detach() # Check if detach could cause problems\n",
    "        for index, (reward, action) in enumerate(zip(new_rewards, actions)):\n",
    "            updated_state_values[index][action] = reward\n",
    "        \n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff 3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"stuff {}\".format(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('old_state', 'next_state', 'action', 'reward', 'valid_actions'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active, passive):\n",
    "    old_state = np.copy(game.board)\n",
    "    old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "    valid_actions = torch.tensor(game.valid_actions, device=active.device).clone().long()\n",
    "    \n",
    "    action = active.next_action(torch.unsqueeze(old_state, dim=0), valid_actions)\n",
    "    #action = torch.tensor(action, device=active.device).long()\n",
    "    \n",
    "    game_over, next_state, _, reward, _, _ = game.take_action(action)\n",
    "    next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "    reward = torch.tensor(reward, device=active.device).float()\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, reward, valid_actions\n",
    "            \n",
    "    # if the move was invalid, add data and repeat\n",
    "    if reward < 0:\n",
    "        print(\"Invalid reward, shouldn't even be possible anymore!!\")\n",
    "        return False, old_state, next_state, action, reward, valid_actions\n",
    "        \n",
    "    # Play another move until the move is a right one and add the data to the memory\n",
    "    passive_reward = -1\n",
    "    counting_stars = 0\n",
    "    while passive_reward < 0:\n",
    "        passive_action = passive.next_action(torch.unsqueeze(next_state, dim=0), game.valid_actions)\n",
    "        game_over, _, _, passive_reward, _, cur_reward = game.take_action(passive_action)\n",
    "        \n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "                print(\"Counting:\", counting_stars)\n",
    "        \n",
    "    cur_reward = torch.tensor(cur_reward, device=active.device).float()\n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, cur_reward, valid_actions\n",
    "    return False, old_state, next_state, action, reward, valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active):\n",
    "    old_state = np.copy(game.board)\n",
    "    old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "    valid_actions = torch.tensor(game.valid_actions, device=active.device).clone().long()\n",
    "    \n",
    "    action = active.next_action(torch.unsqueeze(old_state, dim=0), valid_actions)\n",
    "    \n",
    "    game_over, next_state, rewards = game.take_action(action)\n",
    "    next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "    \n",
    "    if rewards[0] < 0:\n",
    "        print(\"Invalid reward, shouldn't even be possible anymore!!\")\n",
    "        \n",
    "    return game_over, old_state, next_state, action, rewards, valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    #print(\"States before:\\n\", batch.old_state)\n",
    "    #print(\"Next states before:\\n\", batch.next_state)\n",
    "    #print(\"Actions before:\\n\", batch.action)\n",
    "    #print(\"Rewards before:\\n\", batch.reward)\n",
    "    old_state_batch = torch.stack(batch.old_state, dim=0)\n",
    "    next_state_batch = torch.stack(batch.next_state, dim=0)\n",
    "    action_batch = torch.tensor(batch.action, device=active.device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=active.device)\n",
    "    #valid_actions_batch = torch.stack(batch.valid_actions, dim=0)\n",
    "    valid_actions_batch = batch.valid_actions\n",
    "    #action_batch = torch.stack(batch.action, dim=0)\n",
    "    #reward_batch = torch.stack(batch.reward, dim=0)\n",
    "    \n",
    "    return active.update(old_state_batch, next_state_batch, action_batch, reward_batch, valid_actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 # Number of games to play\n",
    "batch_size = 32\n",
    "memory = ReplayMemory(5000)\n",
    "active = DeepQPytorchAgent(iterations=epochs*20)\n",
    "#passive = DeepQPytorchAgent(iterations=epochs*batch_size*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,\n",
      "          0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "5\n",
      "tensor([[-0.0777, -0.0257, -0.0816, -0.0070, -0.0767,  0.0331,  0.0943]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(6, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_batch = torch.unsqueeze(example_board, dim=0)\n",
    "print(y_batch)\n",
    "print(active.next_action(y_batch, [0, 1, 2, 3, 4, 5, 6]))\n",
    "print(active.get_Q(y_batch))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(y_batch)[0], list(range(7))), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "total = [torch.tensor(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Epoch: 5\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Invalid reward, shouldn't even be possible anymore!!\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Time taken: 18175\n",
      "Time taken in sec: 18.175\n",
      "Avg time per episode: 1.8175000000000001\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = int(round(time.time() * 1000))\n",
    "for epoch in range(epochs):\n",
    "    total_rewards.append([torch.tensor(0)])\n",
    "    print(\"Epoch:\", epoch)\n",
    "    game_over = False\n",
    "    game = ConnectFourSimulator()\n",
    "    \n",
    "    game_over, cur_old_state, cur_next_state, cur_action, cur_reward, cur_valid_actions = transition(game, active)\n",
    "    while not game_over:\n",
    "        optimize_model(active, memory, batch_size)\n",
    "        #passive.model.load_state_dict(active.model.state_dict())\n",
    "                      \n",
    "        game_over, old_state, next_state, action, reward, valid_actions = transition(game, active)\n",
    "\n",
    "        if not game_over:\n",
    "            first_reward = torch.tensor(cur_reward[0], device=active.device).float()\n",
    "            memory.push(cur_old_state, cur_next_state, cur_action, first_reward, cur_valid_actions)\n",
    "            cur_old_state = old_state\n",
    "            cur_next_state = next_state\n",
    "            cur_action = action\n",
    "            cur_reward = reward\n",
    "            cur_valid_actions = valid_actions\n",
    "        else:\n",
    "            first_reward = torch.tensor(reward[1], device=active.device).float()\n",
    "            memory.push(cur_old_state, cur_next_state, cur_action, first_reward, cur_valid_actions)\n",
    "            \n",
    "            second_reward = torch.tensor(reward[0], device=active.device).float()\n",
    "            memory.push(old_state, next_state, action, second_reward, valid_actions)\n",
    "            \n",
    "        # Is this even a valid metric now? Both sides cancel out?!\n",
    "        # Maybe let it play a few games in between to evaluate?\n",
    "        # Always play vs a previous version? Would mean the current version should always\n",
    "        # be a bit better and therefore reward should increase\n",
    "        #total_rewards[epoch].append(total_rewards[epoch][-1] + first_reward)\n",
    "        #total.append(total[-1] + first_reward)\n",
    "end = int(round(time.time() * 1000))\n",
    "print(\"Time taken:\", (end - start))\n",
    "print(\"Time taken in sec:\", (end - start) / 1000)\n",
    "print(\"Avg time per episode:\", (end - start) / 1000 / epochs)\n",
    "# Time without batching: 657sec (10), 95 (5), 399 (5)\n",
    "# Time with batching: 12 (5)\n",
    "# Avg 10 ep with 32 batch: 11; 9.1; 9\n",
    "# Avg after action reduction: 1.2; 1.8; 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2, 3], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "------------------------------------------\n",
      "tensor([ 0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2, 3], device='cuda:0')\n",
      "3\n",
      "------------------------------------------\n",
      "tensor([ 0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2, 3], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "------------------------------------------\n",
      "tensor([ 0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2, 3], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "------------------------------------------\n",
      "tensor([ 0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0.,  0.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2, 3], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "------------------------------------------\n",
      "tensor([ 0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0., -1.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "errors = memory.memory[-6:]\n",
    "for error in errors:\n",
    "    print(error.old_state)\n",
    "    print(error.valid_actions)\n",
    "    print(error.action)\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = [ [ val.tolist() for val in rewards ] for rewards in total_rewards ]\n",
    "total = [ val.tolist() for val in total ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average moves to finish: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Average moves to finish:\", len(total) / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1229b128>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot(total_rewards[0])\n",
    "plt.plot(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(invalids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error in the end comes from the network predicting a result, which is wrong and since exploration is way down it almost\n",
    "# always predicts the same action which is always wrong. Should somehow learn though (maybe replay necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Use memory replay --> DONE\n",
    "- Maybe higher rewards needed for backpropagation of Q values?\n",
    "- View reward function by playing vs the network\n",
    "- View network output for certain states\n",
    "<br>\n",
    "<br>\n",
    "- Do I even backpropagate the reward to other states than the winning one in any way?\n",
    "- Maybe the problem are few games (not enough possibilities learned) -> More iterations like 10_000 games instead of iterations\n",
    "- Learning rate?\n",
    "<br>\n",
    "<br>\n",
    "- Rework memory replay batch size and epochs analog to pytorch tutorial\n",
    "- Plot metrics (e.g. total reward every iteration)\n",
    "- Rework code --> Readability and reusability\n",
    "- Maybe rework greedy policy\n",
    "- Test the pytorch agent on the dungeon example --> DONE: Works\n",
    "- Try increasing the performance (For running in the cloud) -> Use timer\n",
    "- Maybe no punishment for invalid moves?\n",
    "- Pass possible moves to network?\n",
    "- Only give out copies of the state... --> FIXED (This literally ruined every single state in the memory...)\n",
    "- Copy pytorch tensors via .copy().detach() (maybe more effectively possible as well?)\n",
    "<br>\n",
    "<br>\n",
    "- How to choose rewards and how does the agent learn the rules (punishment for invalid moves?)\n",
    "- How much training is needed for a game?\n",
    "- Evaluation tactics: Total reward\n",
    "- Model too big?\n",
    "- Don't copy model to update agent? --> Constantly creating optimizer and agent again and again\n",
    "- Only learning negative values atm --> Why?\n",
    "- Ignore invalid moves\n",
    "- Limit reward to between -1 and 1\n",
    "- Let AI learn both sides at the same time, so playing against it makes more sense?!\n",
    "<br>\n",
    "<br>\n",
    "Takeaways:\n",
    "- Batching is so much quicker, it is absurd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_board = np.zeros(shape=(6, 7))\n",
    "empty_board = torch.tensor(empty_board.flatten(), device=active.device).float()\n",
    "empty_board = torch.unsqueeze(empty_board, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board2 = np.array([\n",
    "    [0,0,1,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()\n",
    "example_board = torch.unsqueeze(example_board, dim=0)\n",
    "example_board2 = torch.tensor(example_board2.flatten(), device=active.device).float()\n",
    "example_board2 = torch.unsqueeze(example_board2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n",
      "tensor([[0.1220, 0.1689, 0.0515, 0.2197, 0.1658, 0.2075, 0.2854]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(6, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board, [0, 1, 2, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board)[0], [0, 1, 2, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n",
      "tensor([[0.1259, 0.1757, 0.0545, 0.2176, 0.1731, 0.2145, 0.2920]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(6, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board2, [0, 1, 9, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board2))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board2)[0], [0, 1, 9, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n",
      "tensor([[0.0757, 0.0839, 0.0633, 0.1487, 0.1195, 0.1281, 0.2514]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(6, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(empty_board, range(7)))\n",
    "print(active.get_Q(empty_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(empty_board)[0], range(7)), 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game = ConnectFourSimulator()\n",
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    if not game_over:\n",
    "        confirmation = \"r\"\n",
    "        while confirmation == \"r\":\n",
    "            pc_action = active.next_action(board)\n",
    "            print(pc_action)\n",
    "            confirmation = input()\n",
    "            if confirmation == \"c\":\n",
    "                game_over, board, _, _, _, _ = game.take_action(pc_action)\n",
    "                print(game_over)\n",
    "                print(board)\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = memory.sample(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = [ game for game in games if game[3] == 100 or game[3] == -100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in terminal_states:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 4\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 4\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(-1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  1., -1.,  1., -1., -1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tensor([ 0.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  0., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 4\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(-2., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(-2., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1., -1.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  0.,  0.,  1., -1.,\n",
      "        -1., -1., -1.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  0.,  1., -1.,\n",
      "        -1., -1., -1.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(-1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  0.,  1., -1.,\n",
      "        -1., -1., -1.,  0.,  0.,  1.,  1.,  1., -1.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  0.,  1., -1.,\n",
      "        -1., -1., -1.,  0.,  0.,  1.,  1.,  1., -1., -1.,  0.,  0., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for state in memory.memory[:42]:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"Reward:\", state[3])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayAI:\n",
    "    def __init__(self, ai):\n",
    "        self.ai = ai\n",
    "        self.game = ConnectFourSimulator()\n",
    "        self.game_started = False\n",
    "        self.random_actions = 0\n",
    "        \n",
    "    def start_game(self):\n",
    "        self.__ai_move()\n",
    "        self.game.print_board()\n",
    "        print(\"Valid actions:\", self.game.valid_actions)\n",
    "        self.game_started = True\n",
    "    \n",
    "    def __ai_move(self):\n",
    "        valid_actions = self.game.valid_actions\n",
    "        state = torch.tensor(self.game.board.flatten(), device=self.ai.device).clone().float()\n",
    "        action = self.ai.next_action(torch.unsqueeze(state, dim=0), valid_actions)\n",
    "        if not action in valid_actions:\n",
    "            action_index = random.random(0, len(valid_actions))\n",
    "            action = valid_actions[action_index]\n",
    "            self.random_actions += 1\n",
    "        game_over, _, _ = self.game.take_action(action)\n",
    "        return game_over\n",
    "            \n",
    "        \n",
    "    def play(self):\n",
    "        assert self.game_started == True, \"Game has not yet been started\"\n",
    "        \n",
    "        game_over = False\n",
    "        \n",
    "        while not game_over:\n",
    "            action = input()\n",
    "            if action == \"q\":\n",
    "                return\n",
    "            action = int(action)\n",
    "            game_over, _, reward, = self.game.take_action(action)\n",
    "            assert reward[0] >= 0, \"Invalid action!\"\n",
    "        \n",
    "            self.game.print_board()\n",
    "            if game_over:\n",
    "                print(\"You won!\")\n",
    "                return\n",
    "            \n",
    "            game_over = self.__ai_move()\n",
    "            self.game.print_board()\n",
    "            print(\"Valid actions:\", self.game.valid_actions)\n",
    "            if game_over:\n",
    "                print(\"You lost :/\")\n",
    "                return      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vs_game = PlayAI(active)\n",
    "vs_game.start_game()\n",
    "vs_game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
