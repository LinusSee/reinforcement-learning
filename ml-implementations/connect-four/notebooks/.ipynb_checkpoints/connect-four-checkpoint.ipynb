{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.valid_actions = list(range(self.width))\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tactive_player = self.current_player\n",
    "\t\tinactive_player = self.__negated_player(active_player)\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, self.board, active_player, -2, inactive_player, 0\n",
    "\n",
    "\t\tself.__play_move(action)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(action)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(action)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\t\t\t#elif winner == self.PLAYER1:\n",
    "\t\t\t#\treturn self.__game_over, self.board, active_player, 1, inactive_player, -1\n",
    "\t\t\t#else:\n",
    "\t\t\t#\treturn self.__game_over, self.board, active_player, -1, inactive_player, 1\n",
    "\t\t\treturn self.__game_over, self.board, active_player, 1, inactive_player, -1\n",
    "\n",
    "\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\tboard = self.board\n",
    "\t\tboard = np.where(board == 1, \"X\", board)\n",
    "\t\tboard = np.where(board == \"-1.0\", \"O\", board)\n",
    "\t\tprint(np.where(board == \"0.0\", \"-\", board))\n",
    "\n",
    "\tdef __play_move(self, action):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\t# if 41 > action < 0:\n",
    "\t\t# \treturn False\n",
    "\t\t# x, y = self.__coordinates_from_action(action)\n",
    "\t\t# if x >= self.width or y >= self.height:\n",
    "\t\t# \treturn False\n",
    "\t\t#\n",
    "\t\t# height_x = self.__column_height(x)\n",
    "\t\t#\n",
    "\t\t# if y != height_x:\n",
    "\t\t# \treturn False\n",
    "\t\t# return True\n",
    "\t\tis_valid = action in self.valid_actions\n",
    "\n",
    "\t\tif is_valid:\n",
    "\t\t\tnext_valid_action = action + self.width\n",
    "\t\t\tif next_valid_action < self.width * self.height:\n",
    "\t\t\t\tself.valid_actions.append(next_valid_action)\n",
    "\t\t\tself.valid_actions.remove(action)\n",
    "\t\treturn is_valid\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, last_action):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) >= 42:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, last_action):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(last_action)\n",
    "\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, last_action):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\tx = action % self.width\n",
    "\t\ty = action // self.width\n",
    "\t\treturn x, y\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 64)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(64, 42)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepQPytorchAgent:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state_batch):\n",
    "        return self.model(state_batch)\n",
    "        \n",
    "    def next_action(self, state, valid_actions):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action(valid_actions)\n",
    "        else:\n",
    "            return self.greedy_action(state, valid_actions)\n",
    "        \n",
    "    def random_action(self, valid_actions):\n",
    "        action = random.randrange(0, 42)\n",
    "        while not action in valid_actions:\n",
    "            action = random.randrange(0, 42)\n",
    "        return action\n",
    "    \n",
    "    def greedy_action(self, state_batch, valid_actions):\n",
    "        #print(\"States before:\", state_batch)\n",
    "        #print(\"Greedy:\", torch.max(self.get_Q(state_batch), 1)[1])\n",
    "        Q_values = self.get_Q(state_batch)[0]\n",
    "        Q_values = self.normalized_Q(Q_values, valid_actions)\n",
    "        action = torch.max(Q_values, 0)[1]\n",
    "        assert action in valid_actions, \"Only valid actions may be selected\"\n",
    "        return action\n",
    "        #return torch.max(self.get_Q(state_batch), 1)[1]\n",
    "    \n",
    "    def normalized_Q(self, Q_values, valid_actions):\n",
    "        '''Takes a single Q value array and sets invalid actions to -1.'''\n",
    "        for x in range(0, 41):\n",
    "            if not x in valid_actions:\n",
    "                Q_values[x] = -1.0\n",
    "        return Q_values\n",
    "        \n",
    "    def update(self, old_states, new_states, actions, rewards, valid_actions_batch):\n",
    "        self.train(old_states, new_states, actions, rewards, valid_actions_batch)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_states, next_states, actions, rewards, valid_actions_batch):\n",
    "        old_state_values = self.get_Q(old_states)\n",
    "        next_state_values = self.get_Q(next_states).detach()\n",
    "        \n",
    "        #print(\"Max:\", torch.max(next_state_values, dim=1)[0])\n",
    "        for x in range(len(next_state_values)):\n",
    "            valid_actions = valid_actions_batch[x]\n",
    "            next_state_values[x] = self.normalized_Q(next_state_values[x], valid_actions)\n",
    "            \n",
    "        new_rewards = rewards + self.discount * torch.max(next_state_values, dim=1)[0]\n",
    "        updated_state_values = old_state_values.clone().detach() # Check if detach could cause problems\n",
    "        for index, (reward, action) in enumerate(zip(new_rewards, actions)):\n",
    "            updated_state_values[index][action] = reward\n",
    "        \n",
    "        #print(\"Old state values:\", old_state_values)\n",
    "        #print(\"New reward:\", new_rewards)\n",
    "        #print(\"Updated:\", updated_state_values)\n",
    "        #print(\"Actions:\", actions)\n",
    "        #print(\"SelectedByActions:\", updated_state_values[actions])\n",
    "        #updated_state_values[actions] = new_rewards\n",
    "        \n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('old_state', 'next_state', 'action', 'reward', 'valid_actions'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active, passive):\n",
    "    old_state = np.copy(game.board)\n",
    "    old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "    valid_actions = torch.tensor(game.valid_actions, device=active.device).clone().long()\n",
    "    \n",
    "    action = active.next_action(torch.unsqueeze(old_state, dim=0), valid_actions)\n",
    "    #action = torch.tensor(action, device=active.device).long()\n",
    "    \n",
    "    game_over, next_state, _, reward, _, _ = game.take_action(action)\n",
    "    next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "    reward = torch.tensor(reward, device=active.device).float()\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, reward, valid_actions\n",
    "            \n",
    "    # if the move was invalid, add data and repeat\n",
    "    if reward < 0:\n",
    "        return False, old_state, next_state, action, reward, valid_actions\n",
    "        \n",
    "    # Play another move until the move is a right one and add the data to the memory\n",
    "    passive_reward = -1\n",
    "    counting_stars = 0\n",
    "    while passive_reward < 0:\n",
    "        passive_action = passive.next_action(torch.unsqueeze(next_state, dim=0), game.valid_actions)\n",
    "        game_over, _, _, passive_reward, _, cur_reward = game.take_action(passive_action)\n",
    "        \n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "                print(\"Counting:\", counting_stars)\n",
    "        \n",
    "    cur_reward = torch.tensor(cur_reward, device=active.device).float()\n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, cur_reward, valid_actions\n",
    "    return False, old_state, next_state, action, reward, valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, passive, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    #print(\"States before:\\n\", batch.old_state)\n",
    "    #print(\"Next states before:\\n\", batch.next_state)\n",
    "    #print(\"Actions before:\\n\", batch.action)\n",
    "    #print(\"Rewards before:\\n\", batch.reward)\n",
    "    old_state_batch = torch.stack(batch.old_state, dim=0)\n",
    "    next_state_batch = torch.stack(batch.next_state, dim=0)\n",
    "    action_batch = torch.tensor(batch.action, device=active.device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=active.device)\n",
    "    #valid_actions_batch = torch.stack(batch.valid_actions, dim=0)\n",
    "    valid_actions_batch = batch.valid_actions\n",
    "    #action_batch = torch.stack(batch.action, dim=0)\n",
    "    #reward_batch = torch.stack(batch.reward, dim=0)\n",
    "    \n",
    "    return active.update(old_state_batch, next_state_batch, action_batch, reward_batch, valid_actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20 # Number of games to play\n",
    "batch_size = 128\n",
    "memory = ReplayMemory(10000)\n",
    "active = DeepQPytorchAgent(iterations=epochs*batch_size*20)\n",
    "passive = DeepQPytorchAgent(iterations=epochs*batch_size*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board = torch.tensor(example_board.flatten()).to(active.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,\n",
      "          0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "6\n",
      "tensor([[ 0.2439, -0.0635,  0.1371,  0.2834,  0.0038,  0.1151,  0.0952, -0.3019,\n",
      "         -0.3123,  0.1461, -0.4114, -0.7127, -0.2641,  0.3244, -0.1021,  0.1043,\n",
      "          0.0403, -0.2162, -0.4567,  0.1353,  0.0440, -0.2997,  0.0098,  0.2915,\n",
      "         -0.2426, -0.2436, -0.1595,  0.5339,  0.2176, -0.3533, -0.0505,  0.3680,\n",
      "         -0.1926,  0.2225,  0.0836,  0.6327,  0.2398,  0.5036, -0.6752,  0.0516,\n",
      "         -0.3260,  0.1395]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(3, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_batch = torch.unsqueeze(example_board, dim=0)\n",
    "print(y_batch)\n",
    "print(active.next_action(y_batch, [0, 1, 2, 3, 4, 5, 6]))\n",
    "print(active.get_Q(y_batch))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(y_batch)[0], list(range(7))), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)]]\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Time taken: 269382\n",
      "Time taken in sec: 269.382\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using memory replay\n",
    "total_rewards = [ [torch.tensor(0)] for epoch in range(epochs) ]\n",
    "total = [torch.tensor(0)]\n",
    "print(total_rewards)\n",
    "start = int(round(time.time() * 1000))\n",
    "for epoch in range(epochs):\n",
    "    #invalids = []\n",
    "    #invalid = 0\n",
    "    #for iteration in range(1, iterations + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    game_over = False\n",
    "    game = ConnectFourSimulator()\n",
    "    while not game_over:\n",
    "        optimize_model(active, passive, memory, batch_size)\n",
    "        passive.model.load_state_dict(active.model.state_dict())\n",
    "                      \n",
    "        game_over, old_state, next_state, action, reward, valid_actions = transition(game, active, passive)\n",
    "\n",
    "        memory.push(old_state, next_state, action, reward, valid_actions)\n",
    "        total_rewards[epoch].append(total_rewards[epoch][-1] + reward)\n",
    "        total.append(total[-1] + reward)\n",
    "end = int(round(time.time() * 1000))\n",
    "print(\"Time taken:\", (end - start))\n",
    "print(\"Time taken in sec:\", (end - start) / 1000)\n",
    "# Time without batching: 657sec (10), 95 (5), 399 (5)\n",
    "# Time with batching: 12 (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = [ [ val.tolist() for val in rewards ] for rewards in total_rewards ]\n",
    "total = [ val.tolist() for val in total ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average moves to finish: 11.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Average moves to finish:\", len(total) / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1174a710>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot(total_rewards[0])\n",
    "plt.plot(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(invalids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error in the end comes from the network predicting a result, which is wrong and since exploration is way down it almost\n",
    "# always predicts the same action which is always wrong. Should somehow learn though (maybe replay necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Use memory replay --> DONE\n",
    "- Maybe higher rewards needed for backpropagation of Q values?\n",
    "- View reward function by playing vs the network\n",
    "- View network output for certain states\n",
    "<br>\n",
    "<br>\n",
    "- Do I even backpropagate the reward to other states than the winning one in any way?\n",
    "- Maybe the problem are few games (not enough possibilities learned) -> More iterations like 10_000 games instead of iterations\n",
    "- Learning rate?\n",
    "<br>\n",
    "<br>\n",
    "- Rework memory replay batch size and epochs analog to pytorch tutorial\n",
    "- Plot metrics (e.g. total reward every iteration)\n",
    "- Rework code --> Readability and reusability\n",
    "- Maybe rework greedy policy\n",
    "- Test the pytorch agent on the dungeon example --> DONE: Works\n",
    "- Try increasing the performance (For running in the cloud) -> Use timer\n",
    "- Maybe no punishment for invalid moves?\n",
    "- Pass possible moves to network?\n",
    "- Only give out copies of the state... --> FIXED (This literally ruined every single state in the memory...)\n",
    "- Copy pytorch tensors via .copy().detach() (maybe more effectively possible as well?)\n",
    "<br>\n",
    "<br>\n",
    "- How to choose rewards and how does the agent learn the rules (punishment for invalid moves?)\n",
    "- How much training is needed for a game?\n",
    "- Evaluation tactics: Total reward\n",
    "- Model too big?\n",
    "- Don't copy model to update agent? --> Constantly creating optimizer and agent again and again\n",
    "- Only learning negative values atm --> Why?\n",
    "- Ignore invalid moves\n",
    "- Limit reward to between -1 and 1\n",
    "<br>\n",
    "<br>\n",
    "Takeaways:\n",
    "- Batching is so much quicker, it is absurd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_board = np.zeros(shape=(6, 7))\n",
    "empty_board = torch.tensor(empty_board.flatten(), device=active.device).float()\n",
    "empty_board = torch.unsqueeze(empty_board, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board2 = np.array([\n",
    "    [0,0,1,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()\n",
    "example_board = torch.unsqueeze(example_board, dim=0)\n",
    "example_board2 = torch.tensor(example_board2.flatten(), device=active.device).float()\n",
    "example_board2 = torch.unsqueeze(example_board2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "tensor([[ 0.3897,  0.2352,  0.3593,  0.3941,  0.2876,  0.3899,  0.3445,  0.1270,\n",
      "          0.0195,  0.4242,  0.0358, -0.2118,  0.0908,  0.4432,  0.2159,  0.3811,\n",
      "          0.2344,  0.1671, -0.0865,  0.3752,  0.3529, -0.0559,  0.3377,  0.5287,\n",
      "          0.1188,  0.0652,  0.1514,  0.6879,  0.0555, -0.0172,  0.0614,  0.1374,\n",
      "         -0.1494,  0.1987,  0.4484,  0.6085,  0.4717,  0.3176, -0.3701,  0.3302,\n",
      "         -0.3024,  0.2525]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(5, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board, [0, 1, 2, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board)[0], [0, 1, 2, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 0.3925,  0.2354,  0.3607,  0.3992,  0.2859,  0.3887,  0.3433,  0.1292,\n",
      "          0.0192,  0.4239,  0.0367, -0.2086,  0.0914,  0.4429,  0.2183,  0.3807,\n",
      "          0.2336,  0.1713, -0.0852,  0.3748,  0.3543, -0.0551,  0.3368,  0.5250,\n",
      "          0.1183,  0.0648,  0.1493,  0.6876,  0.0533, -0.0190,  0.0617,  0.1361,\n",
      "         -0.1487,  0.1962,  0.4495,  0.6081,  0.4698,  0.3185, -0.3692,  0.3308,\n",
      "         -0.3054,  0.2541]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(9, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board2, [0, 1, 9, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board2))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board2)[0], [0, 1, 9, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 0.3999,  0.2363,  0.3650,  0.3984,  0.2900,  0.3847,  0.3388,  0.1237,\n",
      "          0.0243,  0.4283,  0.0457, -0.2122,  0.0922,  0.4483,  0.2173,  0.3786,\n",
      "          0.2325,  0.1736, -0.0789,  0.3775,  0.3567, -0.0544,  0.3380,  0.5255,\n",
      "          0.1167,  0.0601,  0.1545,  0.6759,  0.0511, -0.0262,  0.0607,  0.1298,\n",
      "         -0.1560,  0.1991,  0.4517,  0.6038,  0.4728,  0.3174, -0.3693,  0.3318,\n",
      "         -0.3003,  0.2448]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(empty_board, range(7)))\n",
    "print(active.get_Q(empty_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(empty_board)[0], range(7)), 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game = ConnectFourSimulator()\n",
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    if not game_over:\n",
    "        confirmation = \"r\"\n",
    "        while confirmation == \"r\":\n",
    "            pc_action = active.next_action(board)\n",
    "            print(pc_action)\n",
    "            confirmation = input()\n",
    "            if confirmation == \"c\":\n",
    "                game_over, board, _, _, _, _ = game.take_action(pc_action)\n",
    "                print(game_over)\n",
    "                print(board)\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = memory.sample(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = [ game for game in games if game[3] == 100 or game[3] == -100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in terminal_states:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 11\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  0., -1.,  0.,  0.,  1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  0., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 13\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  0., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 7\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  0., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 14\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  1., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  1., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 25\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  1., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  0., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 9\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  1., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  1., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "        -1.,  1.,  0.,  0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 28\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  1., -1.,  0.,  0., -1.,  0.,  0.,\n",
      "        -1.,  1.,  0.,  0.,  1.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  1., -1.,  0.,  0., -1.,  0., -1.,\n",
      "        -1.,  1.,  0.,  0.,  1.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 27\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1.,  0.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  1., -1.,  0.,  0., -1.,  0., -1.,\n",
      "        -1.,  1.,  0.,  0.,  1.,  0.,  0.,  1., -1.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1., -1.,  1., -1.,  0.,  1.,  1., -1., -1., -1.,  1.,  0.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  1., -1.,  0.,  0., -1.,  0., -1.,\n",
      "        -1.,  1.,  0.,  0.,  1.,  0.,  0.,  1., -1.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 10\n",
      "Reward: tensor(-1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 13\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 18\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  0.,  1.,  0., -1.,  0.,  0., -1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1.,  0.,  1.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1.,  0.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1.,  0.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 20\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1., -1.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1., -1.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 25\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1., -1.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  1., -1.,  0.,  0., -1., -1.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 7\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  0.,  0., -1.,  1.,  1.,  1., -1.,  0.,  0., -1., -1.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  0., -1.,  1.,  1.,  1., -1.,  0.,  0., -1., -1.,  1.,\n",
      "         0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  0., -1.,  1.,  1.,  1., -1.,  0.,  0., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  0., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  0., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 10\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1., -1.,  1.,  1.,  0.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 21\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1., -1.,  1.,  1., -1.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1., -1.,  1.,  1., -1.,  0.,  0.,  1.,  0., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 39\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  0.,  1., -1.,  1.,  1., -1.,  0.,  0.,  1.,  0., -1.,\n",
      "         0., -1.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  1.,  1., -1.,  1.,  1., -1.,  0.,  0.,  1.,  0., -1.,\n",
      "         0., -1.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 17\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  0.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  1.,  1., -1.,  1.,  1., -1.,  0.,  0.,  1., -1., -1.,\n",
      "         0., -1.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "        -1., -1.,  0.,  1.,  1., -1.,  1.,  1., -1.,  0.,  0.,  1., -1., -1.,\n",
      "         0., -1.,  0.,  0., -1.,  0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 9\n",
      "Reward: tensor(1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 10\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 17\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1., -1.,  0.,  0., -1.,  0.,  0., -1.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1., -1.,  1.,  0., -1.,  0.,  0., -1.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 4\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1., -1.,  1.,  0., -1.,  0.,  0., -1.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1., -1.,  1.,  0., -1.,  0.,  0., -1.,  1.,  1.,  0.,  0.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 11\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1., -1.,  1.,  0., -1.,  0.,  0., -1.,  1.,  1.,  0.,  0.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  1., -1.,  1.,  0., -1.,  0.,  0., -1.,  1.,  1.,  0.,  0.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  1.,  1., -1.,  1., -1., -1.,  0.,  0., -1.,  1.,  1.,  0.,  0.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  1., -1.,  1., -1., -1.,  0.,  0., -1.,  1.,  1.,  0.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  0.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  0.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 30\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  0.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  0.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 37\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  1.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  1.,  1.,\n",
      "         0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 20\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  1.,  1.,\n",
      "         0.,  0., -1.,  1.,  1.,  0., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  1.,  1.,\n",
      "         0.,  0., -1.,  1.,  1., -1., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 19\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  1.,  1.,\n",
      "         0.,  1., -1.,  1.,  1., -1., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  0., -1., -1.,  1.,  1.,  1.,  1.,\n",
      "         0.,  1., -1.,  1.,  1., -1., -1.,  0.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 31\n",
      "Reward: tensor(-1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 4\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0., -1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0., -1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for state in memory.memory[:42]:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"Reward:\", state[3])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayAI:\n",
    "    def __init__(self, ai):\n",
    "        self.ai = ai\n",
    "        self.game = ConnectFourSimulator()\n",
    "        self.game_started = False\n",
    "        self.random_actions = 0\n",
    "        \n",
    "    def start_game(self):\n",
    "        self.__ai_move()\n",
    "        self.game.print_board()\n",
    "        print(\"Valid actions:\", self.game.valid_actions)\n",
    "        self.game_started = True\n",
    "    \n",
    "    def __ai_move(self):\n",
    "        valid_actions = self.game.valid_actions\n",
    "        state = torch.tensor(self.game.board.flatten(), device=self.ai.device).clone().float()\n",
    "        action = self.ai.next_action(torch.unsqueeze(state, dim=0), valid_actions)\n",
    "        if not action in valid_actions:\n",
    "            action_index = random.random(0, len(valid_actions))\n",
    "            action = valid_actions[action_index]\n",
    "            self.random_actions += 1\n",
    "        game_over, _, _, _, _, _ = self.game.take_action(action)\n",
    "        return game_over\n",
    "            \n",
    "        \n",
    "    def play(self):\n",
    "        assert self.game_started == True, \"Game has not yet been started\"\n",
    "        \n",
    "        game_over = False\n",
    "        \n",
    "        while not game_over:\n",
    "            action = input()\n",
    "            if action == \"q\":\n",
    "                return\n",
    "            action = int(action)\n",
    "            game_over, _, _, reward, _, _, = self.game.take_action(action)\n",
    "            assert reward >= 0, \"Invalid action!\"\n",
    "        \n",
    "            self.game.print_board()\n",
    "            if game_over:\n",
    "                print(\"You won!\")\n",
    "                return\n",
    "            \n",
    "            game_over = self.__ai_move()\n",
    "            self.game.print_board()\n",
    "            print(\"Valid actions:\", self.game.valid_actions)\n",
    "            if game_over:\n",
    "                print(\"You lost :/\")\n",
    "                return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 1, 2, 4, 5, 6, 10]\n",
      "10\n",
      "[['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 1, 2, 4, 5, 6, 24]\n",
      "2\n",
      "[['-' '-' 'O' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['-' '-' 'O' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 1, 4, 5, 6, 9, 31]\n",
      "31\n",
      "[['-' '-' 'O' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['-' 'X' 'O' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 4, 5, 6, 9, 38, 8]\n",
      "8\n",
      "[['-' 'X' 'O' 'X' '-' '-' '-']\n",
      " ['-' 'O' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['-' 'X' 'O' 'X' '-' '-' '-']\n",
      " ['-' 'O' '-' 'O' '-' '-' '-']\n",
      " ['-' 'X' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 4, 5, 6, 9, 38, 22]\n",
      "9\n",
      "[['-' 'X' 'O' 'X' '-' '-' '-']\n",
      " ['-' 'O' 'O' 'O' '-' '-' '-']\n",
      " ['-' 'X' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['-' 'X' 'O' 'X' 'X' '-' '-']\n",
      " ['-' 'O' 'O' 'O' '-' '-' '-']\n",
      " ['-' 'X' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 5, 6, 38, 22, 16, 11]\n",
      "11\n",
      "[['-' 'X' 'O' 'X' 'X' '-' '-']\n",
      " ['-' 'O' 'O' 'O' 'O' '-' '-']\n",
      " ['-' 'X' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n"
     ]
    }
   ],
   "source": [
    "vs_game = PlayAI(active)\n",
    "vs_game.start_game()\n",
    "vs_game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
