{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.valid_actions = list(range(self.width))\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tactive_player = self.current_player\n",
    "\t\tinactive_player = self.__negated_player(active_player)\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, self.board, active_player, -2, inactive_player, 0\n",
    "\n",
    "\t\tself.__play_move(action)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(action)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(action)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\t\t\telif winner == self.PLAYER1:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 1, inactive_player, -1\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, -1, inactive_player, 1\n",
    "\n",
    "\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\tboard = self.board\n",
    "\t\tboard = np.where(board == 1, \"X\", board)\n",
    "\t\tboard = np.where(board == \"-1.0\", \"O\", board)\n",
    "\t\tprint(np.where(board == \"0.0\", \"-\", board))\n",
    "\n",
    "\tdef __play_move(self, action):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\t# if 41 > action < 0:\n",
    "\t\t# \treturn False\n",
    "\t\t# x, y = self.__coordinates_from_action(action)\n",
    "\t\t# if x >= self.width or y >= self.height:\n",
    "\t\t# \treturn False\n",
    "\t\t#\n",
    "\t\t# height_x = self.__column_height(x)\n",
    "\t\t#\n",
    "\t\t# if y != height_x:\n",
    "\t\t# \treturn False\n",
    "\t\t# return True\n",
    "\t\tis_valid = action in self.valid_actions\n",
    "\n",
    "\t\tif is_valid:\n",
    "\t\t\tnext_valid_action = action + self.width\n",
    "\t\t\tif next_valid_action < self.width * self.height:\n",
    "\t\t\t\tself.valid_actions.append(next_valid_action)\n",
    "\t\t\tself.valid_actions.remove(action)\n",
    "\t\treturn is_valid\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, last_action):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) >= 42:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, last_action):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(last_action)\n",
    "\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, last_action):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\tx = action % self.width\n",
    "\t\ty = action // self.width\n",
    "\t\treturn x, y\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 64)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(64, 42)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepQPytorchAgent:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state_batch):\n",
    "        return self.model(state_batch)\n",
    "        \n",
    "    def next_action(self, state, valid_actions):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action(valid_actions)\n",
    "        else:\n",
    "            return self.greedy_action(state, valid_actions)\n",
    "        \n",
    "    def random_action(self, valid_actions):\n",
    "        action = random.randrange(0, 42)\n",
    "        while not action in valid_actions:\n",
    "            action = random.randrange(0, 42)\n",
    "        return action\n",
    "    \n",
    "    def greedy_action(self, state_batch, valid_actions):\n",
    "        #print(\"States before:\", state_batch)\n",
    "        #print(\"Greedy:\", torch.max(self.get_Q(state_batch), 1)[1])\n",
    "        Q_values = self.get_Q(state_batch)[0]\n",
    "        Q_values = self.normalized_Q(Q_values, valid_actions)\n",
    "        action = torch.max(Q_values, 0)[1]\n",
    "        assert action in valid_actions, \"Only valid actions may be selected\"\n",
    "        return action\n",
    "        #return torch.max(self.get_Q(state_batch), 1)[1]\n",
    "    \n",
    "    def normalized_Q(self, Q_values, valid_actions):\n",
    "        '''Takes a single Q value array and sets invalid actions to -1.'''\n",
    "        for x in range(0, 41):\n",
    "            if not x in valid_actions:\n",
    "                Q_values[x] = -1.0\n",
    "        return Q_values\n",
    "        \n",
    "    def update(self, old_states, new_states, actions, rewards, valid_actions_batch):\n",
    "        self.train(old_states, new_states, actions, rewards, valid_actions_batch)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_states, next_states, actions, rewards, valid_actions_batch):\n",
    "        old_state_values = self.get_Q(old_states)\n",
    "        next_state_values = self.get_Q(next_states).detach()\n",
    "        \n",
    "        #print(\"Max:\", torch.max(next_state_values, dim=1)[0])\n",
    "        for x in range(len(next_state_values)):\n",
    "            valid_actions = valid_actions_batch[x]\n",
    "            next_state_values[x] = self.normalized_Q(next_state_values[x], valid_actions)\n",
    "            \n",
    "        new_rewards = rewards + self.discount * torch.max(next_state_values, dim=1)[0]\n",
    "        updated_state_values = old_state_values.clone().detach() # Check if detach could cause problems\n",
    "        for index, (reward, action) in enumerate(zip(new_rewards, actions)):\n",
    "            updated_state_values[index][action] = reward\n",
    "        \n",
    "        #print(\"Old state values:\", old_state_values)\n",
    "        #print(\"New reward:\", new_rewards)\n",
    "        #print(\"Updated:\", updated_state_values)\n",
    "        #print(\"Actions:\", actions)\n",
    "        #print(\"SelectedByActions:\", updated_state_values[actions])\n",
    "        #updated_state_values[actions] = new_rewards\n",
    "        \n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('old_state', 'next_state', 'action', 'reward', 'valid_actions'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active, passive):\n",
    "    old_state = np.copy(game.board)\n",
    "    old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "    valid_actions = torch.tensor(game.valid_actions).clone().long()\n",
    "    \n",
    "    action = active.next_action(torch.unsqueeze(old_state, dim=0), valid_actions)\n",
    "    #action = torch.tensor(action, device=active.device).long()\n",
    "    \n",
    "    game_over, next_state, _, reward, _, _ = game.take_action(action)\n",
    "    next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "    reward = torch.tensor(reward, device=active.device).float()\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, reward, valid_actions\n",
    "            \n",
    "    # if the move was invalid, add data and repeat\n",
    "    if reward < 0:\n",
    "        return False, old_state, next_state, action, reward, valid_actions\n",
    "        \n",
    "    # Play another move until the move is a right one and add the data to the memory\n",
    "    passive_reward = -1\n",
    "    counting_stars = 0\n",
    "    while passive_reward < 0:\n",
    "        passive_action = passive.next_action(torch.unsqueeze(next_state, dim=0), game.valid_actions)\n",
    "        game_over, _, _, passive_reward, _, cur_reward = game.take_action(passive_action)\n",
    "        \n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "                print(\"Counting:\", counting_stars)\n",
    "        \n",
    "    cur_reward = torch.tensor(cur_reward, device=active.device).float()\n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, cur_reward, valid_actions\n",
    "    return False, old_state, next_state, action, reward, valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, passive, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    #print(\"States before:\\n\", batch.old_state)\n",
    "    #print(\"Next states before:\\n\", batch.next_state)\n",
    "    #print(\"Actions before:\\n\", batch.action)\n",
    "    #print(\"Rewards before:\\n\", batch.reward)\n",
    "    old_state_batch = torch.stack(batch.old_state, dim=0)\n",
    "    next_state_batch = torch.stack(batch.next_state, dim=0)\n",
    "    action_batch = torch.tensor(batch.action, device=active.device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=active.device)\n",
    "    #valid_actions_batch = torch.stack(batch.valid_actions, dim=0)\n",
    "    valid_actions_batch = batch.valid_actions\n",
    "    #action_batch = torch.stack(batch.action, dim=0)\n",
    "    #reward_batch = torch.stack(batch.reward, dim=0)\n",
    "    \n",
    "    return active.update(old_state_batch, next_state_batch, action_batch, reward_batch, valid_actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # Number of games to play\n",
    "batch_size = 128\n",
    "memory = ReplayMemory(10000)\n",
    "active = DeepQPytorchAgent(iterations=epochs*batch_size*20)\n",
    "passive = DeepQPytorchAgent(iterations=epochs*batch_size*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board = torch.tensor(example_board.flatten()).to(active.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,\n",
      "          0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "5\n",
      "tensor([[ 0.2538, -0.1114,  0.4036, -0.0712,  0.2001,  0.2356,  0.3839, -0.1994,\n",
      "         -0.2785,  0.4265,  0.1045, -0.2738, -0.0031,  0.1644, -0.5094, -0.0652,\n",
      "         -0.1026, -0.7005, -0.5665, -0.6166, -0.2112, -0.0443,  0.0394,  0.1024,\n",
      "         -0.3530, -0.2709,  0.2213,  0.2292, -0.1257, -0.2148, -0.2620,  0.0278,\n",
      "          0.1149, -0.4240, -0.1949,  0.1201, -0.0157,  0.1991, -0.2172,  0.3576,\n",
      "         -0.0153, -0.0585]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(2, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_batch = torch.unsqueeze(example_board, dim=0)\n",
    "print(y_batch)\n",
    "print(active.next_action(y_batch, [0, 1, 2, 3, 4, 5, 6]))\n",
    "print(active.get_Q(y_batch))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(y_batch)[0], list(range(7))), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)]]\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Time taken: 134255\n",
      "Time taken in sec: 134.255\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using memory replay\n",
    "total_rewards = [ [torch.tensor(0)] for epoch in range(epochs) ]\n",
    "total = [torch.tensor(0)]\n",
    "print(total_rewards)\n",
    "start = int(round(time.time() * 1000))\n",
    "for epoch in range(epochs):\n",
    "    #invalids = []\n",
    "    #invalid = 0\n",
    "    #for iteration in range(1, iterations + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    game_over = False\n",
    "    game = ConnectFourSimulator()\n",
    "    while not game_over:\n",
    "        optimize_model(active, passive, memory, batch_size)\n",
    "        passive.model.load_state_dict(active.model.state_dict())\n",
    "                      \n",
    "        game_over, old_state, next_state, action, reward, valid_actions = transition(game, active, passive)\n",
    "\n",
    "        memory.push(old_state, next_state, action, reward, valid_actions)\n",
    "        total_rewards[epoch].append(total_rewards[epoch][-1] + reward)\n",
    "        total.append(total[-1] + reward)\n",
    "end = int(round(time.time() * 1000))\n",
    "print(\"Time taken:\", (end - start))\n",
    "print(\"Time taken in sec:\", (end - start) / 1000)\n",
    "# Time without batching: 657sec (10), 95 (5), 399 (5)\n",
    "# Time with batching: 12 (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = [ [ val.tolist() for val in rewards ] for rewards in total_rewards ]\n",
    "total = [ val.tolist() for val in total ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average moves to finish:\", len(total) / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11503f28>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot(total_rewards[0])\n",
    "plt.plot(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(invalids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error in the end comes from the network predicting a result, which is wrong and since exploration is way down it almost\n",
    "# always predicts the same action which is always wrong. Should somehow learn though (maybe replay necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Use memory replay --> DONE\n",
    "- Maybe higher rewards needed for backpropagation of Q values?\n",
    "- View reward function by playing vs the network\n",
    "- View network output for certain states\n",
    "<br>\n",
    "<br>\n",
    "- Do I even backpropagate the reward to other states than the winning one in any way?\n",
    "- Maybe the problem are few games (not enough possibilities learned) -> More iterations like 10_000 games instead of iterations\n",
    "- Learning rate?\n",
    "<br>\n",
    "<br>\n",
    "- Rework memory replay batch size and epochs analog to pytorch tutorial\n",
    "- Plot metrics (e.g. total reward every iteration)\n",
    "- Rework code --> Readability and reusability\n",
    "- Maybe rework greedy policy\n",
    "- Test the pytorch agent on the dungeon example --> DONE: Works\n",
    "- Try increasing the performance (For running in the cloud) -> Use timer\n",
    "- Maybe no punishment for invalid moves?\n",
    "- Pass possible moves to network?\n",
    "- Only give out copies of the state... --> FIXED (This literally ruined every single state in the memory...)\n",
    "- Copy pytorch tensors via .copy().detach() (maybe more effectively possible as well?)\n",
    "<br>\n",
    "<br>\n",
    "- How to choose rewards and how does the agent learn the rules (punishment for invalid moves?)\n",
    "- How much training is needed for a game?\n",
    "- Evaluation tactics: Total reward\n",
    "- Model too big?\n",
    "- Don't copy model to update agent? --> Constantly creating optimizer and agent again and again\n",
    "- Only learning negative values atm --> Why?\n",
    "- Ignore invalid moves\n",
    "- Limit reward to between -1 and 1\n",
    "<br>\n",
    "<br>\n",
    "Takeaways:\n",
    "- Batching is so much quicker, it is absurd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_board = np.zeros(shape=(6, 7))\n",
    "empty_board = torch.tensor(empty_board.flatten(), device=active.device).float()\n",
    "empty_board = torch.unsqueeze(empty_board, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board2 = np.array([\n",
    "    [0,0,1,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()\n",
    "example_board = torch.unsqueeze(example_board, dim=0)\n",
    "example_board2 = torch.tensor(example_board2.flatten(), device=active.device).float()\n",
    "example_board2 = torch.unsqueeze(example_board2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "tensor([[ 0.3757,  0.1784,  0.3618,  0.2671,  0.3865,  0.3371,  0.3697,  0.1828,\n",
      "          0.0660,  0.3454,  0.2817,  0.0535,  0.3420,  0.3334, -0.1535,  0.1540,\n",
      "         -0.0662, -0.2724, -0.1454, -0.1781,  0.1377,  0.2167,  0.4163,  0.2809,\n",
      "         -0.1086,  0.0366,  0.3589,  0.3622,  0.1867,  0.0799,  0.0514,  0.2430,\n",
      "         -0.1500, -0.1520,  0.0525,  0.0939,  0.0444,  0.1935, -0.2114,  0.3751,\n",
      "          0.1742,  0.2297]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board, [0, 1, 2, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board)[0], [0, 1, 2, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "tensor([[ 0.3763,  0.1816,  0.3651,  0.2716,  0.3834,  0.3327,  0.3671,  0.1855,\n",
      "          0.0649,  0.3471,  0.2817,  0.0549,  0.3438,  0.3379, -0.1531,  0.1549,\n",
      "         -0.0668, -0.2690, -0.1406, -0.1778,  0.1385,  0.2177,  0.4208,  0.2827,\n",
      "         -0.1075,  0.0357,  0.3593,  0.3589,  0.1869,  0.0809,  0.0537,  0.2407,\n",
      "         -0.1479, -0.1539,  0.0507,  0.0893,  0.0445,  0.1919, -0.2099,  0.3772,\n",
      "          0.1759,  0.2288]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board2, [0, 1, 9, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board2))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board2)[0], [0, 1, 9, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[ 0.3699,  0.1827,  0.3654,  0.2688,  0.3927,  0.3359,  0.3677,  0.1836,\n",
      "          0.0648,  0.3405,  0.2833,  0.0540,  0.3457,  0.3331, -0.1478,  0.1554,\n",
      "         -0.0636, -0.2759, -0.1447, -0.1783,  0.1417,  0.2129,  0.4141,  0.2804,\n",
      "         -0.1075,  0.0361,  0.3558,  0.3576,  0.1859,  0.0849,  0.0619,  0.2409,\n",
      "         -0.1454, -0.1482,  0.0504,  0.0925,  0.0419,  0.1950, -0.2108,  0.3800,\n",
      "          0.1696,  0.2257]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(4, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(empty_board, range(7)))\n",
    "print(active.get_Q(empty_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(empty_board)[0], range(7)), 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game = ConnectFourSimulator()\n",
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    if not game_over:\n",
    "        confirmation = \"r\"\n",
    "        while confirmation == \"r\":\n",
    "            pc_action = active.next_action(board)\n",
    "            print(pc_action)\n",
    "            confirmation = input()\n",
    "            if confirmation == \"c\":\n",
    "                game_over, board, _, _, _, _ = game.take_action(pc_action)\n",
    "                print(game_over)\n",
    "                print(board)\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = memory.sample(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = [ game for game in games if game[3] == 100 or game[3] == -100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in terminal_states:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in memory.memory[:42]:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"Reward:\", state[3])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
