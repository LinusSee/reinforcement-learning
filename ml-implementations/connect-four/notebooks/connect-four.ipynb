{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.valid_actions = list(range(self.width))\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tactive_player = self.current_player\n",
    "\t\tinactive_player = self.__negated_player(active_player)\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, self.board, active_player, -2, inactive_player, 0\n",
    "\n",
    "\t\tself.__play_move(action)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(action)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(action)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\t\t\telif winner == self.PLAYER1:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 1, inactive_player, -1\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, -1, inactive_player, 1\n",
    "\n",
    "\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\tboard = self.board\n",
    "\t\tboard = np.where(board == 1, \"X\", board)\n",
    "\t\tboard = np.where(board == \"-1.0\", \"O\", board)\n",
    "\t\tprint(np.where(board == \"0.0\", \"-\", board))\n",
    "\n",
    "\tdef __play_move(self, action):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\t# if 41 > action < 0:\n",
    "\t\t# \treturn False\n",
    "\t\t# x, y = self.__coordinates_from_action(action)\n",
    "\t\t# if x >= self.width or y >= self.height:\n",
    "\t\t# \treturn False\n",
    "\t\t#\n",
    "\t\t# height_x = self.__column_height(x)\n",
    "\t\t#\n",
    "\t\t# if y != height_x:\n",
    "\t\t# \treturn False\n",
    "\t\t# return True\n",
    "\t\tis_valid = action in self.valid_actions\n",
    "\n",
    "\t\tif is_valid:\n",
    "\t\t\tnext_valid_action = action + self.width\n",
    "\t\t\tif next_valid_action < self.width * self.height:\n",
    "\t\t\t\tself.valid_actions.append(next_valid_action)\n",
    "\t\t\tself.valid_actions.remove(action)\n",
    "\t\treturn is_valid\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, last_action):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) >= 42:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, last_action):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(last_action)\n",
    "\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, last_action):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\tx = action % self.width\n",
    "\t\ty = action // self.width\n",
    "\t\treturn x, y\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 64)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(64, 42)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepQPytorchAgent:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state_batch):\n",
    "        return self.model(state_batch)\n",
    "        \n",
    "    def next_action(self, state, valid_actions):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action(valid_actions)\n",
    "        else:\n",
    "            return self.greedy_action(state, valid_actions)\n",
    "        \n",
    "    def random_action(self, valid_actions):\n",
    "        action = random.randrange(0, 42)\n",
    "        while not action in valid_actions:\n",
    "            action = random.randrange(0, 42)\n",
    "        return action\n",
    "    \n",
    "    def greedy_action(self, state_batch, valid_actions):\n",
    "        #print(\"States before:\", state_batch)\n",
    "        #print(\"Greedy:\", torch.max(self.get_Q(state_batch), 1)[1])\n",
    "        Q_values = self.get_Q(state_batch)[0]\n",
    "        Q_values = self.normalized_Q(Q_values, valid_actions)\n",
    "        action = torch.max(Q_values, 0)[1]\n",
    "        assert action in valid_actions, \"Only valid actions may be selected\"\n",
    "        return action\n",
    "        #return torch.max(self.get_Q(state_batch), 1)[1]\n",
    "    \n",
    "    def normalized_Q(self, Q_values, valid_actions):\n",
    "        '''Takes a single Q value array and sets invalid actions to -1.'''\n",
    "        for x in range(0, 41):\n",
    "            if not x in valid_actions:\n",
    "                Q_values[x] = -1.0\n",
    "        return Q_values\n",
    "        \n",
    "    def update(self, old_states, new_states, actions, rewards, valid_actions_batch):\n",
    "        self.train(old_states, new_states, actions, rewards, valid_actions_batch)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_states, next_states, actions, rewards, valid_actions_batch):\n",
    "        old_state_values = self.get_Q(old_states)\n",
    "        next_state_values = self.get_Q(next_states).detach()\n",
    "        \n",
    "        #print(\"Max:\", torch.max(next_state_values, dim=1)[0])\n",
    "        for x in range(len(next_state_values)):\n",
    "            valid_actions = valid_actions_batch[x]\n",
    "            next_state_values[x] = self.normalized_Q(next_state_values[x], valid_actions)\n",
    "            \n",
    "        new_rewards = rewards + self.discount * torch.max(next_state_values, dim=1)[0]\n",
    "        updated_state_values = old_state_values.clone().detach() # Check if detach could cause problems\n",
    "        for index, (reward, action) in enumerate(zip(new_rewards, actions)):\n",
    "            updated_state_values[index][action] = reward\n",
    "        \n",
    "        #print(\"Old state values:\", old_state_values)\n",
    "        #print(\"New reward:\", new_rewards)\n",
    "        #print(\"Updated:\", updated_state_values)\n",
    "        #print(\"Actions:\", actions)\n",
    "        #print(\"SelectedByActions:\", updated_state_values[actions])\n",
    "        #updated_state_values[actions] = new_rewards\n",
    "        \n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('old_state', 'next_state', 'action', 'reward', 'valid_actions'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active, passive):\n",
    "    old_state = np.copy(game.board)\n",
    "    old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "    valid_actions = torch.tensor(game.valid_actions).clone().long()\n",
    "    \n",
    "    action = active.next_action(torch.unsqueeze(old_state, dim=0), valid_actions)\n",
    "    #action = torch.tensor(action, device=active.device).long()\n",
    "    \n",
    "    game_over, next_state, _, reward, _, _ = game.take_action(action)\n",
    "    next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "    reward = torch.tensor(reward, device=active.device).float()\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, reward, valid_actions\n",
    "            \n",
    "    # if the move was invalid, add data and repeat\n",
    "    if reward < 0:\n",
    "        return False, old_state, next_state, action, reward, valid_actions\n",
    "        \n",
    "    # Play another move until the move is a right one and add the data to the memory\n",
    "    passive_reward = -1\n",
    "    counting_stars = 0\n",
    "    while passive_reward < 0:\n",
    "        passive_action = passive.next_action(torch.unsqueeze(next_state, dim=0), game.valid_actions)\n",
    "        game_over, _, _, passive_reward, _, cur_reward = game.take_action(passive_action)\n",
    "        \n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "                print(\"Counting:\", counting_stars)\n",
    "        \n",
    "    cur_reward = torch.tensor(cur_reward, device=active.device).float()\n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, cur_reward, valid_actions\n",
    "    return False, old_state, next_state, action, reward, valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, passive, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    #print(\"States before:\\n\", batch.old_state)\n",
    "    #print(\"Next states before:\\n\", batch.next_state)\n",
    "    #print(\"Actions before:\\n\", batch.action)\n",
    "    #print(\"Rewards before:\\n\", batch.reward)\n",
    "    old_state_batch = torch.stack(batch.old_state, dim=0)\n",
    "    next_state_batch = torch.stack(batch.next_state, dim=0)\n",
    "    action_batch = torch.tensor(batch.action, device=active.device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=active.device)\n",
    "    #valid_actions_batch = torch.stack(batch.valid_actions, dim=0)\n",
    "    valid_actions_batch = batch.valid_actions\n",
    "    #action_batch = torch.stack(batch.action, dim=0)\n",
    "    #reward_batch = torch.stack(batch.reward, dim=0)\n",
    "    \n",
    "    return active.update(old_state_batch, next_state_batch, action_batch, reward_batch, valid_actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # Number of games to play\n",
    "batch_size = 128\n",
    "memory = ReplayMemory(10000)\n",
    "active = DeepQPytorchAgent(iterations=epochs*batch_size*20)\n",
    "passive = DeepQPytorchAgent(iterations=epochs*batch_size*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board = torch.tensor(example_board.flatten()).to(active.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,\n",
      "          0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "0\n",
      "tensor([[-0.1222,  0.3007, -0.1598,  0.3016, -0.3448,  0.3003,  0.3311,  0.2142,\n",
      "         -0.2104, -0.5564,  0.0696, -0.1571, -0.2803, -0.3656, -0.2932,  0.2341,\n",
      "          0.1172,  0.1617, -0.0783, -0.0634, -0.3856,  0.1783, -0.2165,  0.4328,\n",
      "          0.1907, -0.3452, -0.3517, -0.3849, -0.0471,  0.4139, -0.2694,  0.2710,\n",
      "         -0.2912,  0.1197, -0.0630, -0.2242, -0.1111, -0.0730, -0.0519,  0.1220,\n",
      "         -0.2119, -0.7330]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(6, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_batch = torch.unsqueeze(example_board, dim=0)\n",
    "print(y_batch)\n",
    "print(active.next_action(y_batch, [0, 1, 2, 3, 4, 5, 6]))\n",
    "print(active.get_Q(y_batch))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(y_batch)[0], list(range(7))), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)], [tensor(0)]]\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e899a8998a56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mpassive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mgame_over\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9aab99e8f40a>\u001b[0m in \u001b[0;36mtransition\u001b[1;34m(game, active, passive)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalid_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#action = torch.tensor(action, device=active.device).long()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d97756ceff91>\u001b[0m in \u001b[0;36mnext_action\u001b[1;34m(self, state, valid_actions)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrandom_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d97756ceff91>\u001b[0m in \u001b[0;36mgreedy_action\u001b[1;34m(self, state_batch, valid_actions)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mQ_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalized_Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Only valid actions may be selected\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m#return torch.max(self.get_Q(state_batch), 1)[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \"\"\"\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using memory replay\n",
    "total_rewards = [ [torch.tensor(0)] for epoch in range(epochs) ]\n",
    "total = [torch.tensor(0)]\n",
    "print(total_rewards)\n",
    "start = int(round(time.time() * 1000))\n",
    "for epoch in range(epochs):\n",
    "    #invalids = []\n",
    "    #invalid = 0\n",
    "    #for iteration in range(1, iterations + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    game_over = False\n",
    "    game = ConnectFourSimulator()\n",
    "    while not game_over:\n",
    "        optimize_model(active, passive, memory, batch_size)\n",
    "        passive.model.load_state_dict(active.model.state_dict())\n",
    "                      \n",
    "        game_over, old_state, next_state, action, reward, valid_actions = transition(game, active, passive)\n",
    "\n",
    "        memory.push(old_state, next_state, action, reward, valid_actions)\n",
    "        total_rewards[epoch].append(total_rewards[epoch][-1] + reward)\n",
    "        total.append(total[-1] + reward)\n",
    "end = int(round(time.time() * 1000))\n",
    "print(\"Time taken:\", (end - start))\n",
    "print(\"Time taken in sec:\", (end - start) / 1000)\n",
    "# Time without batching: 657sec (10), 95 (5), 399 (5)\n",
    "# Time with batching: 12 (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = [ [ val.tolist() for val in rewards ] for rewards in total_rewards ]\n",
    "total = [ val.tolist() for val in total ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average moves to finish: 7.71\n"
     ]
    }
   ],
   "source": [
    "print(\"Average moves to finish:\", len(total) / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x595f6a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3XmcW+V97/HPT9Jstsf7gu1hbAzYARzWYQsJZmmCQ5PQVy9JgaSXtGmdpm2apc1Ce9uU+0p6szW3pE0udQkhJYSQErJvJARKCGDA7MYY22Bj431fxtJoee4fOto1MxrpaHQkfd+vl1+Wzjmj85NG+umZ5zzP7zHnHCIi0jpCjQ5ARET8pcQuItJilNhFRFqMEruISItRYhcRaTFK7CIiLUaJXUSkxSixi4i0GCV2EZEWExnPk82cOdMtXLhwPE8pItL0Vq9evcc5N6vS48c1sS9cuJAnnnhiPE8pItL0zGzzWI5XV4yISItRYhcRaTFK7CIiLUaJXUSkxSixi4i0mFETu5ndama7zOz5ou0fNLN1ZrbGzD5fvxBFRGQsKmmx3wYsz99gZpcCVwGnO+dOA77of2giIlKNUcexO+ceNLOFRZs/AHzWORfzjtnlf2giEjSDQwlue3gT0aFkdtubFs/i3IXTGxiVP57YtI8HX9pdsG3W5G7+8IIFDYqoetVOUFoMvMnMPgNEgb9xzj1e7kAzWwGsAOjv76/ydCISBA9v2Mvnf74OADNwDh55eS//9WdvaHBktfvCL9ax6pV9mKXvZ5aDvuLUOcye3N24wKpQbWKPANOAC4Bzge+Y2SJXZmVs59xKYCXAwMCAVs4WaWLH4umW+q8+ejEnze7lfbc9zs7D0QZH5Y9j8SSXLpnF1//oPADueXIrH/3OM9nn3EyqHRWzFbjHpT0GpICZ/oUlIkEUS6QA6IqE0/93hIjFU40MyTexeCr7vCD3HDPPuZlUm9i/D1wGYGaLgU5gj19BiUgwxRLp1mtXJOT9H27KxFdOLJGkqyOXEjPPMdqELfZRu2LM7E7gEmCmmW0FPgXcCtzqDYEcAq4v1w0jIq0lGi9qsUdCTZn4yonGU9lkDmSTfDN+cVUyKubaYXa9x+dYRCTgsi32jkyLPdSUia+cWCJZ0BXT3eF1xTRhV5NmnopIxTJJrjPsJfaOcDbZN7tYoqjFHsm02Jvv+Smxi0jFYokUnZEQoVB6TGC312JvhZ7YWCKVbaVDrrspqha7iLSydHdFfj90GOcgnmzuxJ5IpkimnFrsItJ+0t0V+a3a5k1++bLDOPNGxWT72JvwGoISu4hULBpPlm3VNmN3Rb7MyJ5yX1rNOOpnXNc8FZHgiCWSbN1/rGT77N4uers7svePxhLsOJSeXbrv6FDRWO90Ily/6zCHovGCxwmbsWDGBCwzRz8AnHNs3jtIsuiawO7DMYCywx23H4yycfeRguM7QiGOn94TqOeWT4ldpE195K6n+elzO0q2L54ziXs/six7/7pbVvHMlgPZ+2ccPzV7e3JPOoVc9x+ryp7j07+3lPcEqIjWnY9t4W+/99yw+/O/0LoiYTrDIVY++DIrH3y55NibrjmTq86cX5c4a6XELtKmdh6KsWROL39+6YnZbd998rWCJA6w61CUCxZN59rz0kX8ls6fkt132evmcPN7zinbx/7hu55m16Fg1ZHZ6cVz0zVnluzrioS59HWzsvfDIePOFRewdf9gwXGxRIqP3/0suw7F6htsDZTYRdpULJGkb1pPQavzhW2HeOyVvUXHpTh5dm/Z1mlnJMTypceVffxPfPfZwF14zAzXrLSlfc6CaZyzYFrBtngyndiD3Peui6cibSoWTxX0l0NuJmn+uPRY0QXTSgWxjkzxcM1qREJGyII9WkaJXaRNRYum0ENuXPpQMpV3XOkXQCWCWEcmWlTBsRpm5n1pBeu55VNiF2lTsaKiV5A/Lj2d2HMTd8aeDLs6gldHxo8WO0B3AJ9bPiV2kTZVPIUe0i12yNWEySSv7ipa7N0BbNWmn3Ptaa8rEg7cXyP5lNhF2lS51mvxTNLihTXGIoiLcBQvplGtIP41kk+JXaQNOedKqhlCaVdM8cIaYxHYi6e+tNiD96WVb9RnaGa3mtkub1GN4n1/Y2bOzLQsnkgTGUqmcC7X9ZKRq2iY9P4vraFSqSBePC13XaEa3QEvV1zJM7wNWF680cyOB94MvOpzTCJSZ7kulqIWe8dwLfYqumICuAhH8WIa1Up/aQXrueUbNbE75x4E9pXZ9X+BjwPNXa9TpA3F4sMk9kxXTObi6TDHVSKIQwLLdT9VI4jPLV9VM0/N7B3Aa865Z4JaBEdEch57ZR+f+elaUql0OyyeLH9RNDNK5oZ7nqW3u4OjQ4myx1WiuyPEpj2DvP1fH8puu+x1s/nImxdn7z/40m7++d51pFzhz33pXWdy/PQJYz7ncP753nU8sG43L+85yslzemt+vK5IiCc2Hyp4bm8/Yy4rLj6x5NifPLudm/97I//4jlM5Z8H0ms9diTEndjObAPwd8JYKj18BrADo7+8f6+lExAe/3bCHZ7Yc4LLXzc5uWzhjIheeOKPguNcd18tVZ87jcDSd0GfRxWnzpvD6vPowlbrqrPkc8h4HYM22g/zo2W0lif35bYdYtjhdo+VoLMGqV/bx3GsHfU3sP3xmG7F4ijeeNJPfP6v2wl3vHOgr6Kp4ZssBfvLcjrKJ/b61O3nutYN0hmvvAqpUNS32E4ETgExrvQ940szOc86VlIpzzq0EVgIMDAyo20akATI1Um5977kjHjehM8JN15zlyzkvXTKbS5fkvkj++jvP8OjLhXVoookkU3s6snFt3nuUZV94wPeLrrF4imWLZ/G5q0/35fGWL53L8qVzs/fff/sTbN47WPbYWCLFSbMn8fq+sX85VmvMid059xyQ/W2Z2SZgwDm3x8e4RMRHxQtkNEJ67Hdhwi4epZLp8vH7omvUp2GOwxlpaGcjXvtKhjveCTwCLDGzrWb2vvqHJSJ+Kl7SrhHKjf2OJVIFQy5zF2/9b7HXM7mmn1v5mP26YDsWo7bYnXPXjrJ/oW/RiEhd+FUjpRblWrWli2MXDrf0Q3oylj/DHIcz0kzUep+7HM08FWkDftVIqUV3R4ihZCo7Mge8aosFLfZwdrtfEilHylVX76ZS3SPUjmnEa6/ELtIG/KqRUovM+fNLAhe32MMhoyNsvo4Rr6XeTaVGbLE34LVXYhdpA37VSKlF8eQnKN//7HeNmUzfd70vniZSjkSyNO5GvPZK7CJtoN4XDyuRSW7RvNZ4udZsuhSBfy326DDlE/yUeeyhMok92oDXXoldpA004gJesexQxrwWe7lhiH7XYcm22OvZFeMl7nJx6+KpiNRFI4bcFSuu9Q7DrOLU4XNXzHi02DMLlJT5SyOQwx1FpDKv7h3k3hdyk6/7pvUUzE6sl9cOHOPnz+/ILkAdDhlvP2MeU3o6uHv1Vo7GEuw+HGOxDzVSapGpQ3PX41s4bko3AAePxUtXcYqEWL/zMLf85uWKHndKTwdXn9NHpm7V/S/uYuPuI9n92w5EC85fD5lRL998dDPTJnQW7IvGk3U9dzlK7CI++eoDG/j241sKtq258QomdtX3Y3bLb17m67/dVLBtcCjJuQunc8M9z2W3LZw5sa5xjKZvWg+RkHHLQ68UbF84o7AmzAkzJ/Kz53fw6Z+srfixzzx+KifP6cU5x/tvX13S190RNuZP66k++FEcP20CIYOv3L+x7P4FM/yre1MJJXYRnxyJJVgwYwI/+uAbueuxLXzmp2sZHErWPbEfjSWY3dvFr/56GQBn/+9fcjSW4GgsXYDrjj85n9f3TaG3znGM5pS5k3n+xisKkq4Bvd0dBcd95bqzOTKUoBIPvrSbv/zWUxwdSneBxJOOoWSKD152En968aLscZ3hUF1bzQMLp7PmxuXEU6VdSCEzJo3za6/ELuKTWCJFT0eYyd0dTOnp8LbVv2Z3LJGipzN9XsgtcJE597QJndl9jdbdER41wYZCVnG8mW6PzAXSzHOe3N0x7s+5pzNMD429QJ2hi6ciPsmve1KPqfHDnjeeojtv1EVm2bbMuRs947SeuktWfKp+Kb9W0t7PXsRHsXiSbm/0Q7mhfXU7b9GQwUyxrezqR+N84W48FVeDzH6ZNXhoZ6MpsYv4pHyLfXy6YgoLaYWJ5nXFNHqYYz0VD6Ecj1mmzaC9n72Ij/ITbC7hjEeLvXD2ZqaE7HiM32604r+M2uE5V6K9n72Ij2J5CyrkqhTWv8WeHidd2GKPJVLZc4/3GOrx1F1UpiA6DrNMm4ESu4hP8lvODW+xexdPQwaRUOsuOK8We3mVrKB0q5ntMrPn87Z9wcxeNLNnzex7Zja1vmGKBF/+Rczi0Rp1P2+k8OJpNJ7KJvzMjMxWVDz6SKNi0ip59rcBy4u2/RJY6pw7HXgJuMHnuESaTv6ww1xLchwunsZTRaNi0l0xsXjjS/XWW2d4mIunbd4VU8nSeA+a2cKibffm3X0UuNrfsERqcygaH3Go4fSJnYRr7KI4OBgvmEWZX6kw8//eo0PsPhwr+LnOcIgpE2qbPHM0lmDQm215LF5YPbC7I8TgUCJdh6XFE1woZHRGQhwYjLP7cIw9R4aA1h67Xwk/Zp7+MXCXD48j4ot1Ow7z1pseJG8FthLvOGMeX772rKrP8dsNe3j3LatKtk/sTCfSCZ3pj9Znf/Yin/3ZiyXHfeOPz2PZ4llVnfvA4BAX/J/7CkrETuzKJfBJXRE27x1k895BFs1qbH2Y8TCpK8JtD2/itoc3ZbdlXv92VdOzN7O/AxLAHSMcswJYAdDf31/L6UQqsu3AMVIO3r9sEX3TSosv/efDm9iyf7Cmc2z1fv5jVyxhslc+IGzG8qXHAelk87XrB9h2MFrwc4ejcT7/83XZn6/GniMxovEU7xro4/V9UwkZvOXU47L7//Kykzht/hQAls6bXPV5msVX330263flqjnOnNjJvKn1K/jVDKpO7GZ2PfA24HKXqRdahnNuJbASYGBgYIQ2lIg/Mv2tV50xn1PLJLb/XrebbQeO1XiOdGv5D849npmTusoec/kpc0q2HRgc4vM/X1fTjNRMS/3yU+ZwxWnHlezvmzaBP7xgQdWP32wuWDSDCxbNaHQYgVJVYjez5cAngGXOudqaPiI+G21kRHrh4douaman649xWF3xFPiqzq0hfTKKSoY73gk8Aiwxs61m9j7g34Be4Jdm9rSZ3VznOEUqlkm6w03M6Y6Ea156LfPFMNbJP+VWERqvc0v7qGRUzLVlNn+tDrGI+GK0GinpFnutib26yT+hkNEZru38arHLaPTOkJYzWuLLzMys9RzVTv7JVF+s+tzZbiC12KU8JXZpOaPVC8lM4Kn1HNVO/unqCGVrm1Qj+xdJm4/VluHpnSEtJ5ZIYZZe57KcrkiIoUSK1EgD3Uc7R7z6lee7ImGfWuz6+Ep5emdIy4kl0lP7h+smyVx0LF7weGznqH7l+VpH5ejiqYxGiV1azmg1UrIjU2ppNSdqbLHr4qnUkd4Z0nJGS7p+rG5UXCp3LDKLTddy7vTjqMUu5bV3QQXh4LE4H7/7GY7EEiX7ers6+OK7zmBSV/pt8vDGPXz1/o04Ku+bLn4Mvznn+McfrmHD7tyU8pd2HqFnhG6KTEL8wB1PZotFhcz46JsXc1b/tJLjb/rVeh7btLdg2/OvHeLk2ZOqirkrEuLpV/fz7lseLdk3p7ebL7zzjGyBsr1HYnziu89xLJ77/by6b3DEawgiarG3ubXbD/GLNTvZc3gouwByLJ5i16EYP1+zg5d2Hs4ee++anTzy8t6C40b6l3mM9XmP4bdEyvGNRzazac9g9rwLpk/g6nP6hv2ZgQXTuOikGRhkf+Y36/fw6xd3lT3+9kc38dLOIwXP7eTZk7jqzHlVxXzVmfNZPKe35PV6bf8x7nnqNXYdztWXefa1g/xq7U72Hsn9fub0dvPu8/tbus661EYt9jaX+bP+n35/KecsmJ7d/vDGPVz3H6sK+qFjiSQzJnZy9wfeUNFjP7xhD9fdsqqui01kHvv6NyxgxcUnVvQzC2dO5I4/uaBg2+v+/mfDxhmLp7h6oI9Pvf202oL1XHd+P9edX1oQ754nt/LR7zxT+Jp7t7/0rjPL1r0RKUct9jY33MIEmREX+f3QsXhqTCMxujpqr4symphP63p2d4SHXRQjlhjb864lhsz5cufOPD99VKVyere0uUwSKU4c5dbsjBYtwTaa3OiT+q0iFPVphMhwFzRTKcdQsvoRMGONAUq/TCH3JSlSCSX2NjfcCItyVQiLl2AbzXis++nXUmjDDUHMjHUfjxEoZV/zUereiJSjd0uby02/L99ij+a1tsc6xC9zbLSOLXa/xnSnF4AujXO416ceMl+axa/5eJ1fWofeLW1u2BZ7mdZ2bKxdMePRYs92JdXYYh+m4uN4rnpfbuKUxqxLNZTY29xwBaWy3QJ5rcfoGOuj+LGoxGj8alGnu2JKW+zjWUmx3OsVjSc1Zl3GTIm9zQ1XUKpc//hY66P4sajEaPxqUXd3lC+lO56jUrrLzIgdre6NSDmVrKB0q5ntMrPn87ZNN7Nfmtl67//S6XrSFGKJFJ2RUEni6AyXS+xjbbHXXpNlNPW+eDqeXSHlL1hXXx5Y2lcl75jbgOVF2z4J3OecOxm4z7svTWi4fnMzK1mQIl2qtvIEl3uM8ehj92O4Y5mumHEclVJueGgtxcakfVWyNN6DZrawaPNVwCXe7W8AD5Be3FoCKBpP8os1O8om2LXbD42wIEWINa8d4jtPbAHgcDQ+5tZjVyTEmm0Hs4+R2bZ86XFVtYJf3n2EJzbvz95fvWm/95i1tthD7D0yVBAnwMZdR7L76y3z2j6+aT8TvNo6L+08rAunMmbVlhSY45zbDuCc225ms4c70MxWACsA+vtLp1FL/d3/4i4+9O2nh91/et+UstvnTe3hoQ17eGjDnuy2uVN6xnTueVN7+M36Pfxm/Z6C7Te/52yWL507pscC+IcfrCmIB9Kt9akTOsb8WPnmTu1h79EhPn73syX7zGDO5O6aHr8SXZEw0yd28pPntvOT57Znt59/wvQRfkqkVN1rxTjnVgIrAQYGBqpfskaqdtir3Hj3n13IcVNKE9TMSV1lf+57f34Re4/GsvdDZswt8/MjKX6MHQejXH3zIxyOllaTrMThWILzT5jOP7/rjOy23u4OertrS+wfe8sS3nPBApwrfYtO6IwwfWJnTY9fiXDIePDjl3JgcKhg+3C/H5HhVJvYd5rZXK+1PhcoXxZPAiHTBbNgxkRm9VaeJHo6w/R1Tqjp3MWP0VmmVMFYxOJJ5vR20TettriKhULG/Klj+2ukHiZ1RepW4ljaR7Udhz8ErvduXw/8wJ9wpB6yI0cCMLqi1rHtQ4mU6qaIjKKS4Y53Ao8AS8xsq5m9D/gs8GYzWw+82bsvARWkaenlShWMRTQ+ttmvIu2oklEx1w6z63KfY5E6ybTYM2PTG6lc1cix0PA/kdHpE9IGMskwCLMXzYzOYcaMV6KWtUZF2oUSexsYr4UiKtUdKT99vxLpsgZ624qMRJ+QNjDWqoz11tVRfvr+aJIpRzzp1GIXGUVwPu1SN2NdIKPehpu+P5qhcSyhK9LM9AlpA+kl7YLTyu2qsitmPBe9EGlmmgnRgpIpRzKVm0F5bChgXTGRMMfiyWwLPCNkECkauRNPpshMBj3izaAN0peUSBApsbeYaDzJGz93P3uOxAq2n7swOJWVJ3SG+fWLu1j8v35WsL0rEuLHH3wjJ8/pBeD7T73GR77zNMWz/Cd0KrGLjESJvcXsHxxiz5EYV5w2h9P7pma3X3jijAZGVehvf/cUHtm4t2DbjoNRbn90M1v2D2YT+wavsuLHrliSPa4rEuLyU4atOSciKLG3nEzf9VuXzuX3zprf4GjKO7t/Gmf3F/4F8eKOQ9z+6Oai9T6T9HSE+YtLTxrvEEWaWnA6XsUX0XFcGMJPZVcP0ixTkaroU9NismuYNtmQwHI1ZNJ1YdSfLjJWzfXpl1GN5xqdfipXQyaWCNb4e5FmoU9NixnPNTr9lCnFW7rGanM9D5Eg0KemxUTjmcWdm6vF3p1dyLnw4mmzPQ+RIFBibzHN2mKPhEOEQ6aLpyI+qOlTY2YfMbM1Zva8md1pZvVf8VdGlL142mR97FBaQ0YlekWqU3ViN7P5wF8BA865pUAYuMavwKQ62eGOTXjRsSsSynYlgVZLEqlWrZ+aCNBjZhFgArCt9pCkFrkWe/MlxK5IuLTF3oRfUCKNVvXMU+fca2b2ReBV4Bhwr3PuXt8ik2FF40k+9YM1HIrGS/a9vPso0KRdMR0hHlq/hw98czUA2w4c4/XzpzQ4KpHmU3ViN7NpwFXACcAB4L/M7D3OuW8WHbcCWAHQ399fQ6iSsXb7Ie56Ygvzp/Ywsas0gb/l1DlNucrQ8qXHcf+Lu9i4O10jpm9aD8sWz2pwVCLNp5ZaMb8DvOKc2w1gZvcAbwAKErtzbiWwEmBgYMAVP4iMXWbkyBfeeTpvOHFmg6Pxzw1vPYUb3npKo8MQaXq1NOteBS4wswmWXiX5cmCtP2HJSHILTjRfd4uI1F/Vid05twq4G3gSeM57rJU+xSUjyJUNaL7uFhGpv5rK9jrnPgV8yqdYpEKZxN6M/egiUn/KDE0opq4YERmBEnsTynbFqMUuImUoMzShZi3NKyLjQ4m9CeVGxejXJyKllBmakEbFiMhIlBmaUCyRpDMSIj19QESkUE3DHaX+4skUD23YU7AAxfqdR7ILU4iIFFNiD7j7X9zFittXl2xfNGtiA6IRkWagxB5wB4+lKzh+/b3nMmdybh2TeVO1pomIlKfEHnCZC6WnzZvM7MlK5iIyOnXUBpzGrIvIWCmxB1ysiZe6E5HGULYIuGgTL3UnIo2hbBFwGrMuImOlxB5wsXhKY9ZFZEyUMQIulkjR1aELpyJSuZoSu5lNNbO7zexFM1trZhf6FZikxeJJ9a+LyJjUOo79JuDnzrmrzawTmOBDTJInlkgpsYvImFSd2M1sMnAx8F4A59wQMORPWJIRSyTpVleMiIxBLS32RcBu4OtmdgawGviQc+6oL5G1offf/gT//dLugm2xRIqz+6c1KCIRaUa1JPYIcDbwQefcKjO7Cfgk8Pf5B5nZCmAFQH9/fw2na31PvXqAhTMmsmzxrILtFxfdFxEZSS2JfSuw1Tm3yrt/N+nEXsA5txJYCTAwMOBqOF/LiyVSnH/CdG648pRGhyIiTazqq3LOuR3AFjNb4m26HHjBl6jaVCyR1NBGEalZraNiPgjc4Y2IeRn4o9pDak/OOWIJTUYSkdrVlNidc08DAz7F0tbiSYdzqMUuIjVT8zAgslUc1WIXkRopiwSEqjiKiF+URQIi12JXV4yI1EaJPSCyKyVpQQ0RqZGySEDE4loCT0T8ocQeEFoCT0T8Uus4dhnB3iMxvnzf+mw3S8YZx0/l2vP62bJvkH9/cCOJpGP34Rigi6ciUjsl9jr67ca9fOORzcyc1Ek4lF7a7tCxBL9Ys4Nrz+vnF2t28M1HX2VWbxchg0WzJnLirEkNjlpEmp0Sex1F4+nule//xUX0TUuXqv/0j1/gW4+9CuQumD70iUvVty4ivtHf/XWUSdz59dS7O8LZ7bF4EjPoDOvXICL+UUapo1i8dDZpVyREMuVIJFPZ1ZHMrFEhikgLUmKvo+zY9Lxulsyol1gik9jVBSMi/lJir6NMV0tHONcizyTyaDxJVAtVi0gdKKvUUbmulkwiz7bYNW5dRHymrFJH6RZ5YVdLYVdM6X4RkVopsddRLJGiu6hF3u0l8lgiSSxeul9EpFY1ZxUzC5vZU2b2Yz8CaiXlLo5mW+xxXTwVkfrwo7n4IWCtD4/TctJdLYUvcVe2xZ4qu19EpFY1zTw1sz7gd4HPAB/1JaIGcc7x7NaDDA6lx553RkKcefzUbCmAF7Yd4uCxeMHPzOrt5KTZvQAMJVI8veUAyZTL7t9xMFpycTSTyJ/esp+9R4dYMH1C3Z6TiLSnWksK/AvwcaB3uAPMbAWwAqC/v7/G09XPY6/s4w9WPlqw7d+uO4u3nT6PLfsGufLLvyn5mUjIePpTb2FSV4Q7Vm3mxh+9UHLMm06eWXB/2sROAP7ppy8CcObxU/16CiIiQA2J3czeBuxyzq02s0uGO845txJYCTAwMOCGO67R9g8OAfC5//F6pvR08mffXM3+wXjBvo9dsYSz+6cBcP+6Xax88GWORBNM6oqw/2j6mG/96fkYueGNi+cUFvU6cdYkfvJXb+TQsQQAp82fXN8nJiJtp5YW+0XAO8zsSqAbmGxm33TOvcef0MZXZpbouQunM7O3K73NKwmQ2XdG31QuPHEGANsPHvP25Y7p7gjxhhMLW+jlnDZvir/Bi4jkqfrKnXPuBudcn3NuIXAN8OtmTeqQt4JRR7hgElHhvvyaL+HCYzTCRUQCQkMyPLnFpEPZaou5pF2+mBfkkr5GuIhIUPhSj9059wDwgB+P1SjR7Jqj6RIAXZFQtismWmY90kzrPZrIHaPyACISBMpEnlyrPOz9HyppsXeX64opaLGrK0ZEGk+J3RNLpAoqMXZ1hAsujEJRiz3bD+8dE0+pK0ZEAkGZyBNLpOiOhLOVGLs7QrnWeJkFMzKrIuVfPM1fKUlEpFGU2D2xeLJk1Et+0obiUTFFLXZdPBWRgFAm8kSLulK6IqHsYtQjXjz19hX/vIhIoygTeYovfhZfPO0IW7ZuTHp/5uJpsuzPi4g0ii/DHZvB/qNDvPPfH+FQUSGvjAPH4gUFubo7wqx6ZS/nfeZXHI4mSpJ2ZoTMF+99ia8+sJG9R4c4Za7KA4hI47VNYt+8b5ANu46wbPEs5k3tLnvMRSflygGsuHgRC2bkEn1xGYAJnRE+sfx1vLrvaHbb1ef0+Ry1iMjYtU1iz3SZvH/ZoorquVyyZDaXLJk94jEfuOREX2ITEfFT2/SxR8uMRRcRaUVtk9jLjUUXEWlFbZPlMiNctHi0iLS6tsly5cr4cJx3AAAI70lEQVQCiIi0orZJ7JnJRqrAKCKtrm2ynFrsItIu2iix6+KpiLSHqrOcmR1vZveb2VozW2NmH/IzML/F8hbSEBFpZbVMUEoAf+2ce9LMeoHVZvZL59wLPsXmq2giSae3OpKISCurOrE757YD273bh81sLTAfGPfE/vXfvsL2g9GCbW86eSbhkPHAut0APLxxj1rrItIWfCkpYGYLgbOAVWX2rQBWAPT39/txugL7jg5x449eoCNsREK5GumPvbKPzkiIxzfto9u7YHreCdN9P7+ISNDUnNjNbBLwXeDDzrlDxfudcyuBlQADAwOu1vMVywxj/PTvLeUPzk1/caz4zyd4dd8gzjmWLZ7FbX90nt+nFREJrJoSu5l1kE7qdzjn7vEnpLHJjk8vWAQjvfqRc7pYKiLtp+rEbumrkF8D1jrnvuRfSGOTG59euPpRLJ7EOadx6yLSdmppzl4E/CFwmZk97f270qe4KparAVO6+lF6gWm12EWkvdQyKuYhoOFjB8tVbezOdMWgmaYi0n6afqGNbFdMR1FXTCLTFaMWu4i0l9ZJ7AULUYeJJx2JVFJFv0Sk7TR91suMisnvS88kc+fIjmEXEWkXTZ/Yy7fYS5O8iEi7aPqsV65qY3G3jIhIO2nKPvZdh6LsPhID4NW9g0BhAu8uupAqItJOmi6xJ5IpLv3iAxwdSma3dYSN7s5cAp/S01H2tohIO2i6xH4snuToUJJ3ntPH75w6B4B5U3oKWuzLFs/iG398Hs45LjppZqNCFRFpiKZL7FFvwYzT+6ZwxWnHlT0mEg6xbPGs8QxLRCQwmq4DOnexVBdFRUTKacLEXjrTVEREcpouO+bWLlWLXUSknOZL7JmuGLXYRUTKarrsWK7+uoiI5DRddszVhlFXjIhIOTUldjNbbmbrzGyDmX3Sr6BGoha7iMjIqs6OZhYGvgK8FTgVuNbMTvUrsOGUK/olIiI5tTR7zwM2OOdeds4NAd8GrvInrOGVWzFJRERyapl5Oh/Yknd/K3B+beGU96/3reeHz2wD4OCxOKBRMSIiw6klsZdb79SVHGS2AlgB0N/fX9WJZvV2cfKcSdn7x03uYdakrqoeS0Sk1dWS2LcCx+fd7wO2FR/knFsJrAQYGBgoSfyVuOa8fq45r7ovBRGRdlNLf8bjwMlmdoKZdQLXAD/0JywREalW1S1251zCzP4S+AUQBm51zq3xLTIREalKTWV7nXM/BX7qUywiIuIDDS0REWkxSuwiIi1GiV1EpMUosYuItBgldhGRFmPOVTVnqLqTme0GNlf54zOBPT6G47cgxxfk2CDY8QU5Ngh2fEGODYIdX3FsC5xzsyr94XFN7LUwsyeccwONjmM4QY4vyLFBsOMLcmwQ7PiCHBsEO75aY1NXjIhIi1FiFxFpMc2U2Fc2OoBRBDm+IMcGwY4vyLFBsOMLcmwQ7Phqiq1p+thFRKQyzdRiFxGRCjRFYm/EotlF57/VzHaZ2fN526ab2S/NbL33/zRvu5nZl71YnzWzs8chvuPN7H4zW2tma8zsQ0GJ0cy6zewxM3vGi+1Gb/sJZrbKi+0ur/QzZtbl3d/g7V9Yr9jyYgyb2VNm9uMAxrbJzJ4zs6fN7AlvW8N/r3nxTTWzu83sRe/9d2EQ4jOzJd5rlvl3yMw+HITY8mL8iPeZeN7M7vQ+K/6895xzgf5HuiTwRmAR0Ak8A5w6zjFcDJwNPJ+37fPAJ73bnwQ+592+EvgZ6RWmLgBWjUN8c4Gzvdu9wEukFxhveIzeOSZ5tzuAVd45vwNc422/GfiAd/vPgZu929cAd43D6/dR4FvAj737QYptEzCzaFvDf695sXwD+BPvdicwNUjxeecNAzuABUGJjfTSoq8APXnvuff69d6r+4vqwwtwIfCLvPs3ADc0II6FFCb2dcBc7/ZcYJ13+9+Ba8sdN46x/gB4c9BiBCYAT5JeG3cPECn+HZOu73+hdzviHWd1jKkPuA+4DPix98EORGzeeTZRmtgD8XsFJnvJyYIYX9553gL8NkixkVszerr3XvoxcIVf771m6Iopt2j2/AbFkm+Oc247gPf/bG97Q+P1/kQ7i3TLOBAxel0dTwO7gF+S/gvsgHMuUeb82di8/QeBGfWKDfgX4ONAyrs/I0CxQXod4XvNbLWl1w+GgPxeSf8VvRv4uteVdYuZTQxQfBnXAHd6twMRm3PuNeCLwKvAdtLvpdX49N5rhsRe0aLZAdKweM1sEvBd4MPOuUMjHVpmW91idM4lnXNnkm4dnwecMsL5xy02M3sbsMs5tzp/8wjnb8Tv9iLn3NnAW4G/MLOLRzh2vOOLkO6i/H/OubOAo6S7N4Yz7q+f10f9DuC/Rju0zLa6xeb17V8FnADMAyaS/h0PF8OY4muGxF7RotkNsNPM5gJ4/+/ytjckXjPrIJ3U73DO3RPEGJ1zB4AHSPdhTjWzzApe+efPxubtnwLsq1NIFwHvMLNNwLdJd8f8S0BiA8A5t837fxfwPdJfjEH5vW4FtjrnVnn37yad6IMSH6ST5ZPOuZ3e/aDE9jvAK8653c65OHAP8AZ8eu81Q2IP6qLZPwSu925fT7pfO7P9f3pX2S8ADmb+9KsXMzPga8Ba59yXghSjmc0ys6ne7R7Sb+i1wP3A1cPElon5auDXzutY9Jtz7gbnXJ9zbiHp99WvnXPvDkJsAGY20cx6M7dJ9xU/TwB+rwDOuR3AFjNb4m26HHghKPF5riXXDZOJIQixvQpcYGYTvM9v5rXz571X7wsXPl1ouJL0SI+NwN814Px3ku4Hi5P+5nwf6f6t+4D13v/TvWMN+IoX63PAwDjE90bSf5Y9Czzt/bsyCDECpwNPebE9D/yDt30R8BiwgfSfyV3e9m7v/gZv/6Jx+h1fQm5UTCBi8+J4xvu3JvPeD8LvNS/GM4EnvN/v94FpQYmP9MX6vcCUvG2BiM07543Ai97n4nagy6/3nmaeioi0mGboihERkTFQYhcRaTFK7CIiLUaJXUSkxSixi4i0GCV2EZEWo8QuItJilNhFRFrM/wdBPopaK7ngqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot(total_rewards[0])\n",
    "plt.plot(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(invalids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error in the end comes from the network predicting a result, which is wrong and since exploration is way down it almost\n",
    "# always predicts the same action which is always wrong. Should somehow learn though (maybe replay necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Use memory replay --> DONE\n",
    "- Maybe higher rewards needed for backpropagation of Q values?\n",
    "- View reward function by playing vs the network\n",
    "- View network output for certain states\n",
    "<br>\n",
    "<br>\n",
    "- Do I even backpropagate the reward to other states than the winning one in any way?\n",
    "- Maybe the problem are few games (not enough possibilities learned) -> More iterations like 10_000 games instead of iterations\n",
    "- Learning rate?\n",
    "<br>\n",
    "<br>\n",
    "- Rework memory replay batch size and epochs analog to pytorch tutorial\n",
    "- Plot metrics (e.g. total reward every iteration)\n",
    "- Rework code --> Readability and reusability\n",
    "- Maybe rework greedy policy\n",
    "- Test the pytorch agent on the dungeon example --> DONE: Works\n",
    "- Try increasing the performance (For running in the cloud) -> Use timer\n",
    "- Maybe no punishment for invalid moves?\n",
    "- Pass possible moves to network?\n",
    "- Only give out copies of the state... --> FIXED (This literally ruined every single state in the memory...)\n",
    "- Copy pytorch tensors via .copy().detach() (maybe more effectively possible as well?)\n",
    "<br>\n",
    "<br>\n",
    "- How to choose rewards and how does the agent learn the rules (punishment for invalid moves?)\n",
    "- How much training is needed for a game?\n",
    "- Evaluation tactics: Total reward\n",
    "- Model too big?\n",
    "- Don't copy model to update agent? --> Constantly creating optimizer and agent again and again\n",
    "- Only learning negative values atm --> Why?\n",
    "- Ignore invalid moves\n",
    "- Limit reward to between -1 and 1\n",
    "<br>\n",
    "<br>\n",
    "Takeaways:\n",
    "- Batching is so much quicker, it is absurd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_board = np.zeros(shape=(6, 7))\n",
    "empty_board = torch.tensor(empty_board.flatten(), device=active.device).float()\n",
    "empty_board = torch.unsqueeze(empty_board, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board2 = np.array([\n",
    "    [0,0,1,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()\n",
    "example_board = torch.unsqueeze(example_board, dim=0)\n",
    "example_board2 = torch.tensor(example_board2.flatten(), device=active.device).float()\n",
    "example_board2 = torch.unsqueeze(example_board2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[1.7817, 1.8056, 1.8363, 1.7683, 1.7023, 1.8090, 1.7902, 1.8220, 1.7254,\n",
      "         1.7433, 1.7455, 1.7800, 1.7517, 1.7895, 1.8076, 2.0135, 1.8600, 1.7538,\n",
      "         1.8729, 1.7863, 1.7216, 1.9410, 2.0671, 1.8555, 2.1595, 1.7571, 1.7574,\n",
      "         1.9285, 1.8858, 1.8301, 1.6871, 1.8170, 1.7476, 1.6158, 1.2748, 1.8565,\n",
      "         1.4448, 1.8000, 1.6915, 1.8570, 1.8705, 1.6324]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(24, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board, [0, 1, 2, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board)[0], [0, 1, 2, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[1.7856, 1.8101, 1.8420, 1.7721, 1.7085, 1.8110, 1.7933, 1.8234, 1.7277,\n",
      "         1.7493, 1.7491, 1.7816, 1.7536, 1.7946, 1.8118, 2.0174, 1.8639, 1.7581,\n",
      "         1.8766, 1.7885, 1.7248, 1.9450, 2.0731, 1.8571, 2.1622, 1.7607, 1.7629,\n",
      "         1.9325, 1.8875, 1.8329, 1.6906, 1.8204, 1.7509, 1.6185, 1.2788, 1.8607,\n",
      "         1.4470, 1.8047, 1.6966, 1.8581, 1.8747, 1.6361]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(24, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board2, [0, 1, 9, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board2))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board2)[0], [0, 1, 9, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[1.7646, 1.7940, 1.8222, 1.7538, 1.6875, 1.7944, 1.7801, 1.8068, 1.7053,\n",
      "         1.7278, 1.7306, 1.7674, 1.7367, 1.7726, 1.7930, 2.0023, 1.8519, 1.7400,\n",
      "         1.8610, 1.7653, 1.7089, 1.9322, 2.0486, 1.8473, 2.1413, 1.7470, 1.7519,\n",
      "         1.9142, 1.8769, 1.8160, 1.6806, 1.8016, 1.7325, 1.6030, 1.2626, 1.8393,\n",
      "         1.4361, 1.7867, 1.6698, 1.8479, 1.8589, 1.6233]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(2, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(empty_board, range(7)))\n",
    "print(active.get_Q(empty_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(empty_board)[0], range(7)), 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game = ConnectFourSimulator()\n",
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    if not game_over:\n",
    "        confirmation = \"r\"\n",
    "        while confirmation == \"r\":\n",
    "            pc_action = active.next_action(board)\n",
    "            print(pc_action)\n",
    "            confirmation = input()\n",
    "            if confirmation == \"c\":\n",
    "                game_over, board, _, _, _, _ = game.take_action(pc_action)\n",
    "                print(game_over)\n",
    "                print(board)\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = memory.sample(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = [ game for game in games if game[3] == 100 or game[3] == -100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in terminal_states:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in memory.memory[:42]:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"Reward:\", state[3])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
