{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tactive_player = self.current_player\n",
    "\t\tinactive_player = self.__negated_player(active_player)\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, self.board, active_player, -2, inactive_player, 0\n",
    "\n",
    "\t\tself.__play_move(action)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(action)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(action)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\t\t\telif winner == self.PLAYER1:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 10, inactive_player, -10\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, -10, inactive_player, 10\n",
    "\n",
    "\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\tprint(self.board)\n",
    "\n",
    "\tdef __play_move(self, action):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\tif action < 0:\n",
    "\t\t\treturn False\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tif x >= self.width or y >= self.height:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\theight_x = self.__column_height(x)\n",
    "\n",
    "\t\tif y != height_x:\n",
    "\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, last_action):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) == 0:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, last_action):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(last_action)\n",
    "\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, last_action):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\tx = action % self.width\n",
    "\t\ty = action // self.width\n",
    "\t\treturn x, y\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(42)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "'''\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n",
    "'''\n",
    "class DeepQTensorflowAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10_000):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        self.define_model()\n",
    "        self.session.run(self.initializer)\n",
    "    \n",
    "    def define_model(self):\n",
    "        self.model_input = tf.placeholder(dtype=tf.float32, shape=[ None, self.input_count ])\n",
    "        \n",
    "        fc1 = tf.layers.dense(self.model_input, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((self.input_count, 5))))\n",
    "        fc2 = tf.layers.dense(fc1, 16, activation=tf.sigmoid, kernel_initializer=tf.constant_initializer(np.zeros((6, self.output_count))))\n",
    "        \n",
    "        self.model_output = tf.layers.dense(fc2, self.output_count)\n",
    "        \n",
    "        self.target_output = tf.placeholder(shape=[ None, self.output_count ], dtype=tf.float32)\n",
    "        loss = tf.losses.mean_squared_error(self.target_output, self.model_output)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        \n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "    \n",
    "    def get_Q(self, state):\n",
    "        return self.session.run(self.model_output, feed_dict={ self.model_input: [state.flatten()] })[0]# Batching!! Dimensions!\n",
    "        \n",
    "    def next_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.randrange(0, 42) # Maybe change the probability distribution?\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        return np.argmax(self.get_Q(state))\n",
    "    \n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        old_state_values = self.get_Q(old_state)\n",
    "        new_state_values = self.get_Q(new_state)\n",
    "        \n",
    "        new_reward = reward + self.discount * np.amax(new_state_values)\n",
    "        old_state_values[action] = new_reward\n",
    "        \n",
    "        training_input = [old_state.flatten()]\n",
    "        target_output = [ old_state_values ]\n",
    "        training_data = { self.model_input: training_input, self.target_output: target_output }\n",
    "        \n",
    "        self.session.run(self.optimizer, feed_dict=training_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-fd3ed24ff09a>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\linus\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "deep_Q_learning = DeepQTensorflowAgent(iterations=iterations)\n",
    "deep_Q_dummy = DeepQTensorflowAgent(iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Input:\", input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still running at iteration %d 0\n",
      "Still running at iteration %d 250\n",
      "Still running at iteration %d 500\n",
      "Still running at iteration %d 750\n",
      "Still running at iteration %d 1000\n",
      "Still running at iteration %d 1250\n",
      "Still running at iteration %d 1500\n",
      "Still running at iteration %d 1750\n",
      "Still running at iteration %d 2000\n",
      "Still running at iteration %d 2250\n",
      "Still running at iteration %d 2500\n",
      "Still running at iteration %d 2750\n",
      "Still running at iteration %d 3000\n",
      "Still running at iteration %d 3250\n",
      "Still running at iteration %d 3500\n",
      "Still running at iteration %d 3750\n",
      "Still running at iteration %d 4000\n",
      "Still running at iteration %d 4250\n",
      "Still running at iteration %d 4500\n",
      "Still running at iteration %d 4750\n",
      "Still running at iteration %d 5000\n",
      "Still running at iteration %d 5250\n",
      "Still running at iteration %d 5500\n",
      "Still running at iteration %d 5750\n",
      "Still running at iteration %d 6000\n",
      "Still running at iteration %d 6250\n",
      "Still running at iteration %d 6500\n",
      "Still running at iteration %d 6750\n",
      "Still running at iteration %d 7000\n",
      "Still running at iteration %d 7250\n",
      "Still running at iteration %d 7500\n",
      "Still running at iteration %d 7750\n",
      "Still running at iteration %d 8000\n",
      "Still running at iteration %d 8250\n",
      "Still running at iteration %d 8500\n",
      "Still running at iteration %d 8750\n",
      "Still running at iteration %d 9000\n",
      "Still running at iteration %d 9250\n",
      "Still running at iteration %d 9500\n",
      "Still running at iteration %d 9750\n",
      "Invalids: [75, 81, 84, 88, 86, 77, 83, 96, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "Total: 9870\n"
     ]
    }
   ],
   "source": [
    "invalids = []\n",
    "invalid = 0\n",
    "for step in range(10000):\n",
    "    if step % 250 == 0:\n",
    "        print(\"Still running at iteration %d\", step)\n",
    "        \n",
    "    \n",
    "    old_state = game.board\n",
    "    action = deep_Q_learning.next_action(old_state)\n",
    "    game_over, new_state, cur_player, cur_reward, _, _ = game.take_action(action)\n",
    "    \n",
    "    if (step + 1) % 100 == 0:\n",
    "            invalids.append(invalid)\n",
    "            invalid = 0\n",
    "    if cur_reward < 0: # Invalid action\n",
    "        deep_Q_learning.update(old_state, new_state, action, cur_reward)\n",
    "        invalid += 1\n",
    "        continue\n",
    "        \n",
    "    #print(game_over)\n",
    "    #print(new_state)\n",
    "    #print(cur_player)\n",
    "    #print(cur_reward)\n",
    "    \n",
    "    if game_over:\n",
    "        deep_Q_learning.update(old_state, new_state, action, cur_reward)\n",
    "        game = ConnectFourSimulator()\n",
    "        continue\n",
    "    \n",
    "    next_action = deep_Q_dummy.next_action(new_state)\n",
    "    game_over, next_state, _, active_reward, passive_player, passive_reward = game.take_action(action)\n",
    "    \n",
    "    counting_stars = 0\n",
    "    while active_reward < 0: # Invalid move (infinite loop possible?)\n",
    "        next_action = deep_Q_dummy.next_action(new_state)\n",
    "        game_over, next_state, _, active_reward, passive_player, passive_reward = game.take_action(next_action)\n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "            print(\"Counting:\", counting_stars)\n",
    "            print(\"Using action:\", action)\n",
    "    \n",
    "    #print(game_over)\n",
    "    #print(next_state)\n",
    "    #print(passive_player)\n",
    "    #print(passive_reward)\n",
    "    \n",
    "    if game_over:\n",
    "        deep_Q_learning.update(old_state, new_state, action, passive_reward)\n",
    "        game = ConnectFourSimulator()\n",
    "    else:\n",
    "        deep_Q_learning.update(old_state, new_state, action, cur_reward)\n",
    "    # Missing: check for invalid move\n",
    "    # Maybe add passive mode to game (for the 2nd player)\n",
    "    #print(\"----------------------------\")\n",
    "        \n",
    "print(\"Invalids:\", invalids)\n",
    "print(\"Total:\", sum(invalids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
