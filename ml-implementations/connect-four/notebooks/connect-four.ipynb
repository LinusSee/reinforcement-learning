{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./../../../games/connect-four/connect-four.py\n",
    "import numpy as np\n",
    "\n",
    "class ConnectFourSimulator:\n",
    "\t\"\"\"Creates a connect-4 board and simulates it, returning states and rewards for any taken action.\n",
    "\n",
    "\tThe creates board is a 6 x 7 (rows x cols) array. Empty fields are denoted by 0.\n",
    "\tTokens placed by player one are denoted by '1' and player two uses '-1'.\n",
    "\tEvery field is part of the state and has it's own index, simply counting from 0 to 41 along the rows\n",
    "\tlike so [\n",
    "\t\t[0, 1, 2, 3, 4, 5, 6],\n",
    "\t\t[7, 8, 9, 10, 11, 12, 13],\n",
    "\t\t...\n",
    "\t\t[35, 36, 37, 38, 39, 40, 41]\n",
    "\t]\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.width = 7\n",
    "\t\tself.height = 6\n",
    "\t\tself.board = np.zeros(shape=(self.height, self.width))\n",
    "\t\tself.PLAYER1 = 1\n",
    "\t\tself.PLAYER2 = -1\n",
    "\t\tself.DRAW = 0\n",
    "\t\tself.current_player = self.PLAYER1\n",
    "\t\tself.valid_actions = list(range(self.width))\n",
    "\t\tself.__game_over = False\n",
    "\n",
    "\tdef take_action(self, action):\n",
    "\t\t\"\"\"Executes the action and returns the next state and the received reward.\"\"\"\n",
    "\t\tactive_player = self.current_player\n",
    "\t\tinactive_player = self.__negated_player(active_player)\n",
    "\t\tif not self.__action_is_valid(action):\n",
    "\t\t\treturn self.__game_over, self.board, active_player, -2, inactive_player, 0\n",
    "\n",
    "\t\tself.__play_move(action)\n",
    "\n",
    "\t\tself.__game_over = self.__game_is_over(action)\n",
    "\t\tif self.__game_over:\n",
    "\t\t\twinner = self.__winner(action)\n",
    "\t\t\tif winner == self.DRAW:\n",
    "\t\t\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\t\t\t#elif winner == self.PLAYER1:\n",
    "\t\t\t#\treturn self.__game_over, self.board, active_player, 1, inactive_player, -1\n",
    "\t\t\t#else:\n",
    "\t\t\t#\treturn self.__game_over, self.board, active_player, -1, inactive_player, 1\n",
    "\t\t\treturn self.__game_over, self.board, active_player, 1, inactive_player, -1\n",
    "\n",
    "\t\treturn self.__game_over, self.board, active_player, 0, inactive_player, 0\n",
    "\n",
    "\tdef print_board(self):\n",
    "\t\tboard = self.board\n",
    "\t\tboard = np.where(board == 1, \"X\", board)\n",
    "\t\tboard = np.where(board == \"-1.0\", \"O\", board)\n",
    "\t\tprint(np.where(board == \"0.0\", \"-\", board))\n",
    "\n",
    "\tdef __play_move(self, action):\n",
    "\t\t\"\"\"Takes an action and executes it.\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(action)\n",
    "\t\tself.board[y][x] = self.current_player\n",
    "\t\tself.current_player = self.__negated_player(self.current_player)\n",
    "\n",
    "\tdef __action_is_valid(self, action):\n",
    "\t\t\"\"\"Checks if the intended action is a valid one or if it breaks the rules of the game.\"\"\"\n",
    "\t\t# if 41 > action < 0:\n",
    "\t\t# \treturn False\n",
    "\t\t# x, y = self.__coordinates_from_action(action)\n",
    "\t\t# if x >= self.width or y >= self.height:\n",
    "\t\t# \treturn False\n",
    "\t\t#\n",
    "\t\t# height_x = self.__column_height(x)\n",
    "\t\t#\n",
    "\t\t# if y != height_x:\n",
    "\t\t# \treturn False\n",
    "\t\t# return True\n",
    "\t\tis_valid = action in self.valid_actions\n",
    "\n",
    "\t\tif is_valid:\n",
    "\t\t\tnext_valid_action = action + self.width\n",
    "\t\t\tif next_valid_action < self.width * self.height:\n",
    "\t\t\t\tself.valid_actions.append(next_valid_action)\n",
    "\t\t\tself.valid_actions.remove(action)\n",
    "\t\treturn is_valid\n",
    "\n",
    "\tdef __column_height(self, x):\n",
    "\t\t\"\"\"Returns the height of a column which is equal to the amount of tokens placed.\"\"\"\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\treturn np.count_nonzero(column)\n",
    "\n",
    "\tdef __game_is_over(self, last_action):\n",
    "\t\t\"\"\"Returns True if the game is over and False otherwise.\"\"\"\n",
    "\t\tif np.count_nonzero(self.board) >= 42:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\tif self.__winner_in_line(line) != 0:\n",
    "\t\t\t\treturn True\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "\tdef __extract_lines(self, last_action):\n",
    "\t\t\"\"\"Extracts the horizontal, vertical and the diagonal lines going through the last action\"\"\"\n",
    "\t\tx, y = self.__coordinates_from_action(last_action)\n",
    "\n",
    "\t\trow = self.board[y]\n",
    "\t\tcolumn = self.board[:, x]\n",
    "\t\ttop_down_diagonal = self.board.diagonal(x - y)\n",
    "\n",
    "\t\tmirrored_x = self.width - 1 - x\n",
    "\t\tbot_up_diagonal = np.fliplr(self.board).diagonal(mirrored_x - y)\n",
    "\n",
    "\t\treturn row, column, top_down_diagonal, bot_up_diagonal\n",
    "\n",
    "\tdef __winner(self, last_action):\n",
    "\t\t\"\"\"Returns the winner's number or 0 if the game resulted in a draw (Requires the game to have ended).\"\"\"\n",
    "\t\tlines = self.__extract_lines(last_action)\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\twinner = self.__winner_in_line(line)\n",
    "\t\t\tif winner != 0:\n",
    "\t\t\t\treturn winner\n",
    "\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __winner_in_line(self, line):\n",
    "\t\t\"\"\"Checks if a line contains a winner and returns his number if yes and 0 otherwise.\"\"\"\n",
    "\t\ttoken_sum = 0\n",
    "\t\tfor token in line:\n",
    "\t\t\ttoken_sum += token\n",
    "\t\t\tif token_sum == 4 * self.PLAYER1:\n",
    "\t\t\t\treturn self.PLAYER1\n",
    "\t\t\tif token_sum == 4 * self.PLAYER2:\n",
    "\t\t\t\treturn self.PLAYER2\n",
    "\t\t\tif token_sum < 0 < token or token_sum > 0 > token:\n",
    "\t\t\t\ttoken_sum = 0\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef __coordinates_from_action(self, action):\n",
    "\t\t\"\"\"Translates an action into (x, y) / (column, row) coordinates.\"\"\"\n",
    "\t\tx = action % self.width\n",
    "\t\ty = action // self.width\n",
    "\t\treturn x, y\n",
    "\n",
    "\tdef __negated_player(self, player):\n",
    "\t\t\"\"\"Returns the player not passed to the function (Player1 if Player2 is passed and the other way around).\"\"\"\n",
    "\t\treturn self.PLAYER2 if self.current_player == self.PLAYER1 else self.PLAYER1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(game.take_action(3))\n",
    "print(game.take_action(4))\n",
    "print(game.take_action(10))\n",
    "print(game.take_action(5))\n",
    "print(game.take_action(17))\n",
    "print(game.take_action(6))\n",
    "print(game.take_action(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 64)\n",
    "        #self.fc1.weight.data.fill_(0.0)\n",
    "        #self.fc1.bias.data.fill_(0.0)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        #self.fc2.weight.data.fill_(0.0)\n",
    "        #self.fc2.bias.data.fill_(0.0)\n",
    "        self.fc3 = nn.Linear(64, 42)\n",
    "        #self.fc3.weight.data.fill_(0.0)\n",
    "        #self.fc3.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DeepQPytorchAgent:\n",
    "    def __init__(self, learning_rate=0.0001, discount=0.95, exploration_rate=1.0, iterations=10_000, trained_model=None):\n",
    "        self.q_table = np.zeros(shape=(42, 42))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_delta = exploration_rate / iterations\n",
    "        \n",
    "        self.input_count = 42\n",
    "        self.output_count = 42\n",
    "        \n",
    "        self.define_model(trained_model)\n",
    "    \n",
    "    def define_model(self, trained_model):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if trained_model:\n",
    "            self.model = trained_model.to(self.device)\n",
    "        else:\n",
    "            self.model = Model().to(self.device)\n",
    "        \n",
    "        #self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def get_Q(self, state_batch):\n",
    "        return self.model(state_batch)\n",
    "        \n",
    "    def next_action(self, state, valid_actions):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.random_action(valid_actions)\n",
    "        else:\n",
    "            return self.greedy_action(state, valid_actions)\n",
    "        \n",
    "    def random_action(self, valid_actions):\n",
    "        action = random.randrange(0, 42)\n",
    "        while not action in valid_actions:\n",
    "            action = random.randrange(0, 42)\n",
    "        return action\n",
    "    \n",
    "    def greedy_action(self, state_batch, valid_actions):\n",
    "        #print(\"States before:\", state_batch)\n",
    "        #print(\"Greedy:\", torch.max(self.get_Q(state_batch), 1)[1])\n",
    "        Q_values = self.get_Q(state_batch)[0]\n",
    "        Q_values = self.normalized_Q(Q_values, valid_actions)\n",
    "        action = torch.max(Q_values, 0)[1]\n",
    "        assert action in valid_actions, \"Only valid actions may be selected, action {}\".format(action)\n",
    "        return action\n",
    "        #return torch.max(self.get_Q(state_batch), 1)[1]\n",
    "    \n",
    "    def normalized_Q(self, Q_values, valid_actions):\n",
    "        '''Takes a single Q value array and sets invalid actions to -1.'''\n",
    "        for x in range(0, 41):\n",
    "            if not x in valid_actions:\n",
    "                Q_values[x] = -1.0\n",
    "        return Q_values\n",
    "        \n",
    "    def update(self, old_states, new_states, actions, rewards, valid_actions_batch):\n",
    "        self.train(old_states, new_states, actions, rewards, valid_actions_batch)\n",
    "        # TODO: Maybe change algorithm?\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate = max(0.2, self.exploration_rate - self.exploration_delta)\n",
    "        \n",
    "    def train(self, old_states, next_states, actions, rewards, valid_actions_batch):\n",
    "        old_state_values = self.get_Q(old_states)\n",
    "        next_state_values = self.get_Q(next_states).detach()\n",
    "        \n",
    "        #print(\"Max:\", torch.max(next_state_values, dim=1)[0])\n",
    "        for x in range(len(next_state_values)):\n",
    "            valid_actions = valid_actions_batch[x]\n",
    "            next_state_values[x] = self.normalized_Q(next_state_values[x], valid_actions)\n",
    "            \n",
    "        new_rewards = rewards + self.discount * torch.max(next_state_values, dim=1)[0]\n",
    "        updated_state_values = old_state_values.clone().detach() # Check if detach could cause problems\n",
    "        for index, (reward, action) in enumerate(zip(new_rewards, actions)):\n",
    "            updated_state_values[index][action] = reward\n",
    "        \n",
    "        #print(\"Old state values:\", old_state_values)\n",
    "        #print(\"New reward:\", new_rewards)\n",
    "        #print(\"Updated:\", updated_state_values)\n",
    "        #print(\"Actions:\", actions)\n",
    "        #print(\"SelectedByActions:\", updated_state_values[actions])\n",
    "        #updated_state_values[actions] = new_rewards\n",
    "        \n",
    "        # in your training loop:\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = F.smooth_l1_loss(old_state_values, updated_state_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff 3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"stuff {}\".format(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('old_state', 'next_state', 'action', 'reward', 'valid_actions'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(game, active, passive):\n",
    "    old_state = np.copy(game.board)\n",
    "    old_state = torch.tensor(old_state.flatten(), device=active.device).float()\n",
    "    valid_actions = torch.tensor(game.valid_actions, device=active.device).clone().long()\n",
    "    \n",
    "    action = active.next_action(torch.unsqueeze(old_state, dim=0), valid_actions)\n",
    "    #action = torch.tensor(action, device=active.device).long()\n",
    "    \n",
    "    game_over, next_state, _, reward, _, _ = game.take_action(action)\n",
    "    next_state = torch.tensor(next_state.flatten(), device=active.device).float()\n",
    "    reward = torch.tensor(reward, device=active.device).float()\n",
    "        \n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, reward, valid_actions\n",
    "            \n",
    "    # if the move was invalid, add data and repeat\n",
    "    if reward < 0:\n",
    "        return False, old_state, next_state, action, reward, valid_actions\n",
    "        \n",
    "    # Play another move until the move is a right one and add the data to the memory\n",
    "    passive_reward = -1\n",
    "    counting_stars = 0\n",
    "    while passive_reward < 0:\n",
    "        passive_action = passive.next_action(torch.unsqueeze(next_state, dim=0), game.valid_actions)\n",
    "        game_over, _, _, passive_reward, _, cur_reward = game.take_action(passive_action)\n",
    "        \n",
    "        counting_stars += 1\n",
    "        if counting_stars % 1000 == 0:\n",
    "                print(\"Counting:\", counting_stars)\n",
    "        \n",
    "    cur_reward = torch.tensor(cur_reward, device=active.device).float()\n",
    "    if game_over:\n",
    "        return True, old_state, next_state, action, cur_reward, valid_actions\n",
    "    return False, old_state, next_state, action, reward, valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(active, passive, memory, batch_size=128):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*batch))\n",
    "    #print(\"States before:\\n\", batch.old_state)\n",
    "    #print(\"Next states before:\\n\", batch.next_state)\n",
    "    #print(\"Actions before:\\n\", batch.action)\n",
    "    #print(\"Rewards before:\\n\", batch.reward)\n",
    "    old_state_batch = torch.stack(batch.old_state, dim=0)\n",
    "    next_state_batch = torch.stack(batch.next_state, dim=0)\n",
    "    action_batch = torch.tensor(batch.action, device=active.device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=active.device)\n",
    "    #valid_actions_batch = torch.stack(batch.valid_actions, dim=0)\n",
    "    valid_actions_batch = batch.valid_actions\n",
    "    #action_batch = torch.stack(batch.action, dim=0)\n",
    "    #reward_batch = torch.stack(batch.reward, dim=0)\n",
    "    \n",
    "    return active.update(old_state_batch, next_state_batch, action_batch, reward_batch, valid_actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFourSimulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150 # Number of games to play\n",
    "batch_size = 128\n",
    "memory = ReplayMemory(10000)\n",
    "active = DeepQPytorchAgent(iterations=epochs*batch_size*20)\n",
    "passive = DeepQPytorchAgent(iterations=epochs*batch_size*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,\n",
      "          0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "3\n",
      "tensor([[ 9.0921e-01, -2.6158e-01, -9.2689e-02,  4.4667e-01,  2.0170e-01,\n",
      "         -1.0691e-01,  1.4898e-02, -5.5755e-02, -5.1560e-02, -7.2132e-01,\n",
      "         -1.6313e-01,  2.0654e-03, -5.5427e-02, -6.0464e-01, -8.1798e-04,\n",
      "         -6.1640e-01,  1.1654e-01,  3.3788e-01,  4.7357e-01, -4.5214e-01,\n",
      "         -3.1527e-02, -6.9223e-01, -1.3575e-01,  1.7507e-01,  3.2202e-02,\n",
      "         -1.0946e-01, -4.6527e-01, -6.0050e-01,  1.5571e-01, -8.6690e-03,\n",
      "         -8.8917e-01, -5.1978e-01, -2.1948e-01,  4.3289e-01,  1.6174e-01,\n",
      "         -5.1315e-02,  3.3754e-01,  9.2839e-02, -2.2890e-01,  4.0602e-02,\n",
      "         -2.4755e-01,  4.2710e-01]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_batch = torch.unsqueeze(example_board, dim=0)\n",
    "print(y_batch)\n",
    "print(active.next_action(y_batch, [0, 1, 2, 3, 4, 5, 6]))\n",
    "print(active.get_Q(y_batch))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(y_batch)[0], list(range(7))), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Using memory replay\n",
    "total_rewards = [ [torch.tensor(0)] for epoch in range(epochs) ]\n",
    "total = [torch.tensor(0)]\n",
    "print(total_rewards)\n",
    "start = int(round(time.time() * 1000))\n",
    "for epoch in range(epochs):\n",
    "    #invalids = []\n",
    "    #invalid = 0\n",
    "    #for iteration in range(1, iterations + 1):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    game_over = False\n",
    "    game = ConnectFourSimulator()\n",
    "    while not game_over:\n",
    "        optimize_model(active, passive, memory, batch_size)\n",
    "        passive.model.load_state_dict(active.model.state_dict())\n",
    "                      \n",
    "        game_over, old_state, next_state, action, reward, valid_actions = transition(game, active, passive)\n",
    "\n",
    "        memory.push(old_state, next_state, action, reward, valid_actions)\n",
    "        total_rewards[epoch].append(total_rewards[epoch][-1] + reward)\n",
    "        total.append(total[-1] + reward)\n",
    "end = int(round(time.time() * 1000))\n",
    "print(\"Time taken:\", (end - start))\n",
    "print(\"Time taken in sec:\", (end - start) / 1000)\n",
    "# Time without batching: 657sec (10), 95 (5), 399 (5)\n",
    "# Time with batching: 12 (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n",
      "0\n",
      "------------------------------------------\n",
      "tensor([ 1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "tensor([ 1,  2,  3,  4,  5,  6, 14], device='cuda:0')\n",
      "6\n",
      "------------------------------------------\n",
      "tensor([ 1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "tensor([ 1,  2,  3,  4,  5, 14, 20], device='cuda:0')\n",
      "4\n",
      "------------------------------------------\n",
      "tensor([ 1.,  0.,  0.,  0.,  1.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "tensor([ 1,  2,  3,  5, 20, 11, 21], device='cuda:0')\n",
      "20\n",
      "------------------------------------------\n",
      "tensor([ 1.,  0.,  0.,  0.,  1., -1.,  1., -1.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "tensor([ 1,  2,  3, 11, 21, 27, 12], device='cuda:0')\n",
      "2\n",
      "------------------------------------------\n",
      "tensor([ 1.,  0.,  1.,  0.,  1., -1.,  1., -1.,  0.,  0.,  0., -1.,  0., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "tensor([ 1,  3, 21, 27, 12,  9, 18], device='cuda:0')\n",
      "9\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "errors = memory.memory[-6:]\n",
    "for error in errors:\n",
    "    print(error.old_state)\n",
    "    print(error.valid_actions)\n",
    "    print(error.action)\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = [ [ val.tolist() for val in rewards ] for rewards in total_rewards ]\n",
    "total = [ val.tolist() for val in total ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average moves to finish: 6.3933333333333335\n"
     ]
    }
   ],
   "source": [
    "print(\"Average moves to finish:\", len(total) / epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5a89978>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJztnXmYHGd17t+vqnqmNZs0I2kkWaOxFsuWjDfZki1svAoh2/hCYkxiLhBjSAyEJE7IDbExCXATIIQE7HvJNTg4kLAmeAnGgI0Fxgtg2ZJly9ota7FH60gaaaQZzUx31Xf/qKWrq7fq6uquru739zzzTHd1LV9tp0693/nOEVJKEEIIaQ6UqBtACCGkdtDoE0JIE0GjTwghTQSNPiGENBE0+oQQ0kTQ6BNCSBNBo08IIU0EjT4hhDQRNPqEENJEaFFsdNq0aXLu3LlRbJoQQmLLunXrDkspp1eyjkiM/ty5c7F27dooNk0IIbFFCLGn0nVQ3iGEkCaCRp8QQpoIGn1CCGkiaPQJIaSJoNEnhJAmgkafEEKaCBp9QghpIiKJ0yeEkDjw+KYD2LT3OCa3teDWS+dCUUTUTaoYGn1CCCnAXQ+/gsMnJwAAly+chjNndEbcosqhvEMIIQUYTxuY0zMJADCRNiJuTTjQ6BNCSAF0Q6JVU53PjQCNPiGEFCBtSLRqivO5EaDRJ4SQAuguo09PnxBCGhgpZZa8kzao6RNCSMNiyzmtCXr6hBDS8NhGnpo+IYQ0AbaRTyas6B2dRp8QQhoW28hnPH1q+oQQ0rDYRj7TkUtPnxBCGhbdkXfYkUsIIQ1PyunItTx9avqEENK4eDV9evqEENLAOJp+giGbhBDS8OjekE1G72QQQkwRQjwghNgqhNgihHhzGOslhJCoSDfo4KywiqjcA+AxKeVNQogWAG0hrZcQQiJB93TkNoqmX7HRF0J0AbgCwAcAQEo5AWCi0vUSQkiU0NMvzHwAgwC+KYQ4H8A6ALdLKUdCWDchhNQM3ZD4Xz98GQeHx3BiLA0g05H7vTWv4+ntgwCASQkVn7/xXMzoSkbW1qCEoelrAC4EcK+UcgmAEQB3eGcSQtwmhFgrhFg7ODgYwmYJISRcBk+M4+H1e7Hv2CkkEwquPHM6zp09BTcvm4MZXa1I6QaOjabwi62H8PIbx6JubiDC8PQHAAxIKddY3x9AHqMvpbwPwH0AsHTp0sZ4TyKENBR2mOYfX3UGfm/ZHGf6P7zrPOfz1gPDuPbuZ2Kr8Vfs6UspDwB4QwhxljVpBYDNla6XEEJqjW3INVUUnEdT4q3xhxW986cAvmtF7uwEcGtI6yWEkJphG3JVKWb0zd/i6umHYvSllC8BWBrGugghJCocT18pLILYD4S4evockUsIIRZ2UrWinr5qe/rxHKFLo08IIRYZT7+w0bcfCKmYZt2k0SeEEAs7ekf10ZEbV02fRp8QQizK8fSp6RNCSMwpL3qHmj4hhMQauyOX0TuEENIEOJq+H0+fHbmEEBJvbE0/UaQjl54+IYQ0CH40fSEENEU4bwVxg0afEEIs/IzIBcyHAj19QgiJOX48fcDU9anpE0JIzLHDMIvF6QP09AkhpCHwk3sHADRV4YhcQgiJO37y6QPx9vTDyqffdHz9qdfwzz/fnjXtjN4O/PT2ywsuc/+zu/DFn23NmjZ3Whseu/0KKCU8i0o5PprCyq88hWOjqazpmipw7/suwpVnTq/q9gmpJV98bCvuf2ZXZoIA7rh2ET74lnkAgJPjaaz88lM4cnIiazld2iGbxf3hFlXBD154HQ+uGwAA/M6S0/CPN53v/L7twAm8+2u/wVjKlItmd0/Cz//iipLrrQU0+gHZtG8YyYSC9y4/HQCwdvdRvLB7CIYhCxrwzfuG0aopeN+bzWVe3DOENbuOYkI3kFTUqrb3wPAYDp0Yx9vOnoEFvR0AgIm0gfuf3YVXD56g0ScNxca9xzGlLYF3XdQHAPj2b/dgy/5h5/fBE+PYf3wMKxb14syZnVnLzuxKYlpHa9H1/+3/OBsvWTVyf/bKfmzcO5z1+67DIxgeS+PdF/Vh//ExPLvjMEbG05jS1hLG7lUEjX5AdENiemcr/vraRQCAr/7yVbywewhpQ6KlgNHXDQPd7S3OMvc9/RrW7Dpak9dEO6b4xgv7cO05MwEAI+Np3P/srthqk4QUQjck+nvanHvtxy/vy7rO7Q7bdy6ZjXecf1rZ61/1pplY9SbzPto5eBK7D4/mbB8A/vDy+Viz6wie3XG4buSg6N81YkraMLJe1TS1dLrVtCGztEInRWsNQr/yjTS021IvFyMhYeG91xKqknWd258TIciqmqrkDNSyv2uqqLtUzDT6AdENmdXDrzlDswuP0tMNmRUKljG61R/Zly/+uN4uRkLCwrzXMuZNVUTWde43SscPmmfd9vbt37Q6S9tAox+QtMeAq0661eKevuq5EEstExb5Rhraza+Xi5GQsEjnccrczpXfKB0/5IvkcTtZap0laKPRD0hhT7/wic3x9GvoAeTzbOwcInHNC05IIXTDyHHK9Dzyjloi3YIfinv6Sk3f6P1Aox+QlG54Xh/9afpq1oVYO3mlkGcT53hjQgqR1vN5+u6O3NIVsvyiKoo/T79O7jMa/YAE8/SN6Dz9AnnC45xDhJBC6J6O3FxPv3TefL/k9fT1TDoHavoNgjc6wI9u5/U+Mh5A9V/7Cnk29PRJI+LtyNUUxZE47d+B4nnz/aKpAmndG72TebOu5Ru9H2j0AxIkEsfrfdTW088frRDnHCKEFCJfoEVkmj49/cbA9NpzI3GKndhUgeiddA3klUK1P9UYF4MgpBBp3ciWX1WBlOs6z9wP4Wj6KR+avvdtICpo9ANSMBKniAHP0fRrODiqmKZfi4cOIbUkn/yab0RutTR990OFnn6DkDYMqPlG1xaL3tG9Dwp7mdpp+l4NU1NzL1hC4k5uoEW2pp8u0McVBPuBImX2Q0UIQFGEr9H6tYRGPyA5mmElmn4t5J1Cmn6ecDNC4k46pyPX6+mHOyLXvc7M9kXWNurlPqPRD0i+OGCg+NNcr7MRuXYb6sUDISQsvJ6+qmb3XRXq4wqCmkemdW8/Yxuo6cca3ZBIlNmR6307qK2mX8jTZ0cuaTzShpHzVp3P0w8jDUMij7SbdtmHWgZs+CE0oy+EUIUQ64UQj4a1znombciyNf0c76OWI3L1/LU/6emTRsQbaOEdjxK2pu9ep7192z7YD5Z6uc/C9PRvB7AlxPXVNflyewClPP0oR+Ranr63I5eDs0gD4k1umOvphxi9k8eou+/1eoveCaWIihCiD8DbAXwOwMfDWGfU7Do8ghNjZmlBVRE4c0Ynth884ZzYibSRV9PfcegkutsSAIDTp7ajo1XD1gPD0A2JsZTh8fTNz68NnsSMgVYoQmDRzE6nt78Uh0+OY9+xUznTZ3QlMaMrCQAYnUhjx6GTGBg6ldVOdxuGRlPYMHAMM7uS6LWWGxlP47XBk77aMXNyEr2dSV/z1hN7jozg+KkUTpsyqWSlJBIPxtM6tu4/ASnhccoUnErp2DBgVrt6/ahZ9CQUTd/azoaBY+hpNytjHRoed6bbD59dh0ewYeAY5k5rR1cyUfF2gxJW5ay7AXwCQGepGePAa4MnseKfn8qadl7fZGwYOJ41rbM1c/g6k+bnv3t0szPtknk9uOH80/A3/73RmdaRzCzTYS3/D666uXdetwgfvnKBr3a++2u/xa7DIznTu5IaNnxmFQDgroc34uH1ewGYdT29NTo7kgk8vX0Q7/jqrzGtowVrP7USAPCJBzfgJxv2+2rHzK4knvvkCl/z1gv7jp3ClV/6FQCgv6cNT3/i6mgbRELhHx/bhvufNWvjdnjuz2OjKbzjq792pmmKQGuicqNvb+cD33wha/qC6e0AgPZWsxTql5/Yji8/sR3funUZrjqrt+LtBqVioy+EuAHAISnlOiHEVUXmuw3AbQDQ399f6WarytCIWSz54yvPxFkzO/Hhb6/DrkHTuN773gvRoilQhMCyeT3OMvOnd+CBj7wZx0+Zbwf/8uQOHBtNOeu67/0XQVMFls3NLDOnpw0PfvRSHBs15/nwt9dhyFO4vBhHRyZwzaJevPeSzPF85OV9+NFL+5xavUdHJjBvWjs+9fbFmDV5Uo7R/9JN52Hj3uN4aP1ePLbxQNYxOKO3A3det6hoG364dgBPbjvku831gl0gvqe9BUOjEyXmJnFhaGQC0zpa8E/vPh/L5091pv/pNWdg+fweuELpMaMriWSi8trU1587Cz3tLZhIZwdELJhu1qLu7Uzi4T++FEctW3DO7MkVb7MSwvD0LwPwDiHE9QCSALqEEN+RUr7PPZOU8j4A9wHA0qVL60PcKoCtvS09vdu5cMbSOgBgxeIZaNHyewdLXQb9oRf3YuuBYWddK8+eASFy9cOLTu92PidUpaywLt2QmDu1HSsWz3Cm2cWf7Vq9uiHR3ZbImseNLQW9PHDcGWAihKnzT21vKbiczbo9Q1i9pa5PZ15smW7ypAQODo9F3BoSFmlDoqNVy/GkO5MJXLOo+LUclISq4PKF04vOs6S/u+jvtaTidxsp5Z1Syj4p5VwANwP4pdfgx41MOJcCRREQAkiVmavDjorRDVPHz2fwvWhqeZ2q3rA0c7vZEUHmPKVPs10r1F7OO5CsYJutjmD3aMQ4YIeptmocnNZImNctI9GLwaOTB29Mux1vq1jDqv1gG3BvbH7RZcoMn/SGpQGZNAu2Ucs3Tz68A0y8IxoLttm6weJmN+3j3JpQ6yaUjlSON0KO5BKq0ZdS/kpKeUOY64wCW2LxDqMup6ffNuC67t/o56vAU4x8DxTvKF9vta5i7XUv5w1JLdzm+ioF5xf7OLdqSk7eFBJfvGNhSC709PPgrSfrNf5+sA24X6Nrb8dvFSvDkJAyNx+4NybYt6dvrcfx9PVgD4u4oLuMvvs7iTflvFk3KzT6efAO0XZG1pVxMTmefhkaYzlVrNKeNmbW4dH0PXn/i7XXvZxfTb/ekkn5JePpq1nfSbyhp18aGv082AURvCPqvKNZi6EqZgm1sjx91X8enEJZAr2evl+N01vowZscrmCb7YdFneQV8Yu9n3acNo1+Y2CmL6dZKwaPTh4yQ7SzEyYF8/T9dyyV5+kXzqUDZIywN0dQsfba89v//XUAx9No2u1NWp5+3B5aJD/09EtDo58Hbyk123Mox4NQXdE7fi/ChKL4Nj4FPf080TsJP9q8p9CD35sn7pp+0vH049URTfKTL4yZZEOjn4ccTT9AR26Wpl8VTz//uIGc6J0yNf3AslDMjKZX04/bQ4vkp5z7rVmh0c+DN07f8fjL0vSt6B2fUTD2+v2OyM1EGOWP3rEHk/mP3sku9FBu9E695Ar3i72f1PQbi5RPJ6eZ4dHJg7fKVFBPHzCz/vmVhcLR9L0jcqus6cc1ekdnyGYjQk+/NDT6eUjp2bm2bb27rI5c1Tb6/jVGTRG+PeZClX+8mn750TvukE0f6RvqrOizX7zyjn3OSbxJG0ZZUXbNCI1+HnTD25GbnRfbD46nnyovesev8SxW/hBwdcj6lWly0jA0h6Zvd+TG7aFF8kNPvzQ0+nnwDnxycvCUqekDprzjO3pHVcqO089X6BzIlmm86ZTzkSn36I7Tb+DoHcuzt1Prxk2eIvnxmzOqmeHRyYPXoAZJw5DR9I2yNH3fnr5eyNOvLPQyrTeJps80DA0JPf3S0OjnwTYI9rUTZHCW6jL65eTe8Ws8vRKUd7tBZRrdkAXz+uRvczyNZib3Dj39RsJv4EIzE1a5xIbhRy/txYt7hqApmRz4XpnHD7ahPXJyHH3dk3wtoyoCgyfGnXJvxdhr1bzNV+gcAB7buB+vHjwBQ/prt72Pj76yH5v2DWdNK9VmAPjxy/uccpItqsA7l8yOtA5oPnYOnsST2wYBAGt2HQWQ8fR//PI+nDYliekdrXjk5X04fHICAmbxm572Fjy0fi8m0gZaNAWXLpiKp7YNoqNVw7su6it6fH/2yn7sO24WaZk/vR1XR1gmr1E5cnIcP355H3Rp1namp18cGn0XB46P4fYfvAQAmDet3Zne39OGX+MI5nS3+V5XX3cbhACGx9Lo87lcf08bHt90MKvObjE0RWDW5OyC5L1drWjRFHz/+TecaX7a3duZRIuq4HtrXnftQ+mH1czJ5nL/8ds9WdNbNRW/t2xOyeVryd2rX8UjL+9zvs/sSqJ/ahsSqsD9z+5CZ1LDu5fOca4BANh9ZAQXnd6dVed49pRJ2GsVpF80qxPn9U3Ju73hsRQ++t0Xne/JhIKtf3dd2LvV9Pxw3UBWnely7tNmhEbfxVjKLIn4979zDn7fZbA+/7vn4o7rFqMr6f9wvWXhNGz8zCqkDel7uU9evxh/cs1C39toURVMasmu8Tlr8iRs+PTbMJ7OhJ26C0QXYk5PGzZ8pvzl5k1rz1ru6MgErv6nX2HcKi9ZT4yndSzs7cADH70UADApoaJFU7Dh06tw/md/jvG0gXHrGvjCjefiK09sx1hKx3jK3LdvfmAZbv3WC05NYwAYSxXueLdrpn7y+kU4cnIC9z2zs1q71tTY5+elv10JRRF194ZZb9Dou7B13c6klhXxIoTA5EnlX0jtPoymm6Db8ZJMqIEKPoexnF2MJFWHI3TTukSLpuQc40ktalZWVCBzDaQNiZQV0dTd3gIAGHMVwC4WbWV3ine0JnBqwoCUTAhWDdKGASGAKW0tUTclFrAj14XduecnxJHkx5v7p54oFpFkd6K7k+0l1Ez+JCB/TH+x/XSPmvYOmiPhkTakU9KUlIZHykXayB6JS8onUceplouNMtY8Bl5TFCcthv0gsNMwuym2n+5R03EdzxAH+PZUHjT6LgqFQRL/eBO31RNpo3D4bKa8pfXgVwU0K9V1poh67u1SLBW2e9R0XMczxIF0GXWoCY1+FoVSGxD/qKJ+jVuxgTt2fWL3g9/x9D0x/W58efqKEtsKY3FAZ76dsqDRd1EotQHxj6IIKKI+ZYxiBW28Bl61dHjdMDJpmLU8nn4xTd81ajquFcbiAIuhlwetmwtvdk0SDM2SSuqNop6+Y+DzaPrWtBaX0bc/F+uYdb81UNOvHtT0y4NG30WhdMWkPMrJIVRLilURy+vpW6mudUNCEdl9PbbXXywVtrt/IK7ZSOMAk6yVB4+Ui0IlCEl5lFMXoJaU1PQN6Ug5pneuQLceBJqiQAjhZFq1xyUUe7jR068NZlQW71m/0Oi70HVq+mGgllH2sZYUK7DhLm9pfjc1/bQl+XirpzmeftE4fUbv1IJifTUkF1o3F4zeCQdNEUjVoXErOTjLNSJXsyQZ3ZBI6ZlMpbZDkEnJXHpErhm9U1oOIsFI6/4LFREa/Syo6YeDaoU/1hvFir17NX1bkklbA7ZU1evpl07J7B7sR02/epiePk2ZX3ikXHBEbjjUc/ROoeH6mZQLtqZvRe/oMquj0PYo7ZQMxTz3TFqPTF8ANf3wYeGU8qDRd+HOu0KCo9Wtpl+4wIY35YJqdeSmDQO6a8RneZ4+Nf1aQE2/PCo2+kKIOUKIJ4UQW4QQm4QQt4fRsCjIyDt8FlaCbUDrDb1IFTE7UicrX47l/buNip1bKJN8zU+cfkbTp6cfPrphlFW/utkJI7VyGsBfSilfFEJ0AlgnhHhCSumvEkgdwZDNcLDDH+uNckfkqo6mb+RUT2sJGr1Th30dcadYXw3JpWKXVkq5X0r5ovX5BIAtAGZXut4o0Knph4Jax5p+8Th9w+Od53r6msvjL5VuQs+TWrkeH4ZxR+fgrLIItYiKEGIugCUA1oS5Xr8cOjGGL/x0q1MBq1VT8NfXLcKsycXL/t29eju2HTiB3UdGAdDTrxRNEXjpjWP46HfWAQCWzu3Bh94yr6ZtMAyJz/54Ew6dGHemjaX0oiNy9xwexfeff935rioKjoxMYO3uIXRNMm8VxWX8NUXBT17Zjx2HTjrr+b2lc3D+nCn4+59sxqsHT7rWZS539+rt+O4as7Skpir4+Mozs0pzEn8cGh7DF35m3uuvHjqJC+bkL1lJcgnN6AshOgA8CODPpZTDeX6/DcBtANDf3x/WZrNYu3sID6/fi9OntkERArsOj+CKM6fjxgv7ii73f3+5A5MnJTCtowVXnTUdnSy3VhErz56BRzfsw2uDJ3Hg+BjW7RmqudHfPzyGf//tHszoanUqZZ05oxPL5/fknf/qRb3YfWQEhpS4ZlEvOlo1XL5wGl7cMwQJibedPRMAsOpNMyAAXHHmdAghsGnfcbw2aBr33YdHISVgSImHXtyL/p42XLpgKnq7WtGZ1HDR6d0YHktheCwF3ZB4bXAEF/ZPwbxptT02jcCaXUfx8Pq9mDu1DTO6WnH1WdOjblJsCMXoCyESMA3+d6WUD+WbR0p5H4D7AGDp0qVVece1E6bdf8sytGoKLv/HJ0u+Tktpdt69f/np+IuVZ1ajWU3Hn61YiD9bYdb6vevhV/DYxgM1b4M9TuATqxbhXRcVf+gDwHsu7sd7Ls52Rq4/dxauP3dW1rS/WrUIf7VqEQDkOBPX3fMMdCmdUpH3vu9CvOm0yQDMaJ8Hrdq8gFk0/bzP/JxyT0Ds4/bNWy/mm1KZhBG9IwDcD2CLlPLLlTcpOFm5TlR/IXLsvK0uWkSRPHZd21oOtLNH9fpJ0W1fb/XY9xEHbAeP9235hNH7cRmA9wO4RgjxkvV3fQjrLRvvEHr3tEIwTLO6aKoSiTeruyJnakUmAqj0A4chnJXB0fPBqVjekVI+C6Aujrzbw7JHXup68UFC9PSri+np136gVhQD7byF1IttW2MIZ0UwT1ZwGsq9zYqL9inv6DovnmoSVW79KKqg5Yv1L4SiCAhRn7WE4wCr3AWnoY6YrrtzofuLi/bzKk6CE5Wm7y5gUityRvWWMEhRHZtGgJ5+cBrK6DsXQgBNnxdPdVAVxQxjrLFx8yOxhE05nr79OzX9YLgHvpHyaEijbw+cAUp7+ilq+lXFfoNK1VjGiMITtEf1pn1GlmiK4oR3kvKgpx+chjL6bq/dvhb8a/oNdSjqBtWnzBY2kWn6ekbeKSUtqUp9ZiONAzoz4gamoSydHQmRsOqZaj5uKlv7ZZa+6hBVPHoknr4rKyeAgrn7bRIqNf2g0NMPTkMZfd0wIEQmP4qfFL/U9KuL06FeYxkjCs1X9XTkUtOvHnbyPHNsKCmHhjL63hqomvW6XWoZe14SPqpaOgVxNUhHEIpr1gY2fI8RqNcKY3EgZRh01ALSUEZf9+RL9+NJZbyyhjoUdYPf0NmwcY/OrhV2bWDvG2fR+Wn0A+GuZkbKo6EsnbuWKWCmACg1GpSefnWJqiB4FOfVjrv3vnGWmp+UD0skBqexjL6e/crnp4KTHV7HC6g6RJVuwF3gvFbkK69YDNVK0EbKRzck82UFpKGOGjX9+iOqguDRaPqKU1zdz8OmXmsJxwF6+sFpKKNvPv1dmr7qX9On11Ad7ELikcXp11rT99TULUYiogykjYBuGEjQ6AeioSxdjqbvIzqC8b7VJWpNv9bRO2nD8K3p09MPTtqQNc2r1EiEWiO3lkykDbzr3t/gwPCYM234VAozupLOd00ReGzTASz73GoAgCoEvnDjubh6US8A4OH1A/jsjzc785LwsQe9ve8ba5y3qVZNwb/+wVIsntVV0brHUjpu+tpvcHDYrIPb3qLi2x+6BEdGJvCp/94IoPaa/ljKwIMvDjglGouRUAV++9phLPvcanzsqgX4wGW1LZv4jWd24utP7wRgVg77eB1Xjvv8T7fg4fV7ne/Dp1I4bUrx2tckP7E1+sdGJ/DK3uNYNrcbZ/R2OtMvmZepgXr7Wxfi1zuOWN8kvv/8G9gwcNwx+utfP4bRCR0fvmI+Fs3qBAkfuyj66IRZrH54LIWfbNiP7QdPVGz0h0YnsHHvMC6e14OuZAKrtxzE7iMj2Dt0CgDwPy/pR3db7eod/+6S2Tg2moIhgQv7Sxfq/siVC7B6yyE8umEf1u4ZqrnRf37XUaR0AwlVwQu7jtZ02+Xy3M4jSCgCV57V60wrVO+YFCe2Rt9+Lb7poj78/rL8hdZvOO803HDeac73H7zwRlZahrQh0ZVM4M7rF1e3sU1MVzKBv7nhbOf7niMj+MmG/aFo2XZn7bsv6sMZvR1YveVgVpbLP3/rwpqO2DyjtxOf+91zfc+/YvEMrFg8A+v2HI2s5kBf9yR0tibqvm8hrUucfdpkfOFG/8eX5Ce2mn46QKI0c8Rk5uLmAI/aE2Y0j7uz1smqqvvPZ18vqBGNzDUjYBRoajTVzcpB99lPQkoTj7siD+kAuVW8IyA5lLv22IY4jLh9p1CKomR1GKdiNvZCiyheP20Y0BQRiw7llGGw4zYkYmv0g4TkJRQly9h4QzxJ9cmkWq7cyGXVT3CVx4yigEolRGV007oZ66557ot6hJ5+eMTW6AcZVGXG7Wdr+ryQakvCZ+1iP7gTm7lz/ESRd6cSEj7Gk1QD3ZBIqMLXyPWo8TvgjZQmtkcxSKI0b64TnRdSzQmzqEo+TT8dS00/qjrCpqavUtNvKuJxV+QhkKfv8Wg4lLv2OMY5DE/f9eC39V63px+XU2sXVK81tiGNhafPwVihEV+jH6CzzluT1O9weRIeVfH0XfJO2k6DEKMCG9F6+vHoyLXPKamc+Br9wJ5+tqZPT7+2hJl1MxO9I7I6iON2Xv2U9awGtiGNjacfo3Naz8TW6AdJlKZ5apJSJ6w9iiIgRDjROwU9/ZiNv7ALqtca25CadSfq2+jzXg2P2Br9IAm1vB4NvYdoCKt4iPsacMtGcTuvWoTRO/Hy9GNrruqK2B7FIIWvvSMfdYPRO1EQVplAXc9E6bg7iONWYEONqCPXjNNXYlHMhZ5+eMTnzvAQpEhGjqevc0RuFHg71IOST9NP6wbSMRtpbRdUrzVpl6Zfz/KOlDKn/jUJTmyNfpARud4oBQ7OigZvh3pQ3IOw3Jp+Ooaavh6Bpq9bYZBR5f7xC6vbhUtsjX4qwIXgzXHCNAzRkFDD8SzdHbmKIqAIWJX/w5YpAAASDElEQVSr4nVewzoe5ZI2JBIx0PRZ3S5cQjmKQohrhRDbhBA7hBB3hLHOUuiuZFt+ye/p80KqNWFp+mmXpm//t1Mrx+m8hnU8ykV3afq6ISFlfRp+evrhUvGdIYRQAfwLgOsAnA3gPUKIs4svVTnuvCt+8dYkpU4YDX7KWPrBScVhefWZGrXxOq9hHY9ySVtvRAnXaOZ6RI+gyH0jE0YRlYsB7JBS7gQAIcQPALwTwOYQ1p3FkZPjODmeBgAcOmGWyCtX0x8ZT2PPkREAZrk9eg+1R1UEjp9KYc+REUzvbEVbS+nL8OjIBE6MpbKmHTphlsq0z6GmCBwdmcDwWCpW51VVBFK64VyXADB5UgJT2lpC35aUEgNDp2BI6XR422/Luw6PoEXL+IEzupJIJtTQ21BOGwHg2Kh53uMk2dUzYRj92QDecH0fAHBJCOvN4e7Vr+Lbz+3JmpbU/F+UbS0qth44gSu/9Ctn2qSW2l/UzU5bi4onNh/EE5sP4pzZXXj0Ty8vOv/RkQlc8vnVBSN+7GtgUouKB9YNAACW+ChXWC+0tagYndCzrstWTcG6v1mJjtZwi9t945ld+NxPt2S2nVDRZt0DK7/ydNa8ly6Yiu/90fJQt++Hrz21E198bGvO9EkRPIAakTCuqHyP35y7UwhxG4DbAKC/P395w1LceOHsrJu5tzOJ7nb/3tBdb1+MlWfPyJr2ljOmBWoLCc7dN1+AzfuG8Z8vvIHdLu+2EMdGJ5DSJd63vB8X9ndn/TajK4nJVh3cb9yyFDsOnQQAnNc3OfyGV4kPXjYPC6Z3OJ7tczuP4L/WDuDkWDp0o3/oxBhaVAX/8K5zoQiBq8/qhaoKTGnLLpn477/ZjUHrbbrWHDoxhmRCweddpScTqoK3Lp5RZCnilzCuqAEAc1zf+wDs884kpbwPwH0AsHTp0kDi4ZL+bizx3PTl0Nfdhr7utsDLk3BYNLMLi2Z2Yd2eIbw2eLLk/LYxWj5/albNYy/n9U3BeX3x8fBtuttb8DtLZjvf04bEf60dqEq647Qh0ZpQcOOFfVnT33nB7KzvT24bxKa9x0Pfvh90Q2JSQs1pIwmHMEIcXgCwUAgxTwjRAuBmAI+EsF7S4PgdFJQK0GkfZ8JMSufF78jWKAdsMeVCdanY05dSpoUQfwLgcQAqgH+TUm6quGWk4VF9lukLUjAnzoRZPN5LSvdnUKNMzZDWmUa5moQiGEopfwrgp2GsizQPms+KTfY8zRK9kbAGIVUjhNJvXvroPf3mONdR0ByuE6lL/I4EjVuh80rJePrV0fT9PDyjyvwJZGr3kupAo08iw683GSSNdpxxF3kPG/+afnT5eOjpVxcafRIZqqJASsAoYVziVui8Uqqp6fs1qFGlhgBgFcFpjnMdBTyyJDJsmaGUcUsFqIccZ5zaANWI3vFpUM23sIg6cunpVxUafRIZfoukU9MPj1h4+obRNJ32UUCjTyJD82nc3Hnzm4FqJkDza1AZvdO40OiTyCjf02+Oy7U+NH1//S3VgKURq0tz3EWkLtF8Grfmi96x4vSjHJHrs7+lGtDTry40+iQyVJ8dlnbBnGbx/qrq6ev+NX0gmhz75ohcmqZqwSNLIsO3pt9kRTS0Kmr6ZjF0f9E7ACIp2B63Ijhxg0afRIZfb7LZOnKrGb3j16A65yaCgu1pavpVhUafRIZf3bj5NP3qZdlM+0xxYBchj0LTj1th+7hBo08iw+mwLBW9Yw3OSjSJzqtVNeGaP0+/mqkgShG3wvZxI9yyPISUgW18Ht94AK8MFC7Ysf6NY+b8TeL92Qb3+d1HnWOkKMCVZ/aip4xKcV6e2j6IodEJLJjeUXJee7u/2nYIN1/cD8OQ+PnmgxixalS7Wb5gKmZPmYTxtI4nNh/EeCojS82d1oaLTu/BWErH6i2Z35IJ1ali98TmgxhL6c4yx0ZTTfNWFwU0+iQypne2AgD++YntJeftbNXKqoccZzpaNSQTCh5YN+DU/AWAj129AH+1alGgdR4aHsMt//Y8gMxxL0avNc8dD72Ca8+Zid1HRvGR76zLO+87LzgN99y8BL/aNog/+d76rN/aW1Rs+t/X4smth3J++9c/WApNEfjY917MWaefNpJg0OiTyLjo9G48/8kVGEuV7rCc3JZAi9Ycr/ztrRrW3PlWHD+VcqZdd8/TODURvGP3lOVJf+rti3HrZfNKzn/VWb24fcVC3POLVzGWMjA6YXr499x8AZbMyZQsvfVbz+PUhLlu+//3/vAS9HW34f5nd+I/ntsDABi1fvv+Hy1H2jDw/vufx6mUDlWYHv23bl2G+dMybyCzuycF3ldSHBp9Eim9Xcmom1CXTG5LOAXfASChKc54hSDYHbK9XUnf0olteNOG4Wj7s6dMQv/UTJ3pSS2q85u9jTk9bZjT04ae9lZnVK89T//UNqTS5n7ohgFpGf05PW1Z6yXVg0afkBhQaS6cdIA6w+7O3EIRVKor775dXtGOvHFHZzlht4qA4YpOsvqsGaJZQ2j0CYkBlWa9tGP+y+kgdY8M1p2HRrbE5q5+5n0wuMdh6K7tuwef2XvEjtvaQaNPSAzQFAWpCuL2g6Snduf1L/TQUBXh1DvwJsZzj+pNud40pFSs6RKqzN4WqT40+oTEANPTr1zTD+bpGwVHRWuKwISl0Rf09PWMpq8qAtIy9LpuAJ55SfXh45WQGKCplWn6ttFNqP5veXde/0JvCpqa0fTth1LC0fQzo3rTru2rebR+FkKvHfT0CYkBWqWafoCkdW5NPx1A09cKafoyd7QvPf3aQaNPSAxwR8kEoRJN3+3pe0dFq66oIm9nbz55SBWu6B1XRy41/dpBo09IDKjU009VEr2jZ4dc5rbLsLZhzmPPku3pSygCUBQBDbm5hejp1w4afUJigNujDkKhkMtiaGp+eaZQu3TDgKYICJHdOWtr9/a27VWkDel06jJOv3bQ6BMSA9wedRBCi97J6+lnNH33+r3ykP2bEAIJ1dofISCsNwBSG2j0CYkBqiIqyq/vaPplRMl45Rm7HdntUpx26Xp28ZMseSjPb2lDQgp6+bWGvSeExABNjXZEbsbTLx69k+3pZ8tD7k5gTVGc+H3q+bWFnj4hMUBVFKQMvfSMBah4RK5eQNNXhfNAMSteKVm/AdaIXCO/p29IRu7UmoqOthDiS0KIrUKIDUKIh4UQU8JqGCEkQ8WafkVx+sU1fSfhmmEU9vT13LcAM3unQU+/xlT6iH0CwDlSyvMAbAdwZ+VNIoR40SrU9N0jYv3iHZGr5OlwtWUawHywJPJ05Nohn26P3k4g57dmLwmPioy+lPLnUkq7ftpzAPoqbxIhxEulmn6hkMtiFAq59LYrE7Ips3V7T8inuxM5oZodwNT0a0+Ymv4HAfxniOsjhFioioIDx8dw50MbnGnJhIrbVyzElDazbu4vtx7EE5sP4vSp7fjIlQtw/FQKd6/ejrGUjm0HTgAIpuk/sHYAw2P569aqisB4WsedD23AuteHcrx5APjXZ3Zi1+GRLKOvKgIv7D4KRQhq+jWmpNEXQqwGMDPPT3dJKX9kzXMXgDSA7xZZz20AbgOA/v7+QI0lpFm5eG431uw8gl9sOQTA9L6Pjkxg+fypWPUm8/a87+mdeG7nUQDABy6dixd2HcU3f70bPe0t0BSBJf1T0NHq38/rbk/g3NmTsfvICADgsjOm5cxzwZwp6O1MOu1asXiG81t/TxsWTG/Hlv3DAICVCzO/XbpgKn651VzmijOn+24TqRwhZfBXRgAQQtwC4CMAVkgpR/0ss3TpUrl27dqKtktIM7PtwAmsuvtp/L/3Xojrz50FALjp3t9g7Z4hAMCmz67CM68O4iPfeRE/u/1yLJ7VFWVzSUgIIdZJKZdWso6K5B0hxLUA/hrAlX4NPiGkctx6u433c6GIG9LcVCqmfRVAJ4AnhBAvCSG+FkKbCCElyIRDZsI4067PxUbRkuamIk9fSnlGWA0hhPjHneLAJuuzYRTMgU+aG14NhMQQTc2Vd9whnVl1bRkHT1zQ6BMSQ/Jp+rrnMzV9kg8afUJiSMJOW6y7NX0J4cpVHyTfDml8aPQJiSFqAXmnVbNz2FPTJ/nh1UBIDHEnM7NJGwZaNdX6XLiuLWluaPQJiSF54/T1jKdfrK4taW5o9AmJIe5ShDZpQ6I14S5RWH6SNdL40OgTEkPcxcVtdEMi6ZJ3nLq4gkafZKDRJySGCCFyCqukDcPj6efPgU+aGxp9QmKKXXLQxozesT19o2AOfNLc8IogJKZ4q2mlDU9Hrs5ShCQXGn1CYopdchAwvXwp4YrTt6td0eiTbGj0CYkpmqo4+XXs/944fcboEy80+oTEFM3j6QNAMuEakUtNn+QhzBq5hJAaoikCj27Yj7W7h6BL2+ibnv5nHtmM0Yk0JrWoUTaR1CE0+oTElA9fuQBrdh1xvp9z2mR88C3zoCoCQ6MTAIBlc3uiah6pUyqukRsE1sglhJDyCaNGLgU/QghpImj0CSGkiaDRJ4SQJoJGnxBCmggafUIIaSJo9AkhpImg0SeEkCaCRp8QQpqISAZnCSEGAewJuPg0AIdDbE7caOb9b+Z9B5p7/5t534HM/p8upZxeyYoiMfqVIIRYW+mItDjTzPvfzPsONPf+N/O+A+HuP+UdQghpImj0CSGkiYij0b8v6gZETDPvfzPvO9Dc+9/M+w6EuP+x0/QJIYQEJ46ePiGEkIDEyugLIa4VQmwTQuwQQtwRdXvCRggxRwjxpBBiixBikxDidmt6jxDiCSHEq9b/bmu6EEL8H+t4bBBCXBjtHlSOEEIVQqwXQjxqfZ8nhFhj7ft/CiFarOmt1vcd1u9zo2x3GAghpgghHhBCbLWugTc32bn/C+u63yiE+L4QItmo518I8W9CiENCiI2uaWWfayHELdb8rwohbvGz7dgYfSGECuBfAFwH4GwA7xFCnB1tq0InDeAvpZSLASwH8DFrH+8A8Asp5UIAv7C+A+axWGj93Qbg3to3OXRuB7DF9f2LAL5i7fsQgA9Z0z8EYEhKeQaAr1jzxZ17ADwmpVwE4HyYx6Epzr0QYjaAPwOwVEp5DgAVwM1o3PP/LQDXeqaVda6FED0APg3gEgAXA/i0/aAoipQyFn8A3gzgcdf3OwHcGXW7qrzPPwKwEsA2ALOsabMAbLM+fx3Ae1zzO/PF8Q9An3WxXwPgUQAC5oAUzXsNAHgcwJutz5o1n4h6HyrY9y4Au7z70ETnfjaANwD0WOfzUQCrGvn8A5gLYGPQcw3gPQC+7pqeNV+hv9h4+shcFDYD1rSGxHpdXQJgDYAZUsr9AGD977Vma7RjcjeATwAwrO9TARyTUqat7+79c/bd+v24NX9cmQ9gEMA3LXnrG0KIdjTJuZdS7gXwTwBeB7Af5vlch+Y5/0D55zrQNRAnoy/yTGvI0CMhRAeABwH8uZRyuNiseabF8pgIIW4AcEhKuc49Oc+s0sdvcUQDcCGAe6WUSwCMIPN6n4+G2n9LlngngHkATgPQDlPW8NKo578YhfY10DGIk9EfADDH9b0PwL6I2lI1hBAJmAb/u1LKh6zJB4UQs6zfZwE4ZE1vpGNyGYB3CCF2A/gBTInnbgBThBCaNY97/5x9t36fDOBoLRscMgMABqSUa6zvD8B8CDTDuQeAtwLYJaUclFKmADwE4FI0z/kHyj/Xga6BOBn9FwAstHrzW2B28jwScZtCRQghANwPYIuU8suunx4BYPfM3wJT67en/4HVu78cwHH79TBuSCnvlFL2SSnnwjy3v5RSvhfAkwBusmbz7rt9TG6y5o+tpyelPADgDSHEWdakFQA2ownOvcXrAJYLIdqs+8De/6Y4/xblnuvHAbxNCNFtvSm9zZpWnKg7M8rs+LgewHYArwG4K+r2VGH/3gLz9WwDgJesv+thapW/APCq9b/Hml/AjGh6DcArMCMfIt+PEI7DVQAetT7PB/A8gB0Afgig1ZqetL7vsH6fH3W7Q9jvCwCstc7/fwPobqZzD+CzALYC2Ajg2wBaG/X8A/g+zL6LFEyP/UNBzjWAD1rHYAeAW/1smyNyCSGkiYiTvEMIIaRCaPQJIaSJoNEnhJAmgkafEEKaCBp9QghpImj0CSGkiaDRJ4SQJoJGnxBCmoj/D7Sd73wwAHfQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#plt.plot(total_rewards[0])\n",
    "plt.plot(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(invalids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error in the end comes from the network predicting a result, which is wrong and since exploration is way down it almost\n",
    "# always predicts the same action which is always wrong. Should somehow learn though (maybe replay necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Use memory replay --> DONE\n",
    "- Maybe higher rewards needed for backpropagation of Q values?\n",
    "- View reward function by playing vs the network\n",
    "- View network output for certain states\n",
    "<br>\n",
    "<br>\n",
    "- Do I even backpropagate the reward to other states than the winning one in any way?\n",
    "- Maybe the problem are few games (not enough possibilities learned) -> More iterations like 10_000 games instead of iterations\n",
    "- Learning rate?\n",
    "<br>\n",
    "<br>\n",
    "- Rework memory replay batch size and epochs analog to pytorch tutorial\n",
    "- Plot metrics (e.g. total reward every iteration)\n",
    "- Rework code --> Readability and reusability\n",
    "- Maybe rework greedy policy\n",
    "- Test the pytorch agent on the dungeon example --> DONE: Works\n",
    "- Try increasing the performance (For running in the cloud) -> Use timer\n",
    "- Maybe no punishment for invalid moves?\n",
    "- Pass possible moves to network?\n",
    "- Only give out copies of the state... --> FIXED (This literally ruined every single state in the memory...)\n",
    "- Copy pytorch tensors via .copy().detach() (maybe more effectively possible as well?)\n",
    "<br>\n",
    "<br>\n",
    "- How to choose rewards and how does the agent learn the rules (punishment for invalid moves?)\n",
    "- How much training is needed for a game?\n",
    "- Evaluation tactics: Total reward\n",
    "- Model too big?\n",
    "- Don't copy model to update agent? --> Constantly creating optimizer and agent again and again\n",
    "- Only learning negative values atm --> Why?\n",
    "- Ignore invalid moves\n",
    "- Limit reward to between -1 and 1\n",
    "- Let AI learn both sides at the same time, so playing against it makes more sense?!\n",
    "<br>\n",
    "<br>\n",
    "Takeaways:\n",
    "- Batching is so much quicker, it is absurd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_board = np.zeros(shape=(6, 7))\n",
    "empty_board = torch.tensor(empty_board.flatten(), device=active.device).float()\n",
    "empty_board = torch.unsqueeze(empty_board, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = np.array([\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "example_board2 = np.array([\n",
    "    [0,0,1,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,1,-1,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "example_board = torch.tensor(example_board.flatten(), device=active.device).float()\n",
    "example_board = torch.unsqueeze(example_board, dim=0)\n",
    "example_board2 = torch.tensor(example_board2.flatten(), device=active.device).float()\n",
    "example_board2 = torch.unsqueeze(example_board2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[2.1633, 2.1483, 2.1038, 2.1681, 2.0973, 2.0591, 2.1564, 2.1864, 2.1143,\n",
      "         2.1198, 2.1380, 2.1937, 2.1887, 2.0648, 2.1676, 1.9492, 2.2284, 2.1427,\n",
      "         2.0540, 2.0786, 2.1189, 1.9543, 2.4569, 2.1460, 2.1692, 2.5363, 2.0080,\n",
      "         2.0183, 2.1215, 2.1513, 2.0217, 1.9782, 1.9705, 2.1020, 2.1624, 2.1625,\n",
      "         1.6173, 2.1121, 1.8627, 1.8862, 1.3497, 2.2607]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(25, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board, [0, 1, 2, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board)[0], [0, 1, 2, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "tensor([[2.1580, 2.1424, 2.0978, 2.1605, 2.0906, 2.0524, 2.1502, 2.1825, 2.1077,\n",
      "         2.1119, 2.1316, 2.1882, 2.1815, 2.0563, 2.1602, 1.9445, 2.2233, 2.1357,\n",
      "         2.0496, 2.0741, 2.1117, 1.9475, 2.4523, 2.1390, 2.1603, 2.5277, 2.0021,\n",
      "         2.0111, 2.1145, 2.1456, 2.0163, 1.9709, 1.9647, 2.0959, 2.1550, 2.1557,\n",
      "         1.6145, 2.1052, 1.8568, 1.8835, 1.3453, 2.2552]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(25, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(example_board2, [0, 1, 9, 24, 25, 5, 6]))\n",
    "print(active.get_Q(example_board2))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(example_board2)[0], [0, 1, 9, 24, 25, 5, 6]), 0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[2.1559, 2.1492, 2.1046, 2.1629, 2.0953, 2.0576, 2.1530, 2.1921, 2.1086,\n",
      "         2.1227, 2.1344, 2.1916, 2.1862, 2.0633, 2.1601, 1.9456, 2.2250, 2.1388,\n",
      "         2.0463, 2.0810, 2.1184, 1.9522, 2.4523, 2.1401, 2.1632, 2.5334, 2.0052,\n",
      "         2.0153, 2.1171, 2.1476, 2.0118, 1.9752, 1.9625, 2.0966, 2.1583, 2.1591,\n",
      "         1.6111, 2.1084, 1.8568, 1.8914, 1.3437, 2.2585]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(41, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(active.next_action(empty_board, range(7)))\n",
    "print(active.get_Q(empty_board))\n",
    "print(torch.max(active.normalized_Q(active.get_Q(empty_board)[0], range(7)), 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game = ConnectFourSimulator()\n",
    "val = input()\n",
    "while val != \"q\":\n",
    "    game_over, board, _, _, _, _ = game.take_action(int(val))\n",
    "    print(game_over)\n",
    "    print(board)\n",
    "    print(\"------------------------------------\")\n",
    "    if not game_over:\n",
    "        confirmation = \"r\"\n",
    "        while confirmation == \"r\":\n",
    "            pc_action = active.next_action(board)\n",
    "            print(pc_action)\n",
    "            confirmation = input()\n",
    "            if confirmation == \"c\":\n",
    "                game_over, board, _, _, _, _ = game.take_action(pc_action)\n",
    "                print(game_over)\n",
    "                print(board)\n",
    "    val = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = memory.sample(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = [ game for game in games if game[3] == 100 or game[3] == -100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in terminal_states:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1.,  0., -1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  0.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 18\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 7\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  0.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 8\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  0., -1., -1.,  0.,  0.,\n",
      "         0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 15\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  0., -1., -1., -1.,  0.,\n",
      "         0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  0.,\n",
      "         0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 9\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "         0.,  1.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "         0.,  1.,  0., -1.,  1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 20\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "         0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "         0.,  1.,  0., -1.,  1., -1.,  1.,  0.,  1.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 22\n",
      "Reward: tensor(1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 0.,  1.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 0.,  1.,  0.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1.,  0.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1.,  0.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1.,  0.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 12\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 13\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 20\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 34\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  0.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1.,  0.,  0.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 9\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1., -1.,  0.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1., -1.,  0.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 16\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1., -1.,  0.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0.,  0., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 8\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  0., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  1., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0')\n",
      "With action: 18\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  1., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  1., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0., -1., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 30\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  0.,  1.,  0.,  1., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "         0.,  1.,  1.,  0.,  1., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 15\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "        -1.,  1.,  1.,  0.,  1., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,  0., -1.,  1.,  1.,\n",
      "        -1.,  1.,  1.,  0.,  1., -1.,  1.,  0.,  1., -1.,  0.,  0., -1., -1.,\n",
      "         0.,  0.,  1.,  0.,  0., -1.,  1.,  0.,  0., -1.,  0.,  0., -1., -1.],\n",
      "       device='cuda:0')\n",
      "With action: 22\n",
      "Reward: tensor(1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 4\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  0., -1.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  0., -1.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 11\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  0., -1.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  0., -1.,  0.,  1., -1.,  0.,  0.,  0.,  1.,  0.,  1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 9\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  0., -1.,  0.,  1., -1.,  0.,  0.,  0.,  1.,  0.,  1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  0., -1.,  0.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 6\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  0., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 1\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 20\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 19\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 39\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  0.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  1.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "         0.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 7\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  1.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "        -1.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  1., -1., -1.,  1., -1.,  1.,  1.,  0.,  1.,  0.,  1., -1., -1.,\n",
      "        -1.,  0.,  0.,  0., -1.,  1.,  1.,  0.,  0.,  0.,  0., -1., -1.,  1.,\n",
      "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 27\n",
      "Reward: tensor(-1., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "With action: 0\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 3\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  0.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 7\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  0.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  1.,  1., -1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 2\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n",
      "Before:\n",
      " tensor([ 1., -1.,  1.,  1., -1.,  0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "After:\n",
      " tensor([ 1., -1.,  1.,  1., -1.,  1.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       device='cuda:0')\n",
      "With action: 5\n",
      "Reward: tensor(0., device='cuda:0')\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for state in memory.memory[:42]:\n",
    "    print(\"Before:\\n\", state[0])\n",
    "    print(\"After:\\n\", state[1])\n",
    "    print(\"With action:\", state[2])\n",
    "    print(\"Reward:\", state[3])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayAI:\n",
    "    def __init__(self, ai):\n",
    "        self.ai = ai\n",
    "        self.game = ConnectFourSimulator()\n",
    "        self.game_started = False\n",
    "        self.random_actions = 0\n",
    "        \n",
    "    def start_game(self):\n",
    "        self.__ai_move()\n",
    "        self.game.print_board()\n",
    "        print(\"Valid actions:\", self.game.valid_actions)\n",
    "        self.game_started = True\n",
    "    \n",
    "    def __ai_move(self):\n",
    "        valid_actions = self.game.valid_actions\n",
    "        state = torch.tensor(self.game.board.flatten(), device=self.ai.device).clone().float()\n",
    "        action = self.ai.next_action(torch.unsqueeze(state, dim=0), valid_actions)\n",
    "        if not action in valid_actions:\n",
    "            action_index = random.random(0, len(valid_actions))\n",
    "            action = valid_actions[action_index]\n",
    "            self.random_actions += 1\n",
    "        game_over, _, _, _, _, _ = self.game.take_action(action)\n",
    "        return game_over\n",
    "            \n",
    "        \n",
    "    def play(self):\n",
    "        assert self.game_started == True, \"Game has not yet been started\"\n",
    "        \n",
    "        game_over = False\n",
    "        \n",
    "        while not game_over:\n",
    "            action = input()\n",
    "            if action == \"q\":\n",
    "                return\n",
    "            action = int(action)\n",
    "            game_over, _, _, reward, _, _, = self.game.take_action(action)\n",
    "            assert reward >= 0, \"Invalid action!\"\n",
    "        \n",
    "            self.game.print_board()\n",
    "            if game_over:\n",
    "                print(\"You won!\")\n",
    "                return\n",
    "            \n",
    "            game_over = self.__ai_move()\n",
    "            self.game.print_board()\n",
    "            print(\"Valid actions:\", self.game.valid_actions)\n",
    "            if game_over:\n",
    "                print(\"You lost :/\")\n",
    "                return      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' 'X' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [0, 2, 3, 4, 5, 6, 8]\n",
      "3\n",
      "[['-' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['X' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [2, 4, 5, 6, 8, 10, 7]\n",
      "10\n",
      "[['X' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['X' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [2, 4, 5, 6, 8, 7, 24]\n",
      "2\n",
      "[['X' 'X' 'O' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['X' 'X' 'O' 'O' '-' '-' '-']\n",
      " ['X' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [4, 5, 6, 8, 24, 9, 14]\n",
      "4\n",
      "[['X' 'X' 'O' 'O' 'O' '-' '-']\n",
      " ['X' '-' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['X' 'X' 'O' 'O' 'O' '-' '-']\n",
      " ['X' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [5, 6, 24, 9, 14, 11, 15]\n",
      "5\n",
      "[['X' 'X' 'O' 'O' 'O' 'O' '-']\n",
      " ['X' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "[['X' 'X' 'O' 'O' 'O' 'O' '-']\n",
      " ['X' 'X' '-' 'O' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' 'X' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']\n",
      " ['-' '-' '-' '-' '-' '-' '-']]\n",
      "Valid actions: [6, 9, 14, 11, 15, 12, 31]\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "vs_game = PlayAI(active)\n",
    "vs_game.start_game()\n",
    "vs_game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
